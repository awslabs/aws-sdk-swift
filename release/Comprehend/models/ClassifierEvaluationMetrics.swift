// Code generated by smithy-swift-codegen. DO NOT EDIT!



/// <p>Describes the result metrics for the test data associated with an documentation
///       classifier.</p>
public struct ClassifierEvaluationMetrics: Equatable {
    /// <p>The fraction of the labels that were correct recognized. It is computed by dividing the
    ///       number of labels in the test documents that were correctly recognized by the total number of
    ///       labels in the test documents.</p>
    public let accuracy: Double?
    /// <p>A measure of how accurate the classifier results are for the test data. It is derived from
    ///       the <code>Precision</code> and <code>Recall</code> values. The <code>F1Score</code> is the
    ///       harmonic average of the two scores. The highest score is 1, and the worst score is 0. </p>
    public let f1Score: Double?
    /// <p>Indicates the fraction of labels that are incorrectly predicted. Also seen as the fraction
    ///       of wrong labels compared to the total number of labels. Scores closer to zero are
    ///       better.</p>
    public let hammingLoss: Double?
    /// <p>A measure of how accurate the classifier results are for the test data. It is a
    ///       combination of the <code>Micro Precision</code> and <code>Micro Recall</code> values. The
    ///         <code>Micro F1Score</code> is the harmonic mean of the two scores. The highest score is 1,
    ///       and the worst score is 0.</p>
    public let microF1Score: Double?
    /// <p>A measure of the usefulness of the recognizer results in the test data. High precision
    ///       means that the recognizer returned substantially more relevant results than irrelevant ones.
    ///       Unlike the Precision metric which comes from averaging the precision of all available labels,
    ///       this is based on the overall score of all precision scores added together.</p>
    public let microPrecision: Double?
    /// <p>A measure of how complete the classifier results are for the test data. High recall means
    ///       that the classifier returned most of the relevant results. Specifically, this indicates how
    ///       many of the correct categories in the text that the model can predict. It is a percentage of
    ///       correct categories in the text that can found. Instead of averaging the recall scores of all
    ///       labels (as with Recall), micro Recall is based on the overall score of all recall scores added
    ///       together.</p>
    public let microRecall: Double?
    /// <p>A measure of the usefulness of the classifier results in the test data. High precision
    ///       means that the classifier returned substantially more relevant results than irrelevant
    ///       ones.</p>
    public let precision: Double?
    /// <p>A measure of how complete the classifier results are for the test data. High recall means
    ///       that the classifier returned most of the relevant results. </p>
    public let recall: Double?

    public init (
        accuracy: Double? = nil,
        f1Score: Double? = nil,
        hammingLoss: Double? = nil,
        microF1Score: Double? = nil,
        microPrecision: Double? = nil,
        microRecall: Double? = nil,
        precision: Double? = nil,
        recall: Double? = nil
    )
    {
        self.accuracy = accuracy
        self.f1Score = f1Score
        self.hammingLoss = hammingLoss
        self.microF1Score = microF1Score
        self.microPrecision = microPrecision
        self.microRecall = microRecall
        self.precision = precision
        self.recall = recall
    }
}

// Code generated by smithy-swift-codegen. DO NOT EDIT!

import AWSClientRuntime
import ClientRuntime
import Foundation

public class RekognitionClient {
    let client: SdkHttpClient
    let config: RekognitionClientConfiguration
    let serviceName = "Rekognition"
    let encoder: RequestEncoder
    let decoder: ResponseDecoder

    public init(config: RekognitionClientConfiguration) {
        client = SdkHttpClient(engine: config.httpClientEngine, config: config.httpClientConfiguration)
        let encoder = JSONEncoder()
        encoder.dateEncodingStrategy = .secondsSince1970
        self.encoder = config.encoder ?? encoder
        let decoder = JSONDecoder()
        decoder.dateDecodingStrategy = .secondsSince1970
        self.decoder = config.decoder ?? decoder
        self.config = config
    }

    public class RekognitionClientConfiguration: ClientRuntime.Configuration, AWSClientConfiguration {

        public var region: String
        public var credentialsProvider: AWSCredentialsProvider
        public var signingRegion: String
        public var endpointResolver: EndpointResolver

        public init (
            credentialsProvider: AWSCredentialsProvider,
            endpointResolver: EndpointResolver,
            region: String,
            signingRegion: String
        ) throws
        {
            self.credentialsProvider = credentialsProvider
            self.endpointResolver = endpointResolver
            self.region = region
            self.signingRegion = signingRegion
        }

        public convenience init(credentialsProvider: AWSCredentialsProvider) throws {
            let region = "us-east-1"
            let signingRegion = "us-east-1"
            let endpointResolver = DefaultEndpointResolver()
            try self.init(
                credentialsProvider: credentialsProvider,
                endpointResolver: endpointResolver,
                region: region,
                signingRegion: signingRegion
            )
        }

        public static func `default`() throws -> RekognitionClientConfiguration {
            let awsCredsProvider = try AWSCredentialsProvider.fromEnv()
            return try RekognitionClientConfiguration(credentialsProvider: awsCredsProvider)
        }
    }
}

extension RekognitionClient: RekognitionClientProtocol {
    /// <p>Compares a face in the <i>source</i> input image with
    ///       each of the 100 largest faces detected in the <i>target</i> input image.
    ///     </p>
    ///
    ///          <p> If the source image contains multiple faces, the service detects the largest face
    ///         and compares it with each face detected in the target image. </p>
    ///
    ///
    ///          <note>
    ///             <p>CompareFaces uses machine learning algorithms, which are probabilistic.
    ///       A false negative is an incorrect prediction that
    ///       a face in the target image has a low similarity confidence score when compared to the face
    ///       in the source image. To reduce the probability of false negatives,
    ///       we recommend that you compare the target image against multiple source images.
    ///       If you plan to use <code>CompareFaces</code> to make a decision that impacts an individual's rights,
    ///       privacy, or access to services, we recommend that you pass the result to a human for review and further
    ///       validation before taking action.</p>
    ///          </note>
    ///
    ///
    ///          <p>You pass the input and target images either as base64-encoded image bytes or as
    ///       references to images in an Amazon S3 bucket. If you use the
    ///       AWS
    ///       CLI to call Amazon Rekognition operations, passing image bytes isn't
    ///       supported. The image must be formatted as a PNG or JPEG file. </p>
    ///          <p>In response, the operation returns an array of face matches ordered by similarity score
    ///       in descending order. For each face match, the response provides a bounding box of the face,
    ///       facial landmarks, pose details (pitch, role, and yaw), quality (brightness and sharpness), and
    ///       confidence value (indicating the level of confidence that the bounding box contains a face).
    ///       The response also provides a similarity score, which indicates how closely the faces match. </p>
    ///
    ///          <note>
    ///             <p>By default, only faces with a similarity score of greater than or equal to 80% are
    ///         returned in the response. You can change this value by specifying the
    ///           <code>SimilarityThreshold</code> parameter.</p>
    ///          </note>
    ///
    ///          <p>
    ///             <code>CompareFaces</code> also returns an array of faces that don't match the source image.
    ///       For each face, it returns a bounding box, confidence value, landmarks, pose details, and quality.
    ///     The response also returns information about the face in the source image, including the bounding box
    ///       of the face and confidence value.</p>
    ///
    ///          <p>The <code>QualityFilter</code> input parameter allows you to filter out detected faces
    ///       that donâ€™t meet a required quality bar. The quality bar is based on a
    ///       variety of common use cases.  Use <code>QualityFilter</code> to set the quality bar
    ///       by specifying <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>.
    ///       If you do not want to filter detected faces, specify <code>NONE</code>. The default value is <code>NONE</code>. </p>
    ///
    ///          <p>If the image doesn't contain Exif metadata, <code>CompareFaces</code> returns orientation information for the
    ///         source and target images. Use these values to display the images with the correct image orientation.</p>
    ///          <p>If no faces are detected in the source or target images, <code>CompareFaces</code> returns an
    ///     <code>InvalidParameterException</code> error. </p>
    ///
    ///
    ///          <note>
    ///             <p> This is a stateless API operation. That is, data returned by this operation doesn't persist.</p>
    ///          </note>
    ///
    ///
    ///          <p>For an example, see Comparing Faces in Images in the Amazon Rekognition Developer Guide.</p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:CompareFaces</code>
    ///       action.</p>
    public func compareFaces(input: CompareFacesInput, completion: @escaping (SdkResult<CompareFacesOutput, CompareFacesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "compareFaces")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<CompareFacesInput, CompareFacesOutput, CompareFacesOutputError>(id: "compareFaces")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: CompareFacesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: CompareFacesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<CompareFacesInput, CompareFacesOutput, CompareFacesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: CompareFacesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<CompareFacesInput, CompareFacesOutput, CompareFacesOutputError>(xAmzTarget: "RekognitionService.CompareFaces"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Creates a collection in an AWS Region. You can add faces to the collection using the
    ///         <a>IndexFaces</a> operation. </p>
    ///          <p>For example, you might create collections, one for each of your application users. A
    ///       user can then index faces using the <code>IndexFaces</code> operation and persist results in a
    ///       specific collection. Then, a user can search the collection for faces in the user-specific
    ///       container. </p>
    ///          <p>When you create a collection, it is associated with the latest version of the face model version.</p>
    ///          <note>
    ///             <p>Collection names are case-sensitive.</p>
    ///          </note>
    ///
    ///          <p>This operation requires permissions to perform the
    ///       <code>rekognition:CreateCollection</code> action. If you want to tag your collection, you also require permission to perform the <code>rekognition:TagResource</code> operation.</p>
    public func createCollection(input: CreateCollectionInput, completion: @escaping (SdkResult<CreateCollectionOutput, CreateCollectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "createCollection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<CreateCollectionInput, CreateCollectionOutput, CreateCollectionOutputError>(id: "createCollection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: CreateCollectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: CreateCollectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<CreateCollectionInput, CreateCollectionOutput, CreateCollectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: CreateCollectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<CreateCollectionInput, CreateCollectionOutput, CreateCollectionOutputError>(xAmzTarget: "RekognitionService.CreateCollection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Creates a new Amazon Rekognition Custom Labels project. A project is a logical grouping of resources (images, Labels, models)
    ///          and operations (training, evaluation and detection).  </p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:CreateProject</code> action.</p>
    public func createProject(input: CreateProjectInput, completion: @escaping (SdkResult<CreateProjectOutput, CreateProjectOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "createProject")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<CreateProjectInput, CreateProjectOutput, CreateProjectOutputError>(id: "createProject")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: CreateProjectInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: CreateProjectInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<CreateProjectInput, CreateProjectOutput, CreateProjectOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: CreateProjectInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<CreateProjectInput, CreateProjectOutput, CreateProjectOutputError>(xAmzTarget: "RekognitionService.CreateProject"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Creates a new version of a model and begins training.
    ///          Models are managed as part of an Amazon Rekognition Custom Labels project.  You can specify
    ///          one training dataset and one testing dataset. The response from <code>CreateProjectVersion</code>
    ///          is an Amazon Resource Name (ARN) for the version of the model. </p>
    ///          <p>Training takes a while to complete. You can get the current status by calling
    ///          <a>DescribeProjectVersions</a>.</p>
    ///          <p>Once training has successfully completed, call <a>DescribeProjectVersions</a> to
    ///          get the training results and evaluate the model.
    ///       </p>
    ///          <p>After evaluating the model, you start the model
    ///        by calling <a>StartProjectVersion</a>.</p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:CreateProjectVersion</code> action.</p>
    public func createProjectVersion(input: CreateProjectVersionInput, completion: @escaping (SdkResult<CreateProjectVersionOutput, CreateProjectVersionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "createProjectVersion")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<CreateProjectVersionInput, CreateProjectVersionOutput, CreateProjectVersionOutputError>(id: "createProjectVersion")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: CreateProjectVersionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: CreateProjectVersionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<CreateProjectVersionInput, CreateProjectVersionOutput, CreateProjectVersionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: CreateProjectVersionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<CreateProjectVersionInput, CreateProjectVersionOutput, CreateProjectVersionOutputError>(xAmzTarget: "RekognitionService.CreateProjectVersion"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Creates an Amazon Rekognition stream processor that you can use to detect and recognize faces in a streaming video.</p>
    ///         <p>Amazon Rekognition Video is a consumer of live video from Amazon Kinesis Video Streams. Amazon Rekognition Video sends analysis results to Amazon Kinesis Data Streams.</p>
    ///         <p>You provide as input a Kinesis video stream (<code>Input</code>) and a Kinesis data stream (<code>Output</code>) stream. You also specify the
    ///             face recognition criteria in <code>Settings</code>. For example, the collection containing faces that you want to recognize.
    ///             Use <code>Name</code> to assign an identifier for the stream processor. You use <code>Name</code>
    ///             to manage the stream processor. For example, you can start processing the source video by calling <a>StartStreamProcessor</a> with
    ///             the <code>Name</code> field. </p>
    ///         <p>After you have finished analyzing a streaming video, use <a>StopStreamProcessor</a> to
    ///         stop processing. You can delete the stream processor by calling <a>DeleteStreamProcessor</a>.</p>
    ///         <p>This operation requires permissions to perform the
    ///             <code>rekognition:CreateStreamProcessor</code> action. If you want to tag your stream processor, you also require permission to perform the <code>rekognition:TagResource</code> operation.</p>
    public func createStreamProcessor(input: CreateStreamProcessorInput, completion: @escaping (SdkResult<CreateStreamProcessorOutput, CreateStreamProcessorOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "createStreamProcessor")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<CreateStreamProcessorInput, CreateStreamProcessorOutput, CreateStreamProcessorOutputError>(id: "createStreamProcessor")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: CreateStreamProcessorInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: CreateStreamProcessorInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<CreateStreamProcessorInput, CreateStreamProcessorOutput, CreateStreamProcessorOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: CreateStreamProcessorInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<CreateStreamProcessorInput, CreateStreamProcessorOutput, CreateStreamProcessorOutputError>(xAmzTarget: "RekognitionService.CreateStreamProcessor"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Deletes the specified collection. Note that this operation
    ///       removes all faces in the collection. For an example, see <a>delete-collection-procedure</a>.</p>
    ///
    ///          <p>This operation requires permissions to perform the
    ///         <code>rekognition:DeleteCollection</code> action.</p>
    public func deleteCollection(input: DeleteCollectionInput, completion: @escaping (SdkResult<DeleteCollectionOutput, DeleteCollectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "deleteCollection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DeleteCollectionInput, DeleteCollectionOutput, DeleteCollectionOutputError>(id: "deleteCollection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DeleteCollectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DeleteCollectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DeleteCollectionInput, DeleteCollectionOutput, DeleteCollectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DeleteCollectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DeleteCollectionInput, DeleteCollectionOutput, DeleteCollectionOutputError>(xAmzTarget: "RekognitionService.DeleteCollection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Deletes faces from a collection. You specify a collection ID and an array of face IDs
    ///       to remove from the collection.</p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:DeleteFaces</code>
    ///       action.</p>
    public func deleteFaces(input: DeleteFacesInput, completion: @escaping (SdkResult<DeleteFacesOutput, DeleteFacesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "deleteFaces")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DeleteFacesInput, DeleteFacesOutput, DeleteFacesOutputError>(id: "deleteFaces")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DeleteFacesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DeleteFacesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DeleteFacesInput, DeleteFacesOutput, DeleteFacesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DeleteFacesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DeleteFacesInput, DeleteFacesOutput, DeleteFacesOutputError>(xAmzTarget: "RekognitionService.DeleteFaces"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Deletes an Amazon Rekognition Custom Labels project.  To delete a project you must first delete all models associated
    ///          with the project. To delete a model, see <a>DeleteProjectVersion</a>.</p>
    ///          <p>This operation requires permissions to perform the
    ///          <code>rekognition:DeleteProject</code> action. </p>
    public func deleteProject(input: DeleteProjectInput, completion: @escaping (SdkResult<DeleteProjectOutput, DeleteProjectOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "deleteProject")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DeleteProjectInput, DeleteProjectOutput, DeleteProjectOutputError>(id: "deleteProject")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DeleteProjectInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DeleteProjectInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DeleteProjectInput, DeleteProjectOutput, DeleteProjectOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DeleteProjectInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DeleteProjectInput, DeleteProjectOutput, DeleteProjectOutputError>(xAmzTarget: "RekognitionService.DeleteProject"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Deletes an Amazon Rekognition Custom Labels model.  </p>
    ///          <p>You can't delete a model if it is running or if it is training.
    ///           To check the status of a model, use the <code>Status</code> field returned
    ///          from <a>DescribeProjectVersions</a>.
    ///          To stop a running model call <a>StopProjectVersion</a>. If the model
    ///       is training, wait until it finishes.</p>
    ///          <p>This operation requires permissions to perform the
    ///          <code>rekognition:DeleteProjectVersion</code> action. </p>
    public func deleteProjectVersion(input: DeleteProjectVersionInput, completion: @escaping (SdkResult<DeleteProjectVersionOutput, DeleteProjectVersionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "deleteProjectVersion")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DeleteProjectVersionInput, DeleteProjectVersionOutput, DeleteProjectVersionOutputError>(id: "deleteProjectVersion")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DeleteProjectVersionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DeleteProjectVersionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DeleteProjectVersionInput, DeleteProjectVersionOutput, DeleteProjectVersionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DeleteProjectVersionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DeleteProjectVersionInput, DeleteProjectVersionOutput, DeleteProjectVersionOutputError>(xAmzTarget: "RekognitionService.DeleteProjectVersion"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Deletes the stream processor identified by <code>Name</code>. You assign the value for <code>Name</code> when you create the stream processor with
    ///             <a>CreateStreamProcessor</a>. You might not be able to use the same name for a stream processor for a few seconds after calling <code>DeleteStreamProcessor</code>.</p>
    public func deleteStreamProcessor(input: DeleteStreamProcessorInput, completion: @escaping (SdkResult<DeleteStreamProcessorOutput, DeleteStreamProcessorOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "deleteStreamProcessor")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DeleteStreamProcessorInput, DeleteStreamProcessorOutput, DeleteStreamProcessorOutputError>(id: "deleteStreamProcessor")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DeleteStreamProcessorInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DeleteStreamProcessorInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DeleteStreamProcessorInput, DeleteStreamProcessorOutput, DeleteStreamProcessorOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DeleteStreamProcessorInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DeleteStreamProcessorInput, DeleteStreamProcessorOutput, DeleteStreamProcessorOutputError>(xAmzTarget: "RekognitionService.DeleteStreamProcessor"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Describes the specified collection. You can use <code>DescribeCollection</code> to get
    ///          information, such as the number of faces indexed into a collection and the version of the
    ///          model used by the collection for face detection.</p>
    ///
    ///          <p>For more information, see Describing a Collection in the
    ///      Amazon Rekognition Developer Guide.</p>
    public func describeCollection(input: DescribeCollectionInput, completion: @escaping (SdkResult<DescribeCollectionOutput, DescribeCollectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "describeCollection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DescribeCollectionInput, DescribeCollectionOutput, DescribeCollectionOutputError>(id: "describeCollection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DescribeCollectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DescribeCollectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DescribeCollectionInput, DescribeCollectionOutput, DescribeCollectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DescribeCollectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DescribeCollectionInput, DescribeCollectionOutput, DescribeCollectionOutputError>(xAmzTarget: "RekognitionService.DescribeCollection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Lists and describes the models in an Amazon Rekognition Custom Labels project. You
    ///          can specify up to 10 model versions in <code>ProjectVersionArns</code>. If
    ///          you don't specify a value, descriptions for all models are returned.</p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:DescribeProjectVersions</code>
    ///             action.</p>
    public func describeProjectVersions(input: DescribeProjectVersionsInput, completion: @escaping (SdkResult<DescribeProjectVersionsOutput, DescribeProjectVersionsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "describeProjectVersions")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DescribeProjectVersionsInput, DescribeProjectVersionsOutput, DescribeProjectVersionsOutputError>(id: "describeProjectVersions")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DescribeProjectVersionsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DescribeProjectVersionsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DescribeProjectVersionsInput, DescribeProjectVersionsOutput, DescribeProjectVersionsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DescribeProjectVersionsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DescribeProjectVersionsInput, DescribeProjectVersionsOutput, DescribeProjectVersionsOutputError>(xAmzTarget: "RekognitionService.DescribeProjectVersions"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Lists and gets information about your Amazon Rekognition Custom Labels projects.</p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:DescribeProjects</code> action.</p>
    public func describeProjects(input: DescribeProjectsInput, completion: @escaping (SdkResult<DescribeProjectsOutput, DescribeProjectsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "describeProjects")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DescribeProjectsInput, DescribeProjectsOutput, DescribeProjectsOutputError>(id: "describeProjects")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DescribeProjectsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DescribeProjectsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DescribeProjectsInput, DescribeProjectsOutput, DescribeProjectsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DescribeProjectsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DescribeProjectsInput, DescribeProjectsOutput, DescribeProjectsOutputError>(xAmzTarget: "RekognitionService.DescribeProjects"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Provides information about a stream processor created by <a>CreateStreamProcessor</a>. You can get information about the input and output streams, the input parameters for the face recognition being performed,
    ///             and the current status of the stream processor.</p>
    public func describeStreamProcessor(input: DescribeStreamProcessorInput, completion: @escaping (SdkResult<DescribeStreamProcessorOutput, DescribeStreamProcessorOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "describeStreamProcessor")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DescribeStreamProcessorInput, DescribeStreamProcessorOutput, DescribeStreamProcessorOutputError>(id: "describeStreamProcessor")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DescribeStreamProcessorInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DescribeStreamProcessorInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DescribeStreamProcessorInput, DescribeStreamProcessorOutput, DescribeStreamProcessorOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DescribeStreamProcessorInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DescribeStreamProcessorInput, DescribeStreamProcessorOutput, DescribeStreamProcessorOutputError>(xAmzTarget: "RekognitionService.DescribeStreamProcessor"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects custom labels in a supplied image by using an Amazon Rekognition Custom Labels model. </p>
    ///          <p>You specify which version of a model version to use by using the <code>ProjectVersionArn</code> input
    ///       parameter. </p>
    ///          <p>You pass the input image as base64-encoded image bytes or as a reference to an image in
    ///          an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition operations, passing
    ///          image bytes is not supported. The image must be either a PNG or JPEG formatted file. </p>
    ///          <p> For each object that the model version detects on an image, the API returns a
    ///          (<code>CustomLabel</code>) object in an array (<code>CustomLabels</code>).
    ///          Each <code>CustomLabel</code> object provides the label name (<code>Name</code>), the level
    ///          of confidence that the image contains the object (<code>Confidence</code>), and
    ///          object location information, if it exists,  for the label on the image (<code>Geometry</code>). </p>
    ///          <p>During training model calculates a threshold value that determines
    ///          if a prediction for a label is true. By default, <code>DetectCustomLabels</code> doesn't
    ///          return labels whose confidence value is below the model's calculated threshold value.  To filter
    ///          labels that are returned, specify a value for <code>MinConfidence</code> that is higher than the
    ///          model's calculated threshold. You can get the model's calculated threshold from the model's
    ///          training results shown in the Amazon Rekognition Custom Labels console.
    ///          To get all labels, regardless of confidence, specify a <code>MinConfidence</code>
    ///          value of 0. </p>
    ///          <p>You can also add the <code>MaxResults</code> parameter
    ///            to limit the number of labels returned. </p>
    ///
    ///          <p>This is a stateless API operation. That is, the operation does not persist any
    ///          data.</p>
    ///          <p>This operation requires permissions to perform the
    ///          <code>rekognition:DetectCustomLabels</code> action. </p>
    public func detectCustomLabels(input: DetectCustomLabelsInput, completion: @escaping (SdkResult<DetectCustomLabelsOutput, DetectCustomLabelsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "detectCustomLabels")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DetectCustomLabelsInput, DetectCustomLabelsOutput, DetectCustomLabelsOutputError>(id: "detectCustomLabels")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DetectCustomLabelsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DetectCustomLabelsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DetectCustomLabelsInput, DetectCustomLabelsOutput, DetectCustomLabelsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DetectCustomLabelsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DetectCustomLabelsInput, DetectCustomLabelsOutput, DetectCustomLabelsOutputError>(xAmzTarget: "RekognitionService.DetectCustomLabels"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects faces within an image that is provided as input.</p>
    ///
    ///          <p>
    ///             <code>DetectFaces</code> detects the 100 largest faces in the image. For each face
    ///       detected, the operation returns face details. These details include a bounding box of the
    ///       face, a confidence value (that the bounding box contains a face), and a fixed set of
    ///       attributes such as facial landmarks (for example, coordinates of eye and mouth),
    ///       presence of beard, sunglasses, and so on. </p>
    ///          <p>The face-detection algorithm is most effective on frontal faces. For non-frontal or
    ///       obscured faces, the algorithm might not detect the faces or might detect faces with lower
    ///       confidence. </p>
    ///          <p>You pass the input image either as base64-encoded image bytes or as a reference to an
    ///       image in an Amazon S3 bucket. If you use the AWS CLI
    ///        to call Amazon Rekognition operations, passing image bytes is not
    ///       supported. The image must be either a PNG or JPEG formatted file. </p>
    ///
    ///          <note>
    ///             <p>This is a stateless API operation. That is, the operation does not persist any
    ///         data.</p>
    ///          </note>
    ///
    ///          <p>This operation requires permissions to perform the
    ///       <code>rekognition:DetectFaces</code> action. </p>
    public func detectFaces(input: DetectFacesInput, completion: @escaping (SdkResult<DetectFacesOutput, DetectFacesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "detectFaces")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DetectFacesInput, DetectFacesOutput, DetectFacesOutputError>(id: "detectFaces")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DetectFacesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DetectFacesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DetectFacesInput, DetectFacesOutput, DetectFacesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DetectFacesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DetectFacesInput, DetectFacesOutput, DetectFacesOutputError>(xAmzTarget: "RekognitionService.DetectFaces"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects instances of real-world entities within an image (JPEG or PNG)
    ///        provided as input. This includes objects like flower, tree, and table; events like
    ///        wedding, graduation, and birthday party; and concepts like landscape, evening, and nature.
    ///      </p>
    ///
    ///          <p>For an example, see Analyzing Images Stored in an Amazon S3 Bucket in the Amazon Rekognition Developer Guide.</p>
    ///          <note>
    ///
    ///             <p>
    ///                <code>DetectLabels</code> does not support the detection of activities. However, activity detection
    ///         is supported for label detection in videos. For more information, see StartLabelDetection in the Amazon Rekognition Developer Guide.</p>
    ///          </note>
    ///
    ///          <p>You pass the input image as base64-encoded image bytes or as a reference to an image in
    ///       an Amazon S3 bucket. If you use the
    ///       AWS
    ///       CLI to call Amazon Rekognition operations, passing image bytes is not
    ///       supported. The image must be either a PNG or JPEG formatted file. </p>
    ///          <p> For each object, scene, and concept the API returns one or more labels. Each label
    ///       provides the object name, and the level of confidence that the image contains the object. For
    ///       example, suppose the input image has a lighthouse, the sea, and a rock. The response includes
    ///       all three labels, one for each object. </p>
    ///
    ///          <p>
    ///             <code>{Name: lighthouse, Confidence: 98.4629}</code>
    ///          </p>
    ///          <p>
    ///             <code>{Name: rock,Confidence: 79.2097}</code>
    ///          </p>
    ///          <p>
    ///             <code> {Name: sea,Confidence: 75.061}</code>
    ///          </p>
    ///          <p>In the preceding example, the operation returns one label for each of the three
    ///       objects. The operation can also return multiple labels for the same object in the image. For
    ///       example, if the input image shows a flower (for example, a tulip), the operation might return
    ///       the following three labels. </p>
    ///          <p>
    ///             <code>{Name: flower,Confidence: 99.0562}</code>
    ///          </p>
    ///          <p>
    ///             <code>{Name: plant,Confidence: 99.0562}</code>
    ///          </p>
    ///          <p>
    ///             <code>{Name: tulip,Confidence: 99.0562}</code>
    ///          </p>
    ///
    ///          <p>In this example, the detection algorithm more precisely identifies the flower as a
    ///       tulip.</p>
    ///          <p>In response, the API returns an array of labels. In addition, the response also
    ///       includes the orientation correction. Optionally, you can specify <code>MinConfidence</code> to
    ///       control the confidence threshold for the labels returned. The default is 55%. You can also add
    ///       the <code>MaxLabels</code> parameter to limit the number of labels returned. </p>
    ///          <note>
    ///             <p>If the object detected is a person, the operation doesn't provide the same facial
    ///         details that the <a>DetectFaces</a> operation provides.</p>
    ///          </note>
    ///          <p>
    ///             <code>DetectLabels</code> returns bounding boxes for instances of common object labels in an array of
    ///       <a>Instance</a> objects. An <code>Instance</code> object contains a
    ///       <a>BoundingBox</a> object, for the location of the label on the image. It also includes
    ///       the confidence by which the bounding box was detected.</p>
    ///          <p>
    ///             <code>DetectLabels</code> also returns a hierarchical taxonomy of detected labels. For example,
    ///       a detected car might be assigned the label <i>car</i>. The label <i>car</i>
    ///       has two parent labels: <i>Vehicle</i> (its parent) and <i>Transportation</i> (its
    ///       grandparent).
    ///       The response returns the entire list of ancestors for a label. Each ancestor is a unique label in the response.
    ///       In the previous example, <i>Car</i>, <i>Vehicle</i>, and <i>Transportation</i>
    ///       are returned as unique labels in the response.
    ///       </p>
    ///          <p>This is a stateless API operation. That is, the operation does not persist any
    ///       data.</p>
    ///          <p>This operation requires permissions to perform the
    ///         <code>rekognition:DetectLabels</code> action. </p>
    public func detectLabels(input: DetectLabelsInput, completion: @escaping (SdkResult<DetectLabelsOutput, DetectLabelsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "detectLabels")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DetectLabelsInput, DetectLabelsOutput, DetectLabelsOutputError>(id: "detectLabels")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DetectLabelsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DetectLabelsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DetectLabelsInput, DetectLabelsOutput, DetectLabelsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DetectLabelsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DetectLabelsInput, DetectLabelsOutput, DetectLabelsOutputError>(xAmzTarget: "RekognitionService.DetectLabels"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects unsafe content in a specified JPEG or PNG format image.
    ///      Use <code>DetectModerationLabels</code> to moderate images depending on your requirements.
    ///      For example, you might want to filter images that contain nudity, but not images containing
    ///      suggestive content.</p>
    ///          <p>To filter images, use the labels returned by <code>DetectModerationLabels</code>
    ///      to determine which types of content are appropriate.</p>
    ///
    ///          <p>For information about moderation labels,
    ///       see Detecting Unsafe Content in the Amazon Rekognition Developer Guide.</p>
    ///          <p>You pass the input image either as base64-encoded image bytes or as a reference to an
    ///       image in an Amazon S3 bucket. If you use the
    ///       AWS
    ///       CLI to call Amazon Rekognition operations, passing image bytes is not
    ///       supported. The image must be either a PNG or JPEG formatted file. </p>
    public func detectModerationLabels(input: DetectModerationLabelsInput, completion: @escaping (SdkResult<DetectModerationLabelsOutput, DetectModerationLabelsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "detectModerationLabels")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DetectModerationLabelsInput, DetectModerationLabelsOutput, DetectModerationLabelsOutputError>(id: "detectModerationLabels")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DetectModerationLabelsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DetectModerationLabelsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DetectModerationLabelsInput, DetectModerationLabelsOutput, DetectModerationLabelsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DetectModerationLabelsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DetectModerationLabelsInput, DetectModerationLabelsOutput, DetectModerationLabelsOutputError>(xAmzTarget: "RekognitionService.DetectModerationLabels"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects Personal Protective Equipment (PPE) worn by people detected in an image. Amazon Rekognition can detect the
    ///          following types of PPE.</p>
    ///          <ul>
    ///             <li>
    ///                <p>Face cover</p>
    ///             </li>
    ///             <li>
    ///                <p>Hand cover</p>
    ///             </li>
    ///             <li>
    ///                <p>Head cover</p>
    ///             </li>
    ///          </ul>
    ///
    ///          <p>You pass the input image as base64-encoded image bytes or as a reference to an image in an Amazon S3 bucket.
    ///          The image must be either a PNG or JPG formatted file. </p>
    ///
    ///          <p>
    ///             <code>DetectProtectiveEquipment</code> detects PPE worn by up to 15 persons detected in an image.</p>
    ///          <p>For each person detected in the image the API returns an array of body parts (face, head, left-hand, right-hand).
    ///          For each body part, an array of detected items of PPE is returned, including an indicator of whether or not the PPE
    ///          covers the body part. The API returns the confidence it has in each detection
    ///          (person, PPE, body part and body part coverage). It also returns a bounding box (<a>BoundingBox</a>) for each detected
    ///          person and each detected item of PPE. </p>
    ///          <p>You can optionally request a summary of detected PPE items with the <code>SummarizationAttributes</code> input parameter.
    ///          The summary provides the following information. </p>
    ///          <ul>
    ///             <li>
    ///                <p>The persons detected as wearing all of the types of PPE that you specify.</p>
    ///             </li>
    ///             <li>
    ///                <p>The persons detected as not wearing all of the types PPE that you specify.</p>
    ///             </li>
    ///             <li>
    ///                <p>The persons detected where PPE adornment could not be determined. </p>
    ///             </li>
    ///          </ul>
    ///          <p>This is a stateless API operation. That is, the operation does not persist any data.</p>
    ///
    ///          <p>This operation requires permissions to perform the <code>rekognition:DetectProtectiveEquipment</code> action. </p>
    public func detectProtectiveEquipment(input: DetectProtectiveEquipmentInput, completion: @escaping (SdkResult<DetectProtectiveEquipmentOutput, DetectProtectiveEquipmentOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "detectProtectiveEquipment")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DetectProtectiveEquipmentInput, DetectProtectiveEquipmentOutput, DetectProtectiveEquipmentOutputError>(id: "detectProtectiveEquipment")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DetectProtectiveEquipmentInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DetectProtectiveEquipmentInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DetectProtectiveEquipmentInput, DetectProtectiveEquipmentOutput, DetectProtectiveEquipmentOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DetectProtectiveEquipmentInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DetectProtectiveEquipmentInput, DetectProtectiveEquipmentOutput, DetectProtectiveEquipmentOutputError>(xAmzTarget: "RekognitionService.DetectProtectiveEquipment"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects text in the input image and converts it into machine-readable text.</p>
    ///          <p>Pass the input image as base64-encoded image bytes or as a reference to an image in an
    ///       Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition operations, you must pass it as a
    ///       reference to an image in an Amazon S3 bucket. For the AWS CLI, passing image bytes is not
    ///       supported. The image must be either a .png or .jpeg formatted file. </p>
    ///          <p>The <code>DetectText</code> operation returns text in an array of <a>TextDetection</a> elements, <code>TextDetections</code>. Each
    ///         <code>TextDetection</code> element provides information about a single word or line of text
    ///       that was detected in the image. </p>
    ///          <p>A word is one or more ISO basic latin script characters that are not separated by spaces.
    ///         <code>DetectText</code> can detect up to 50 words in an image.</p>
    ///          <p>A line is a string of equally spaced words. A line isn't necessarily a complete
    ///       sentence. For example, a driver's license number is detected as a line. A line ends when there
    ///       is no aligned text after it. Also, a line ends when there is a large gap between words,
    ///       relative to the length of the words. This means, depending on the gap between words, Amazon Rekognition
    ///       may detect multiple lines in text aligned in the same direction. Periods don't represent the
    ///       end of a line. If a sentence spans multiple lines, the <code>DetectText</code> operation
    ///       returns multiple lines.</p>
    ///          <p>To determine whether a <code>TextDetection</code> element is a line of text or a word,
    ///       use the <code>TextDetection</code> object <code>Type</code> field. </p>
    ///          <p>To be detected, text must be within +/- 90 degrees orientation of the horizontal axis.</p>
    ///
    ///          <p>For more information, see DetectText in the Amazon Rekognition Developer Guide.</p>
    public func detectText(input: DetectTextInput, completion: @escaping (SdkResult<DetectTextOutput, DetectTextOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "detectText")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<DetectTextInput, DetectTextOutput, DetectTextOutputError>(id: "detectText")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: DetectTextInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: DetectTextInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<DetectTextInput, DetectTextOutput, DetectTextOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: DetectTextInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<DetectTextInput, DetectTextOutput, DetectTextOutputError>(xAmzTarget: "RekognitionService.DetectText"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the name and additional information about a celebrity based on his or her
    ///       Amazon Rekognition ID. The additional information is returned as an array of URLs. If there is no
    ///       additional information about the celebrity, this list is empty.</p>
    ///
    ///          <p>For more information, see Recognizing Celebrities in an Image in
    ///       the Amazon Rekognition Developer Guide.</p>
    ///          <p>This operation requires permissions to perform the
    ///         <code>rekognition:GetCelebrityInfo</code> action. </p>
    public func getCelebrityInfo(input: GetCelebrityInfoInput, completion: @escaping (SdkResult<GetCelebrityInfoOutput, GetCelebrityInfoOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getCelebrityInfo")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetCelebrityInfoInput, GetCelebrityInfoOutput, GetCelebrityInfoOutputError>(id: "getCelebrityInfo")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetCelebrityInfoInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetCelebrityInfoInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetCelebrityInfoInput, GetCelebrityInfoOutput, GetCelebrityInfoOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetCelebrityInfoInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetCelebrityInfoInput, GetCelebrityInfoOutput, GetCelebrityInfoOutputError>(xAmzTarget: "RekognitionService.GetCelebrityInfo"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the celebrity recognition results for a Amazon Rekognition Video analysis started by
    ///      <a>StartCelebrityRecognition</a>.</p>
    ///          <p>Celebrity recognition in a video is an asynchronous operation. Analysis is started by a call
    ///       to <a>StartCelebrityRecognition</a>  which returns a job identifier (<code>JobId</code>).
    ///       When the celebrity recognition operation finishes, Amazon Rekognition Video publishes a completion status to the Amazon Simple Notification Service
    ///       topic registered in the initial call to <code>StartCelebrityRecognition</code>.
    ///       To get the results of the celebrity recognition analysis, first check that the status value published to the Amazon SNS
    ///       topic is <code>SUCCEEDED</code>. If so, call  <code>GetCelebrityDetection</code> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartCelebrityDetection</code>. </p>
    ///
    ///          <p>For more information, see Working With Stored Videos in the Amazon Rekognition Developer Guide.</p>
    ///          <p>
    ///             <code>GetCelebrityRecognition</code> returns detected celebrities and the time(s) they are detected in an array
    ///       (<code>Celebrities</code>) of <a>CelebrityRecognition</a>
    ///
    ///       objects. Each <code>CelebrityRecognition</code> contains information about the celebrity in a <a>CelebrityDetail</a>
    ///       object and the time, <code>Timestamp</code>, the celebrity was detected.
    ///       </p>
    ///          <note>
    ///
    ///             <p>
    ///                <code>GetCelebrityRecognition</code> only returns the default
    ///         facial attributes (<code>BoundingBox</code>, <code>Confidence</code>,
    ///         <code>Landmarks</code>, <code>Pose</code>, and <code>Quality</code>). The other facial attributes listed
    ///         in the <code>Face</code> object of the following response syntax are not returned. For more information,
    ///         see FaceDetail in the Amazon Rekognition Developer Guide. </p>
    ///          </note>
    ///          <p>By default, the <code>Celebrities</code> array is sorted by time (milliseconds from the start of the video).
    ///       You can also sort the array by celebrity by specifying the value <code>ID</code> in the <code>SortBy</code> input parameter.</p>
    ///          <p>The <code>CelebrityDetail</code> object includes the celebrity identifer and additional information urls. If you don't store
    ///       the additional information urls, you can get them later by calling <a>GetCelebrityInfo</a> with the celebrity identifer.</p>
    ///          <p>No information is returned for faces not recognized as celebrities.</p>
    ///          <p>Use MaxResults parameter to limit the number of labels returned. If there are more results than
    ///       specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains a
    ///       pagination token for getting the next set of results. To get the next page of results, call <code>GetCelebrityDetection</code>
    ///       and populate the <code>NextToken</code> request parameter with the token
    ///       value returned from the previous call to <code>GetCelebrityRecognition</code>.</p>
    public func getCelebrityRecognition(input: GetCelebrityRecognitionInput, completion: @escaping (SdkResult<GetCelebrityRecognitionOutput, GetCelebrityRecognitionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getCelebrityRecognition")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetCelebrityRecognitionInput, GetCelebrityRecognitionOutput, GetCelebrityRecognitionOutputError>(id: "getCelebrityRecognition")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetCelebrityRecognitionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetCelebrityRecognitionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetCelebrityRecognitionInput, GetCelebrityRecognitionOutput, GetCelebrityRecognitionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetCelebrityRecognitionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetCelebrityRecognitionInput, GetCelebrityRecognitionOutput, GetCelebrityRecognitionOutputError>(xAmzTarget: "RekognitionService.GetCelebrityRecognition"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the unsafe content analysis results for a Amazon Rekognition Video analysis started by
    ///        <a>StartContentModeration</a>.</p>
    ///
    ///          <p>Unsafe content analysis of a video is an asynchronous operation. You start analysis by calling
    ///        <a>StartContentModeration</a> which returns a job identifier (<code>JobId</code>).
    ///        When analysis finishes, Amazon Rekognition Video publishes a completion status to the Amazon Simple Notification Service
    ///        topic registered in the initial call to <code>StartContentModeration</code>.
    ///        To get the results of the unsafe content analysis, first check that the status value published to the Amazon SNS
    ///        topic is <code>SUCCEEDED</code>. If so, call  <code>GetContentModeration</code> and pass the job identifier
    ///        (<code>JobId</code>) from the initial call to <code>StartContentModeration</code>. </p>
    ///
    ///          <p>For more information, see Working with Stored Videos in the
    ///      Amazon Rekognition Devlopers Guide.</p>
    ///          <p>
    ///             <code>GetContentModeration</code> returns detected unsafe content labels,
    ///       and the time they are detected, in an array, <code>ModerationLabels</code>, of
    ///       <a>ContentModerationDetection</a> objects.
    ///      </p>
    ///          <p>By default, the moderated labels are returned sorted by time, in milliseconds from the start of the
    ///        video. You can also sort them by moderated label by specifying <code>NAME</code> for the <code>SortBy</code>
    ///        input parameter. </p>
    ///          <p>Since video analysis can return a large number of results, use the <code>MaxResults</code> parameter to limit
    ///       the number of labels returned in a single call to <code>GetContentModeration</code>. If there are more results than
    ///        specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains a
    ///        pagination token for getting the next set of results. To get the next page of results, call <code>GetContentModeration</code>
    ///        and populate the <code>NextToken</code> request parameter with the value of <code>NextToken</code>
    ///        returned from the previous call to <code>GetContentModeration</code>.</p>
    ///
    ///          <p>For more information, see Detecting Unsafe Content in the Amazon Rekognition Developer Guide.</p>
    public func getContentModeration(input: GetContentModerationInput, completion: @escaping (SdkResult<GetContentModerationOutput, GetContentModerationOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getContentModeration")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetContentModerationInput, GetContentModerationOutput, GetContentModerationOutputError>(id: "getContentModeration")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetContentModerationInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetContentModerationInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetContentModerationInput, GetContentModerationOutput, GetContentModerationOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetContentModerationInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetContentModerationInput, GetContentModerationOutput, GetContentModerationOutputError>(xAmzTarget: "RekognitionService.GetContentModeration"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets face detection results for a Amazon Rekognition Video analysis started by <a>StartFaceDetection</a>.</p>
    ///          <p>Face detection with Amazon Rekognition Video is an asynchronous operation. You start face detection by calling <a>StartFaceDetection</a>
    ///      which returns a job identifier (<code>JobId</code>). When the face detection operation finishes, Amazon Rekognition Video publishes a completion status to
    ///      the Amazon Simple Notification Service topic registered in the initial call to <code>StartFaceDetection</code>. To get the results
    ///      of the face detection operation, first check that the status value published to the Amazon SNS topic is <code>SUCCEEDED</code>.
    ///      If so, call  <a>GetFaceDetection</a> and pass the job identifier
    ///      (<code>JobId</code>) from the initial call to <code>StartFaceDetection</code>.</p>
    ///          <p>
    ///             <code>GetFaceDetection</code> returns an array of detected faces (<code>Faces</code>) sorted by the time the faces were detected. </p>
    ///          <p>Use MaxResults parameter to limit the number of labels returned. If there are more results than
    ///    specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains a pagination token for getting the next set
    ///    of results. To get the next page of results, call <code>GetFaceDetection</code> and populate the <code>NextToken</code> request parameter with the token
    ///     value returned from the previous call to <code>GetFaceDetection</code>.</p>
    public func getFaceDetection(input: GetFaceDetectionInput, completion: @escaping (SdkResult<GetFaceDetectionOutput, GetFaceDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getFaceDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetFaceDetectionInput, GetFaceDetectionOutput, GetFaceDetectionOutputError>(id: "getFaceDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetFaceDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetFaceDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetFaceDetectionInput, GetFaceDetectionOutput, GetFaceDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetFaceDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetFaceDetectionInput, GetFaceDetectionOutput, GetFaceDetectionOutputError>(xAmzTarget: "RekognitionService.GetFaceDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the face search results for Amazon Rekognition Video face search started by
    ///       <a>StartFaceSearch</a>. The search returns faces in a collection that match the faces
    ///     of persons detected in a video. It also includes the time(s) that faces are matched in the video.</p>
    ///          <p>Face search in a video is an asynchronous operation. You start face search by calling
    ///       to <a>StartFaceSearch</a> which returns a job identifier (<code>JobId</code>).
    ///       When the search operation finishes, Amazon Rekognition Video publishes a completion status to the Amazon Simple Notification Service
    ///       topic registered in the initial call to <code>StartFaceSearch</code>.
    ///       To get the search results, first check that the status value published to the Amazon SNS
    ///       topic is <code>SUCCEEDED</code>. If so, call  <code>GetFaceSearch</code> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartFaceSearch</code>.</p>
    ///
    ///          <p>For more information, see Searching Faces in a Collection in the
    ///       Amazon Rekognition Developer Guide.</p>
    ///          <p>The search results are retured in an array, <code>Persons</code>, of
    ///     <a>PersonMatch</a> objects. Each<code>PersonMatch</code> element contains
    ///     details about the matching faces in the input collection, person information (facial attributes,
    ///     bounding boxes, and person identifer)
    ///     for the matched person, and the time the person was matched in the video.</p>
    ///          <note>
    ///
    ///             <p>
    ///                <code>GetFaceSearch</code> only returns the default
    ///         facial attributes (<code>BoundingBox</code>, <code>Confidence</code>,
    ///         <code>Landmarks</code>, <code>Pose</code>, and <code>Quality</code>). The other facial attributes listed
    ///         in the <code>Face</code> object of the following response syntax are not returned. For more information,
    ///         see FaceDetail in the Amazon Rekognition Developer Guide. </p>
    ///          </note>
    ///
    ///          <p>By default, the <code>Persons</code> array is sorted by the time, in milliseconds from the
    ///     start of the video, persons are matched.
    ///     You can also sort by persons by specifying <code>INDEX</code> for the <code>SORTBY</code> input
    ///     parameter.</p>
    public func getFaceSearch(input: GetFaceSearchInput, completion: @escaping (SdkResult<GetFaceSearchOutput, GetFaceSearchOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getFaceSearch")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetFaceSearchInput, GetFaceSearchOutput, GetFaceSearchOutputError>(id: "getFaceSearch")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetFaceSearchInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetFaceSearchInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetFaceSearchInput, GetFaceSearchOutput, GetFaceSearchOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetFaceSearchInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetFaceSearchInput, GetFaceSearchOutput, GetFaceSearchOutputError>(xAmzTarget: "RekognitionService.GetFaceSearch"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the label detection results of a Amazon Rekognition Video analysis started by <a>StartLabelDetection</a>.  </p>
    ///
    ///          <p>The label detection operation is started by a call to <a>StartLabelDetection</a>
    ///       which returns a job identifier (<code>JobId</code>). When the label detection operation finishes, Amazon Rekognition publishes a completion status to
    ///       the Amazon Simple Notification Service topic registered in the initial call to <code>StartlabelDetection</code>. To get the results
    ///       of the label detection operation, first check that the status value published to the Amazon SNS topic is <code>SUCCEEDED</code>.
    ///       If so, call  <a>GetLabelDetection</a> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartLabelDetection</code>.</p>
    ///          <p>
    ///             <code>GetLabelDetection</code> returns an array of detected labels (<code>Labels</code>) sorted by the time
    ///        the labels were detected. You can also sort by the label name by specifying <code>NAME</code> for the
    ///        <code>SortBy</code> input parameter.</p>
    ///          <p>The labels returned include the label name, the percentage confidence in the accuracy of the detected label,
    ///         and the time the label was detected in the video.</p>
    ///          <p>The returned labels also include bounding box information for common objects, a
    ///        hierarchical taxonomy of detected labels, and the version of the label model used for detection.</p>
    ///
    ///          <p>Use MaxResults parameter to limit the number of labels returned. If there are more results than
    ///     specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains a pagination token for getting the next set
    ///     of results. To get the next page of results, call <code>GetlabelDetection</code> and populate the <code>NextToken</code> request parameter with the token
    ///      value returned from the previous call to <code>GetLabelDetection</code>.</p>
    public func getLabelDetection(input: GetLabelDetectionInput, completion: @escaping (SdkResult<GetLabelDetectionOutput, GetLabelDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getLabelDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetLabelDetectionInput, GetLabelDetectionOutput, GetLabelDetectionOutputError>(id: "getLabelDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetLabelDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetLabelDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetLabelDetectionInput, GetLabelDetectionOutput, GetLabelDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetLabelDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetLabelDetectionInput, GetLabelDetectionOutput, GetLabelDetectionOutputError>(xAmzTarget: "RekognitionService.GetLabelDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the path tracking results of a Amazon Rekognition Video analysis started by <a>StartPersonTracking</a>.</p>
    ///
    ///          <p>The person path tracking operation is started by a call to <code>StartPersonTracking</code>
    ///      which returns a job identifier (<code>JobId</code>). When the operation finishes, Amazon Rekognition Video publishes a completion status to
    ///      the Amazon Simple Notification Service topic registered in the initial call to <code>StartPersonTracking</code>.</p>
    ///          <p>To get the results of the person path tracking operation, first check
    ///        that the status value published to the Amazon SNS topic is <code>SUCCEEDED</code>.
    ///      If so, call  <a>GetPersonTracking</a> and pass the job identifier
    ///      (<code>JobId</code>) from the initial call to <code>StartPersonTracking</code>.</p>
    ///          <p>
    ///             <code>GetPersonTracking</code> returns an array, <code>Persons</code>, of tracked persons and the time(s) their
    ///        paths were tracked in the video. </p>
    ///          <note>
    ///             <p>
    ///                <code>GetPersonTracking</code> only returns the default
    ///        facial attributes (<code>BoundingBox</code>, <code>Confidence</code>,
    ///        <code>Landmarks</code>, <code>Pose</code>, and <code>Quality</code>). The other facial attributes listed
    ///        in the <code>Face</code> object of the following response syntax are not returned. </p>
    ///
    ///             <p>For more information, see FaceDetail in the Amazon Rekognition Developer Guide.</p>
    ///          </note>
    ///
    ///
    ///          <p>By default, the array is sorted by the time(s) a person's path is tracked in the video.
    ///       You can sort by tracked persons by specifying <code>INDEX</code> for the <code>SortBy</code> input parameter.</p>
    ///
    ///          <p>Use the <code>MaxResults</code> parameter to limit the number of items returned. If there are more results than
    ///    specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains a pagination token for getting the next set
    ///    of results. To get the next page of results, call <code>GetPersonTracking</code> and populate the <code>NextToken</code> request parameter with the token
    ///     value returned from the previous call to <code>GetPersonTracking</code>.</p>
    public func getPersonTracking(input: GetPersonTrackingInput, completion: @escaping (SdkResult<GetPersonTrackingOutput, GetPersonTrackingOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getPersonTracking")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetPersonTrackingInput, GetPersonTrackingOutput, GetPersonTrackingOutputError>(id: "getPersonTracking")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetPersonTrackingInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetPersonTrackingInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetPersonTrackingInput, GetPersonTrackingOutput, GetPersonTrackingOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetPersonTrackingInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetPersonTrackingInput, GetPersonTrackingOutput, GetPersonTrackingOutputError>(xAmzTarget: "RekognitionService.GetPersonTracking"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the segment detection results of a Amazon Rekognition Video analysis started by <a>StartSegmentDetection</a>.</p>
    ///          <p>Segment detection with Amazon Rekognition Video is an asynchronous operation. You start segment detection by
    ///       calling <a>StartSegmentDetection</a> which returns a job identifier (<code>JobId</code>).
    ///       When the segment detection operation finishes, Amazon Rekognition publishes a completion status to the Amazon Simple Notification Service
    ///       topic registered in the initial call to <code>StartSegmentDetection</code>. To get the results
    ///       of the segment detection operation, first check that the status value published to the Amazon SNS topic is <code>SUCCEEDED</code>.
    ///       if so, call <code>GetSegmentDetection</code> and pass the job identifier (<code>JobId</code>) from the initial call
    ///       of <code>StartSegmentDetection</code>.</p>
    ///          <p>
    ///             <code>GetSegmentDetection</code> returns detected segments in an array (<code>Segments</code>)
    ///       of <a>SegmentDetection</a> objects. <code>Segments</code> is sorted by the segment types
    ///       specified in the <code>SegmentTypes</code> input parameter of <code>StartSegmentDetection</code>.
    ///     Each element of the array includes the detected segment, the precentage confidence in the acuracy
    ///       of the detected segment, the type of the segment, and the frame in which the segment was detected.</p>
    ///          <p>Use <code>SelectedSegmentTypes</code> to find out the type of segment detection requested in the
    ///     call to <code>StartSegmentDetection</code>.</p>
    ///          <p>Use the <code>MaxResults</code> parameter to limit the number of segment detections returned. If there are more results than
    ///       specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains
    ///       a pagination token for getting the next set of results. To get the next page of results, call <code>GetSegmentDetection</code>
    ///       and populate the <code>NextToken</code> request parameter with the token value returned from the previous
    ///       call to <code>GetSegmentDetection</code>.</p>
    ///
    ///          <p>For more information, see Detecting Video Segments in Stored Video in the Amazon Rekognition Developer Guide.</p>
    public func getSegmentDetection(input: GetSegmentDetectionInput, completion: @escaping (SdkResult<GetSegmentDetectionOutput, GetSegmentDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getSegmentDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetSegmentDetectionInput, GetSegmentDetectionOutput, GetSegmentDetectionOutputError>(id: "getSegmentDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetSegmentDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetSegmentDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetSegmentDetectionInput, GetSegmentDetectionOutput, GetSegmentDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetSegmentDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetSegmentDetectionInput, GetSegmentDetectionOutput, GetSegmentDetectionOutputError>(xAmzTarget: "RekognitionService.GetSegmentDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets the text detection results of a Amazon Rekognition Video analysis started by <a>StartTextDetection</a>.</p>
    ///          <p>Text detection with Amazon Rekognition Video is an asynchronous operation. You start text detection by
    ///      calling <a>StartTextDetection</a> which returns a job identifier (<code>JobId</code>)
    ///      When the text detection operation finishes, Amazon Rekognition publishes a completion status to the Amazon Simple Notification Service
    ///      topic registered in the initial call to <code>StartTextDetection</code>. To get the results
    ///      of the text detection operation, first check that the status value published to the Amazon SNS topic is <code>SUCCEEDED</code>.
    ///      if so, call <code>GetTextDetection</code> and pass the job identifier (<code>JobId</code>) from the initial call
    ///      of <code>StartLabelDetection</code>.</p>
    ///          <p>
    ///             <code>GetTextDetection</code> returns an array of detected text (<code>TextDetections</code>) sorted by
    ///        the time the text was detected, up to 50 words per frame of video.</p>
    ///          <p>Each element of the array includes the detected text, the precentage confidence in the acuracy
    ///        of the detected text, the time the text was detected, bounding box information for where the text
    ///        was located, and unique identifiers for words and their lines.</p>
    ///          <p>Use MaxResults parameter to limit the number of text detections returned. If there are more results than
    ///      specified in <code>MaxResults</code>, the value of <code>NextToken</code> in the operation response contains
    ///      a pagination token for getting the next set of results. To get the next page of results, call <code>GetTextDetection</code>
    ///      and populate the <code>NextToken</code> request parameter with the token value returned from the previous
    ///      call to <code>GetTextDetection</code>.</p>
    public func getTextDetection(input: GetTextDetectionInput, completion: @escaping (SdkResult<GetTextDetectionOutput, GetTextDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "getTextDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<GetTextDetectionInput, GetTextDetectionOutput, GetTextDetectionOutputError>(id: "getTextDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: GetTextDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: GetTextDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<GetTextDetectionInput, GetTextDetectionOutput, GetTextDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: GetTextDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<GetTextDetectionInput, GetTextDetectionOutput, GetTextDetectionOutputError>(xAmzTarget: "RekognitionService.GetTextDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Detects faces in the input image and adds them to the specified collection. </p>
    ///          <p>Amazon Rekognition doesn't save the actual faces that are detected. Instead, the underlying
    ///       detection algorithm first detects the faces in the input image. For each face, the algorithm
    ///       extracts facial features into a feature vector, and stores it in the backend database.
    ///       Amazon Rekognition uses feature vectors when it performs face match and search operations using the
    ///         <a>SearchFaces</a> and <a>SearchFacesByImage</a>
    ///       operations.</p>
    ///
    ///          <p>For more information, see Adding Faces to a Collection in the Amazon Rekognition
    ///       Developer Guide.</p>
    ///          <p>To get the number of faces in a collection, call <a>DescribeCollection</a>. </p>
    ///
    ///          <p>If you're using version 1.0 of the face detection model, <code>IndexFaces</code>
    ///       indexes the 15 largest faces in the input image. Later versions of the face detection model
    ///       index the 100 largest faces in the input image. </p>
    ///          <p>If you're using version 4 or later of the face model, image orientation information
    ///      is not returned in the <code>OrientationCorrection</code> field. </p>
    ///          <p>To determine which version of the model you're using, call <a>DescribeCollection</a>
    ///       and supply the collection ID. You can also get the model version from the value of <code>FaceModelVersion</code> in the response
    ///       from <code>IndexFaces</code>
    ///          </p>
    ///
    ///          <p>For more information, see Model Versioning in the Amazon Rekognition Developer
    ///       Guide.</p>
    ///          <p>If you provide the optional <code>ExternalImageId</code> for the input image you
    ///       provided, Amazon Rekognition associates this ID with all faces that it detects. When you call the <a>ListFaces</a> operation, the response returns the external ID. You can use this
    ///       external image ID to create a client-side index to associate the faces with each image. You
    ///       can then use the index to find all faces in an image.</p>
    ///          <p>You can specify the maximum number of faces to index with the <code>MaxFaces</code> input
    ///       parameter. This is useful when you want to index the largest faces in an image and don't want to index
    ///       smaller faces, such as those belonging to people standing in the background.</p>
    ///          <p>The <code>QualityFilter</code> input parameter allows you to filter out detected faces
    ///       that donâ€™t meet a required quality bar. The quality bar is based on a
    ///       variety of common use cases. By default, <code>IndexFaces</code> chooses the quality bar that's
    ///       used to filter faces.  You can also explicitly choose
    ///       the quality bar. Use <code>QualityFilter</code>, to set the quality bar
    ///       by specifying <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>.
    ///       If you do not want to filter detected faces, specify <code>NONE</code>. </p>
    ///          <note>
    ///             <p>To use quality filtering, you need a collection associated with version 3 of the
    ///     face model or higher. To get the version of the face model associated with a collection, call
    ///       <a>DescribeCollection</a>. </p>
    ///          </note>
    ///          <p>Information about faces detected in an image, but not indexed, is returned in an array of
    ///       <a>UnindexedFace</a> objects, <code>UnindexedFaces</code>. Faces aren't
    ///       indexed for reasons such as:</p>
    ///          <ul>
    ///             <li>
    ///                <p>The number of faces detected exceeds the value of the <code>MaxFaces</code> request
    ///           parameter.</p>
    ///             </li>
    ///             <li>
    ///                <p>The face is too small compared to the image dimensions.</p>
    ///             </li>
    ///             <li>
    ///                <p>The face is too blurry.</p>
    ///             </li>
    ///             <li>
    ///                <p>The image is too dark.</p>
    ///             </li>
    ///             <li>
    ///                <p>The face has an extreme pose.</p>
    ///             </li>
    ///             <li>
    ///                <p>The face doesnâ€™t have enough detail to be suitable for face search.</p>
    ///             </li>
    ///          </ul>
    ///          <p>In response, the <code>IndexFaces</code> operation returns an array of metadata for
    ///       all detected faces, <code>FaceRecords</code>. This includes: </p>
    ///          <ul>
    ///             <li>
    ///                <p>The bounding box, <code>BoundingBox</code>, of the detected face. </p>
    ///             </li>
    ///             <li>
    ///                <p>A confidence value, <code>Confidence</code>, which indicates the confidence that the
    ///           bounding box contains a face.</p>
    ///             </li>
    ///             <li>
    ///                <p>A face ID, <code>FaceId</code>, assigned by the service for each face that's detected
    ///           and stored.</p>
    ///             </li>
    ///             <li>
    ///                <p>An image ID, <code>ImageId</code>, assigned by the service for the input image.</p>
    ///             </li>
    ///          </ul>
    ///          <p>If you request all facial attributes (by using the <code>detectionAttributes</code>
    ///       parameter), Amazon Rekognition returns detailed facial attributes, such as facial landmarks (for
    ///       example, location of eye and mouth) and other facial attributes. If you provide
    ///       the same image, specify the same collection, and use the same external ID in the
    ///         <code>IndexFaces</code> operation, Amazon Rekognition doesn't save duplicate face metadata.</p>
    ///
    ///
    ///          <p></p>
    ///
    ///
    ///          <p>The input image is passed either as base64-encoded image bytes, or as a reference to an
    ///       image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition operations,
    ///       passing image bytes isn't supported. The image must be formatted as a PNG or JPEG file. </p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:IndexFaces</code>
    ///       action.</p>
    public func indexFaces(input: IndexFacesInput, completion: @escaping (SdkResult<IndexFacesOutput, IndexFacesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "indexFaces")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<IndexFacesInput, IndexFacesOutput, IndexFacesOutputError>(id: "indexFaces")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: IndexFacesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: IndexFacesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<IndexFacesInput, IndexFacesOutput, IndexFacesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: IndexFacesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<IndexFacesInput, IndexFacesOutput, IndexFacesOutputError>(xAmzTarget: "RekognitionService.IndexFaces"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Returns list of collection IDs in your account.
    ///     If the result is truncated, the response also provides a <code>NextToken</code>
    ///     that you can use in the subsequent request to fetch the next set of collection IDs.</p>
    ///
    ///          <p>For an example, see Listing Collections in the Amazon Rekognition Developer Guide.</p>
    ///          <p>This operation requires permissions to perform the <code>rekognition:ListCollections</code> action.</p>
    public func listCollections(input: ListCollectionsInput, completion: @escaping (SdkResult<ListCollectionsOutput, ListCollectionsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "listCollections")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<ListCollectionsInput, ListCollectionsOutput, ListCollectionsOutputError>(id: "listCollections")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: ListCollectionsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ListCollectionsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<ListCollectionsInput, ListCollectionsOutput, ListCollectionsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: ListCollectionsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<ListCollectionsInput, ListCollectionsOutput, ListCollectionsOutputError>(xAmzTarget: "RekognitionService.ListCollections"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Returns metadata for faces in the specified collection.
    ///       This metadata includes information such as the bounding box coordinates, the confidence
    ///       (that the bounding box contains a face), and face ID. For an example, see Listing Faces in a Collection
    ///       in the Amazon Rekognition Developer Guide.</p>
    ///
    ///
    ///          <p>This operation requires permissions to perform the
    ///       <code>rekognition:ListFaces</code> action.</p>
    public func listFaces(input: ListFacesInput, completion: @escaping (SdkResult<ListFacesOutput, ListFacesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "listFaces")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<ListFacesInput, ListFacesOutput, ListFacesOutputError>(id: "listFaces")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: ListFacesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ListFacesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<ListFacesInput, ListFacesOutput, ListFacesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: ListFacesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<ListFacesInput, ListFacesOutput, ListFacesOutputError>(xAmzTarget: "RekognitionService.ListFaces"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Gets a list of stream processors that you have created with <a>CreateStreamProcessor</a>. </p>
    public func listStreamProcessors(input: ListStreamProcessorsInput, completion: @escaping (SdkResult<ListStreamProcessorsOutput, ListStreamProcessorsOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "listStreamProcessors")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<ListStreamProcessorsInput, ListStreamProcessorsOutput, ListStreamProcessorsOutputError>(id: "listStreamProcessors")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: ListStreamProcessorsInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ListStreamProcessorsInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<ListStreamProcessorsInput, ListStreamProcessorsOutput, ListStreamProcessorsOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: ListStreamProcessorsInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<ListStreamProcessorsInput, ListStreamProcessorsOutput, ListStreamProcessorsOutputError>(xAmzTarget: "RekognitionService.ListStreamProcessors"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>
    ///       Returns a list of tags in an Amazon Rekognition collection, stream processor, or Custom Labels model.
    ///     </p>
    ///          <p>This operation requires permissions to perform the
    ///       <code>rekognition:ListTagsForResource</code> action. </p>
    public func listTagsForResource(input: ListTagsForResourceInput, completion: @escaping (SdkResult<ListTagsForResourceOutput, ListTagsForResourceOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "listTagsForResource")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<ListTagsForResourceInput, ListTagsForResourceOutput, ListTagsForResourceOutputError>(id: "listTagsForResource")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: ListTagsForResourceInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ListTagsForResourceInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<ListTagsForResourceInput, ListTagsForResourceOutput, ListTagsForResourceOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: ListTagsForResourceInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<ListTagsForResourceInput, ListTagsForResourceOutput, ListTagsForResourceOutputError>(xAmzTarget: "RekognitionService.ListTagsForResource"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Returns an array of celebrities recognized in the input image.  For more information, see Recognizing Celebrities
    ///     in the Amazon Rekognition Developer Guide. </p>
    ///          <p>
    ///             <code>RecognizeCelebrities</code> returns the 64 largest faces in the image. It lists
    ///       recognized celebrities in the <code>CelebrityFaces</code> array and unrecognized faces in the
    ///         <code>UnrecognizedFaces</code> array. <code>RecognizeCelebrities</code> doesn't return
    ///       celebrities whose faces aren't among the largest 64 faces in the image.</p>
    ///
    ///          <p>For each celebrity recognized, <code>RecognizeCelebrities</code> returns a
    ///         <code>Celebrity</code> object. The <code>Celebrity</code> object contains the celebrity
    ///       name, ID, URL links to additional information, match confidence, and a
    ///         <code>ComparedFace</code> object that you can use to locate the celebrity's face on the
    ///       image.</p>
    ///          <p>Amazon Rekognition doesn't retain information about which images a celebrity has been recognized
    ///       in. Your application must store this information and use the <code>Celebrity</code> ID
    ///       property as a unique identifier for the celebrity. If you don't store the celebrity name or
    ///       additional information URLs returned by <code>RecognizeCelebrities</code>, you will need the
    ///       ID to identify the celebrity in a call to the <a>GetCelebrityInfo</a>
    ///       operation.</p>
    ///          <p>You pass the input image either as base64-encoded image bytes or as a reference to an
    ///       image in an Amazon S3 bucket. If you use the
    ///       AWS
    ///       CLI to call Amazon Rekognition operations, passing image bytes is not
    ///       supported. The image must be either a PNG or JPEG formatted file. </p>
    ///
    ///
    ///
    ///
    ///          <p>For an example, see Recognizing Celebrities in an Image in the Amazon Rekognition Developer Guide.</p>
    ///          <p>This operation requires permissions to perform the
    ///         <code>rekognition:RecognizeCelebrities</code> operation.</p>
    public func recognizeCelebrities(input: RecognizeCelebritiesInput, completion: @escaping (SdkResult<RecognizeCelebritiesOutput, RecognizeCelebritiesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "recognizeCelebrities")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<RecognizeCelebritiesInput, RecognizeCelebritiesOutput, RecognizeCelebritiesOutputError>(id: "recognizeCelebrities")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: RecognizeCelebritiesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: RecognizeCelebritiesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<RecognizeCelebritiesInput, RecognizeCelebritiesOutput, RecognizeCelebritiesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: RecognizeCelebritiesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<RecognizeCelebritiesInput, RecognizeCelebritiesOutput, RecognizeCelebritiesOutputError>(xAmzTarget: "RekognitionService.RecognizeCelebrities"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>For a given input face ID, searches for matching faces in the collection the face
    ///       belongs to. You get a face ID when you add a face to the collection using the <a>IndexFaces</a> operation. The operation compares the features of the input face with
    ///       faces in the specified collection. </p>
    ///          <note>
    ///             <p>You can also search faces without indexing faces by using the
    ///           <code>SearchFacesByImage</code> operation.</p>
    ///          </note>
    ///
    ///          <p>
    ///      The operation response returns
    ///       an array of faces that match, ordered by similarity score with the highest
    ///       similarity first. More specifically, it is an
    ///       array of metadata for each face match that is found. Along with the metadata, the response also
    ///       includes a <code>confidence</code> value for each face match, indicating the confidence
    ///       that the specific face matches the input face.
    ///     </p>
    ///
    ///          <p>For an example, see Searching for a Face Using Its Face ID in the Amazon Rekognition Developer Guide.</p>
    ///
    ///          <p>This operation requires permissions to perform the <code>rekognition:SearchFaces</code>
    ///       action.</p>
    public func searchFaces(input: SearchFacesInput, completion: @escaping (SdkResult<SearchFacesOutput, SearchFacesOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "searchFaces")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<SearchFacesInput, SearchFacesOutput, SearchFacesOutputError>(id: "searchFaces")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: SearchFacesInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: SearchFacesInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<SearchFacesInput, SearchFacesOutput, SearchFacesOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: SearchFacesInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<SearchFacesInput, SearchFacesOutput, SearchFacesOutputError>(xAmzTarget: "RekognitionService.SearchFaces"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>For a given input image, first detects the largest face in the image, and then searches
    ///       the specified collection for matching faces. The operation compares the features of the input
    ///       face with faces in the specified collection. </p>
    ///          <note>
    ///             <p>To search for all faces in an input image, you might first call the <a>IndexFaces</a> operation, and then use the face IDs returned in subsequent
    ///         calls to the <a>SearchFaces</a> operation. </p>
    ///             <p> You can also call the <code>DetectFaces</code> operation and use the bounding boxes
    ///         in the response to make face crops, which then you can pass in to the
    ///           <code>SearchFacesByImage</code> operation. </p>
    ///          </note>
    ///
    ///          <p>You pass the input image either as base64-encoded image bytes or as a reference to an
    ///       image in an Amazon S3 bucket. If you use the
    ///       AWS
    ///       CLI to call Amazon Rekognition operations, passing image bytes is not
    ///       supported. The image must be either a PNG or JPEG formatted file. </p>
    ///          <p>
    ///       The response returns an array of faces that match, ordered by similarity score with the
    ///       highest similarity first. More specifically, it is an
    ///       array of metadata for each face match found. Along with the metadata, the response also
    ///       includes a <code>similarity</code> indicating how similar the face is
    ///       to the input face.
    ///
    ///       In the response, the operation also returns the bounding
    ///       box (and a confidence level that the bounding box contains a face) of the face that Amazon Rekognition
    ///       used for the input image.
    ///     </p>
    ///          <p>If no faces are detected in the input image, <code>SearchFacesByImage</code> returns an
    ///       <code>InvalidParameterException</code> error. </p>
    ///
    ///          <p>For an example, Searching for a Face Using an Image in the Amazon Rekognition Developer Guide.</p>
    ///
    ///          <p>The <code>QualityFilter</code> input parameter allows you to filter out detected faces
    ///       that donâ€™t meet a required quality bar. The quality bar is based on a
    ///       variety of common use cases.
    ///       Use <code>QualityFilter</code> to set the quality bar for
    ///       filtering by specifying <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>.
    ///       If you do not want to filter detected faces, specify <code>NONE</code>. The default
    ///       value is <code>NONE</code>.</p>
    ///          <note>
    ///             <p>To use quality filtering, you need a collection associated with version 3 of the
    ///       face model or higher. To get the version of the face model associated with a collection, call
    ///       <a>DescribeCollection</a>. </p>
    ///          </note>
    ///
    ///          <p>This operation requires permissions to perform the <code>rekognition:SearchFacesByImage</code>
    ///       action.</p>
    public func searchFacesByImage(input: SearchFacesByImageInput, completion: @escaping (SdkResult<SearchFacesByImageOutput, SearchFacesByImageOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "searchFacesByImage")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<SearchFacesByImageInput, SearchFacesByImageOutput, SearchFacesByImageOutputError>(id: "searchFacesByImage")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: SearchFacesByImageInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: SearchFacesByImageInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<SearchFacesByImageInput, SearchFacesByImageOutput, SearchFacesByImageOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: SearchFacesByImageInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<SearchFacesByImageInput, SearchFacesByImageOutput, SearchFacesByImageOutputError>(xAmzTarget: "RekognitionService.SearchFacesByImage"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts asynchronous recognition of celebrities in a stored video.</p>
    ///          <p>Amazon Rekognition Video can detect celebrities in a video must be stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name
    ///       and the filename of the video.
    ///       <code>StartCelebrityRecognition</code>
    ///       returns a job identifier (<code>JobId</code>) which you use to get the results of the analysis.
    ///       When celebrity recognition analysis is finished, Amazon Rekognition Video publishes a completion status
    ///       to the Amazon Simple Notification Service topic that you specify in <code>NotificationChannel</code>.
    ///       To get the results of the celebrity recognition analysis, first check that the status value published to the Amazon SNS
    ///       topic is <code>SUCCEEDED</code>. If so, call  <a>GetCelebrityRecognition</a> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartCelebrityRecognition</code>. </p>
    ///
    ///          <p>For more information, see Recognizing Celebrities in the Amazon Rekognition Developer Guide.</p>
    public func startCelebrityRecognition(input: StartCelebrityRecognitionInput, completion: @escaping (SdkResult<StartCelebrityRecognitionOutput, StartCelebrityRecognitionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startCelebrityRecognition")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartCelebrityRecognitionInput, StartCelebrityRecognitionOutput, StartCelebrityRecognitionOutputError>(id: "startCelebrityRecognition")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartCelebrityRecognitionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartCelebrityRecognitionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartCelebrityRecognitionInput, StartCelebrityRecognitionOutput, StartCelebrityRecognitionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartCelebrityRecognitionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartCelebrityRecognitionInput, StartCelebrityRecognitionOutput, StartCelebrityRecognitionOutputError>(xAmzTarget: "RekognitionService.StartCelebrityRecognition"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p> Starts asynchronous detection of unsafe content in a stored video.</p>
    ///          <p>Amazon Rekognition Video can moderate content in a video stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name
    ///       and the filename of the video. <code>StartContentModeration</code>
    ///         returns a job identifier (<code>JobId</code>) which you use to get the results of the analysis.
    ///         When unsafe content analysis is finished, Amazon Rekognition Video publishes a completion status
    ///         to the Amazon Simple Notification Service topic that you specify in <code>NotificationChannel</code>.</p>
    ///         <p>To get the results of the unsafe content analysis, first check that the status value published to the Amazon SNS
    ///         topic is <code>SUCCEEDED</code>. If so, call <a>GetContentModeration</a> and pass the job identifier
    ///         (<code>JobId</code>) from the initial call to <code>StartContentModeration</code>. </p>
    ///
    ///          <p>For more information, see Detecting Unsafe Content in the Amazon Rekognition Developer Guide.</p>
    public func startContentModeration(input: StartContentModerationInput, completion: @escaping (SdkResult<StartContentModerationOutput, StartContentModerationOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startContentModeration")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartContentModerationInput, StartContentModerationOutput, StartContentModerationOutputError>(id: "startContentModeration")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartContentModerationInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartContentModerationInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartContentModerationInput, StartContentModerationOutput, StartContentModerationOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartContentModerationInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartContentModerationInput, StartContentModerationOutput, StartContentModerationOutputError>(xAmzTarget: "RekognitionService.StartContentModeration"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts asynchronous detection of faces in a stored video.</p>
    ///          <p>Amazon Rekognition Video can detect faces in a video stored in an Amazon S3 bucket.
    ///        Use <a>Video</a> to specify the bucket name and the filename of the video.
    ///        <code>StartFaceDetection</code> returns a job identifier (<code>JobId</code>) that you
    ///        use to get the results of the operation.
    ///        When face detection is finished, Amazon Rekognition Video publishes a completion status
    ///        to the Amazon Simple Notification Service topic that you specify in <code>NotificationChannel</code>.
    ///        To get the results of the face detection operation, first check that the status value published to the Amazon SNS
    ///        topic is <code>SUCCEEDED</code>. If so, call  <a>GetFaceDetection</a> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartFaceDetection</code>.</p>
    ///
    ///          <p>For more information, see Detecting Faces in a Stored Video in the
    ///      Amazon Rekognition Developer Guide.</p>
    public func startFaceDetection(input: StartFaceDetectionInput, completion: @escaping (SdkResult<StartFaceDetectionOutput, StartFaceDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startFaceDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartFaceDetectionInput, StartFaceDetectionOutput, StartFaceDetectionOutputError>(id: "startFaceDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartFaceDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartFaceDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartFaceDetectionInput, StartFaceDetectionOutput, StartFaceDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartFaceDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartFaceDetectionInput, StartFaceDetectionOutput, StartFaceDetectionOutputError>(xAmzTarget: "RekognitionService.StartFaceDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts the asynchronous search for faces in a collection that match the faces of persons detected in a stored video.</p>
    ///          <p>The video must be stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name
    ///       and the filename of the video. <code>StartFaceSearch</code>
    ///       returns a job identifier (<code>JobId</code>) which you use to get the search results once the search has completed.
    ///       When searching is finished, Amazon Rekognition Video publishes a completion status
    ///       to the Amazon Simple Notification Service topic that you specify in <code>NotificationChannel</code>.
    ///       To get the search results, first check that the status value published to the Amazon SNS
    ///       topic is <code>SUCCEEDED</code>. If so, call <a>GetFaceSearch</a> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartFaceSearch</code>. For more information, see
    ///       <a>procedure-person-search-videos</a>.</p>
    public func startFaceSearch(input: StartFaceSearchInput, completion: @escaping (SdkResult<StartFaceSearchOutput, StartFaceSearchOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startFaceSearch")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartFaceSearchInput, StartFaceSearchOutput, StartFaceSearchOutputError>(id: "startFaceSearch")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartFaceSearchInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartFaceSearchInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartFaceSearchInput, StartFaceSearchOutput, StartFaceSearchOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartFaceSearchInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartFaceSearchInput, StartFaceSearchOutput, StartFaceSearchOutputError>(xAmzTarget: "RekognitionService.StartFaceSearch"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts asynchronous detection of labels in a stored video.</p>
    ///          <p>Amazon Rekognition Video can detect labels in a video. Labels are instances of real-world entities.
    ///        This includes objects like flower, tree, and table; events like
    ///        wedding, graduation, and birthday party; concepts like landscape, evening, and nature; and activities
    ///        like a person getting out of a car or a person skiing.</p>
    ///
    ///          <p>The video must be stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name
    ///        and the filename of the video.
    ///         <code>StartLabelDetection</code> returns a job identifier (<code>JobId</code>) which you use to get the
    ///        results of the operation. When label detection is finished, Amazon Rekognition Video publishes a completion status
    ///         to the Amazon Simple Notification Service topic that you specify in <code>NotificationChannel</code>.</p>
    ///          <p>To get the results of the label detection operation, first check that the status value published to the Amazon SNS
    ///         topic is <code>SUCCEEDED</code>. If so, call  <a>GetLabelDetection</a> and pass the job identifier
    ///        (<code>JobId</code>) from the initial call to <code>StartLabelDetection</code>.</p>
    ///         <p></p>
    public func startLabelDetection(input: StartLabelDetectionInput, completion: @escaping (SdkResult<StartLabelDetectionOutput, StartLabelDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startLabelDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartLabelDetectionInput, StartLabelDetectionOutput, StartLabelDetectionOutputError>(id: "startLabelDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartLabelDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartLabelDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartLabelDetectionInput, StartLabelDetectionOutput, StartLabelDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartLabelDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartLabelDetectionInput, StartLabelDetectionOutput, StartLabelDetectionOutputError>(xAmzTarget: "RekognitionService.StartLabelDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts the asynchronous tracking of a person's path in a stored video.</p>
    ///          <p>Amazon Rekognition Video can track the path of people in a video stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name
    ///        and the filename of the video. <code>StartPersonTracking</code>
    ///        returns a job identifier (<code>JobId</code>) which you use to get the results of the operation.
    ///        When label detection is finished, Amazon Rekognition publishes a completion status
    ///        to the Amazon Simple Notification Service topic that you specify in <code>NotificationChannel</code>. </p>
    ///          <p>To get the results of the person detection operation, first check that the status value published to the Amazon SNS
    ///        topic is <code>SUCCEEDED</code>. If so, call  <a>GetPersonTracking</a> and pass the job identifier
    ///       (<code>JobId</code>) from the initial call to <code>StartPersonTracking</code>.</p>
    public func startPersonTracking(input: StartPersonTrackingInput, completion: @escaping (SdkResult<StartPersonTrackingOutput, StartPersonTrackingOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startPersonTracking")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartPersonTrackingInput, StartPersonTrackingOutput, StartPersonTrackingOutputError>(id: "startPersonTracking")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartPersonTrackingInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartPersonTrackingInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartPersonTrackingInput, StartPersonTrackingOutput, StartPersonTrackingOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartPersonTrackingInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartPersonTrackingInput, StartPersonTrackingOutput, StartPersonTrackingOutputError>(xAmzTarget: "RekognitionService.StartPersonTracking"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts the running of the version of a model. Starting a model takes a while
    ///       to complete. To check the current state of the model, use <a>DescribeProjectVersions</a>.</p>
    ///          <p>Once the model is running, you can detect custom labels in new images by calling
    ///          <a>DetectCustomLabels</a>.</p>
    ///          <note>
    ///             <p>You are charged for the amount of time that the model is running. To stop a running
    ///       model, call <a>StopProjectVersion</a>.</p>
    ///          </note>
    ///          <p>This operation requires permissions to perform the
    ///          <code>rekognition:StartProjectVersion</code> action.</p>
    public func startProjectVersion(input: StartProjectVersionInput, completion: @escaping (SdkResult<StartProjectVersionOutput, StartProjectVersionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startProjectVersion")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartProjectVersionInput, StartProjectVersionOutput, StartProjectVersionOutputError>(id: "startProjectVersion")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartProjectVersionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartProjectVersionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartProjectVersionInput, StartProjectVersionOutput, StartProjectVersionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartProjectVersionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartProjectVersionInput, StartProjectVersionOutput, StartProjectVersionOutputError>(xAmzTarget: "RekognitionService.StartProjectVersion"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts asynchronous detection of segment detection in a stored video.</p>
    ///          <p>Amazon Rekognition Video can detect segments in a video stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name and
    ///       the filename of the video. <code>StartSegmentDetection</code> returns a job identifier (<code>JobId</code>) which you use to get
    ///       the results of the operation. When segment detection is finished, Amazon Rekognition Video publishes a completion status to the Amazon Simple Notification Service topic
    ///       that you specify in <code>NotificationChannel</code>.</p>
    ///          <p>You can use the <code>Filters</code> (<a>StartSegmentDetectionFilters</a>)
    ///       input parameter to specify the minimum detection confidence returned in the response.
    ///       Within <code>Filters</code>, use <code>ShotFilter</code> (<a>StartShotDetectionFilter</a>)
    ///       to filter detected shots. Use  <code>TechnicalCueFilter</code> (<a>StartTechnicalCueDetectionFilter</a>)
    ///       to filter technical cues. </p>
    ///          <p>To get the results of the segment detection operation, first check that the status value published to the Amazon SNS
    ///       topic is <code>SUCCEEDED</code>. if so, call <a>GetSegmentDetection</a> and pass the job identifier (<code>JobId</code>)
    ///       from the initial call to <code>StartSegmentDetection</code>. </p>
    ///
    ///
    ///          <p>For more information, see Detecting Video Segments in Stored Video in the Amazon Rekognition Developer Guide.</p>
    public func startSegmentDetection(input: StartSegmentDetectionInput, completion: @escaping (SdkResult<StartSegmentDetectionOutput, StartSegmentDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startSegmentDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartSegmentDetectionInput, StartSegmentDetectionOutput, StartSegmentDetectionOutputError>(id: "startSegmentDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartSegmentDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartSegmentDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartSegmentDetectionInput, StartSegmentDetectionOutput, StartSegmentDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartSegmentDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartSegmentDetectionInput, StartSegmentDetectionOutput, StartSegmentDetectionOutputError>(xAmzTarget: "RekognitionService.StartSegmentDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts processing a stream processor. You create a stream processor by calling <a>CreateStreamProcessor</a>.
    ///             To tell <code>StartStreamProcessor</code> which stream processor to start, use the value of the <code>Name</code> field specified in the call to
    ///             <code>CreateStreamProcessor</code>.</p>
    public func startStreamProcessor(input: StartStreamProcessorInput, completion: @escaping (SdkResult<StartStreamProcessorOutput, StartStreamProcessorOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startStreamProcessor")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartStreamProcessorInput, StartStreamProcessorOutput, StartStreamProcessorOutputError>(id: "startStreamProcessor")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartStreamProcessorInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartStreamProcessorInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartStreamProcessorInput, StartStreamProcessorOutput, StartStreamProcessorOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartStreamProcessorInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartStreamProcessorInput, StartStreamProcessorOutput, StartStreamProcessorOutputError>(xAmzTarget: "RekognitionService.StartStreamProcessor"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Starts asynchronous detection of text in a stored video.</p>
    ///          <p>Amazon Rekognition Video can detect text in a video stored in an Amazon S3 bucket. Use <a>Video</a> to specify the bucket name and
    ///        the filename of the video. <code>StartTextDetection</code> returns a job identifier (<code>JobId</code>) which you use to get
    ///        the results of the operation. When text detection is finished, Amazon Rekognition Video publishes a completion status to the Amazon Simple Notification Service topic
    ///        that you specify in <code>NotificationChannel</code>.</p>
    ///          <p>To get the results of the text detection operation, first check that the status value published to the Amazon SNS
    ///        topic is <code>SUCCEEDED</code>. if so, call <a>GetTextDetection</a> and pass the job identifier (<code>JobId</code>)
    ///        from the initial call to <code>StartTextDetection</code>. </p>
    public func startTextDetection(input: StartTextDetectionInput, completion: @escaping (SdkResult<StartTextDetectionOutput, StartTextDetectionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "startTextDetection")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StartTextDetectionInput, StartTextDetectionOutput, StartTextDetectionOutputError>(id: "startTextDetection")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StartTextDetectionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StartTextDetectionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StartTextDetectionInput, StartTextDetectionOutput, StartTextDetectionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StartTextDetectionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StartTextDetectionInput, StartTextDetectionOutput, StartTextDetectionOutputError>(xAmzTarget: "RekognitionService.StartTextDetection"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Stops a running model. The operation might take a while to complete. To
    ///          check the current status, call <a>DescribeProjectVersions</a>. </p>
    public func stopProjectVersion(input: StopProjectVersionInput, completion: @escaping (SdkResult<StopProjectVersionOutput, StopProjectVersionOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "stopProjectVersion")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StopProjectVersionInput, StopProjectVersionOutput, StopProjectVersionOutputError>(id: "stopProjectVersion")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StopProjectVersionInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StopProjectVersionInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StopProjectVersionInput, StopProjectVersionOutput, StopProjectVersionOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StopProjectVersionInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StopProjectVersionInput, StopProjectVersionOutput, StopProjectVersionOutputError>(xAmzTarget: "RekognitionService.StopProjectVersion"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>Stops a running stream processor that was created by <a>CreateStreamProcessor</a>.</p>
    public func stopStreamProcessor(input: StopStreamProcessorInput, completion: @escaping (SdkResult<StopStreamProcessorOutput, StopStreamProcessorOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "stopStreamProcessor")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<StopStreamProcessorInput, StopStreamProcessorOutput, StopStreamProcessorOutputError>(id: "stopStreamProcessor")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: StopStreamProcessorInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: StopStreamProcessorInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<StopStreamProcessorInput, StopStreamProcessorOutput, StopStreamProcessorOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: StopStreamProcessorInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<StopStreamProcessorInput, StopStreamProcessorOutput, StopStreamProcessorOutputError>(xAmzTarget: "RekognitionService.StopStreamProcessor"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>
    ///       Adds one or more key-value tags to an Amazon Rekognition collection, stream processor, or Custom Labels model. For more information, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html">Tagging AWS Resources</a>.
    ///     </p>
    ///          <p>This operation requires permissions to perform the
    ///       <code>rekognition:TagResource</code> action. </p>
    public func tagResource(input: TagResourceInput, completion: @escaping (SdkResult<TagResourceOutput, TagResourceOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "tagResource")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<TagResourceInput, TagResourceOutput, TagResourceOutputError>(id: "tagResource")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: TagResourceInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: TagResourceInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<TagResourceInput, TagResourceOutput, TagResourceOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: TagResourceInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<TagResourceInput, TagResourceOutput, TagResourceOutputError>(xAmzTarget: "RekognitionService.TagResource"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

    /// <p>
    ///       Removes one or more tags from an Amazon Rekognition collection, stream processor, or Custom Labels model.
    ///     </p>
    ///          <p>This operation requires permissions to perform the
    ///       <code>rekognition:UntagResource</code> action. </p>
    public func untagResource(input: UntagResourceInput, completion: @escaping (SdkResult<UntagResourceOutput, UntagResourceOutputError>) -> Void)
    {
        let urlPath = "/"
        let context = HttpContextBuilder()
                      .withEncoder(value: encoder)
                      .withDecoder(value: decoder)
                      .withMethod(value: .post)
                      .withPath(value: urlPath)
                      .withServiceName(value: serviceName)
                      .withOperation(value: "untagResource")
                      .withIdempotencyTokenGenerator(value: config.idempotencyTokenGenerator)
                      .withLogger(value: config.logger)
                      .withCredentialsProvider(value: config.credentialsProvider)
                      .withRegion(value: config.region)
                      .withHost(value: "rekognition.\(config.region).amazonaws.com")
                      .withSigningName(value: "rekognition")
                      .withSigningRegion(value: config.signingRegion)
        var operation = OperationStack<UntagResourceInput, UntagResourceOutput, UntagResourceOutputError>(id: "untagResource")
        operation.addDefaultOperationMiddlewares()
        operation.serializeStep.intercept(position: .before, middleware: UntagResourceInputHeadersMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: UntagResourceInputQueryItemMiddleware())
        operation.serializeStep.intercept(position: .before, middleware: ContentTypeMiddleware<UntagResourceInput, UntagResourceOutput, UntagResourceOutputError>(contentType: "application/x-amz-json-1.1"))
        operation.serializeStep.intercept(position: .before, middleware: UntagResourceInputBodyMiddleware())
        operation.deserializeStep.intercept(position: .before, middleware: LoggerMiddleware(clientLogMode: config.clientLogMode))
        operation.buildStep.intercept(position: .before, middleware: EndpointResolverMiddleware(endpointResolver: config.endpointResolver, serviceId: serviceName))
        operation.finalizeStep.intercept(position: .after, middleware: RetrierMiddleware(retrier: config.retrier))
        let sigv4Config = SigV4Config(unsignedBody: false)
        operation.finalizeStep.intercept(position: .before,
                                                 middleware: SigV4Middleware(config: sigv4Config))
        operation.buildStep.intercept(position: .before, middleware: UserAgentMiddleware(metadata: AWSUserAgentMetadata.fromEnv(apiMetadata: APIMetadata(serviceId: serviceName, version: "1.0"))))
        operation.serializeStep.intercept(position: .before, middleware: XAmzTargetMiddleware<UntagResourceInput, UntagResourceOutput, UntagResourceOutputError>(xAmzTarget: "RekognitionService.UntagResource"))
        let result = operation.handleMiddleware(context: context.build(), input: input, next: client.getHandler())
        completion(result)
    }

}

// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct DetectLabelsOutputResponse: Equatable {
    /// <p>Version number of the label detection model that was used to detect labels.</p>
    public let labelModelVersion: String?
    /// <p>An array of labels for the real-world objects detected. </p>
    public let labels: [Label]?
    /// <p>The value of <code>OrientationCorrection</code> is always null.</p>
    ///          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata
    ///       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform
    ///       image correction. The bounding box coordinates are translated to represent object locations
    ///       after the orientation information in the Exif metadata is used to correct the image orientation.
    ///       Images in .png format don't contain Exif metadata.</p>
    ///          <p>Amazon Rekognition doesnâ€™t perform image correction for images in .png format and
    ///          .jpeg images without orientation information in the image Exif metadata. The bounding box
    ///          coordinates aren't translated and represent the object locations before the image is rotated.
    ///       </p>
    public let orientationCorrection: OrientationCorrection?

    public init (
        labelModelVersion: String? = nil,
        labels: [Label]? = nil,
        orientationCorrection: OrientationCorrection? = nil
    )
    {
        self.labelModelVersion = labelModelVersion
        self.labels = labels
        self.orientationCorrection = orientationCorrection
    }
}

extension DetectLabelsOutputResponse: CustomDebugStringConvertible {
    public var debugDescription: String {
        "DetectLabelsOutputResponse(labelModelVersion: \(String(describing: labelModelVersion)), labels: \(String(describing: labels)), orientationCorrection: \(String(describing: orientationCorrection)))"}
}

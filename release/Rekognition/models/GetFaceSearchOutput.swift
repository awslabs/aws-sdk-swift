// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct GetFaceSearchOutput: Equatable {
    /// <p>The current status of the face search job.</p>
    public let jobStatus: VideoJobStatus?
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>
    public let nextToken: String?
    /// <p>An array of persons,  <a>PersonMatch</a>,
    ///       in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information
    ///        for when persons are matched in the video.
    ///       You specify the input collection in an initial call to <code>StartFaceSearch</code>.
    ///       Each  <code>Persons</code> element includes a time the person was matched,
    ///       face match details (<code>FaceMatches</code>) for matching faces in the collection,
    ///        and person information (<code>Person</code>) for the matched person. </p>
    public let persons: [PersonMatch]?
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    public let statusMessage: String?
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses
    ///       from a Amazon Rekognition Video operation. </p>
    public let videoMetadata: VideoMetadata?

    public init (
        jobStatus: VideoJobStatus? = nil,
        nextToken: String? = nil,
        persons: [PersonMatch]? = nil,
        statusMessage: String? = nil,
        videoMetadata: VideoMetadata? = nil
    )
    {
        self.jobStatus = jobStatus
        self.nextToken = nextToken
        self.persons = persons
        self.statusMessage = statusMessage
        self.videoMetadata = videoMetadata
    }
}

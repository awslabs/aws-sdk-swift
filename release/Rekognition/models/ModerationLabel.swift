// Code generated by smithy-swift-codegen. DO NOT EDIT!



/// <p>Provides information about a single type of unsafe content found in an image or video. Each type of
///       moderated content has a label within a hierarchical taxonomy. For more information, see
///       Detecting Unsafe Content in the Amazon Rekognition Developer Guide.</p>
public struct ModerationLabel: Equatable {
    /// <p>Specifies the confidence that Amazon Rekognition has that the label has been correctly
    ///       identified.</p>
    ///          <p>If you don't specify the <code>MinConfidence</code> parameter in the call to
    ///         <code>DetectModerationLabels</code>, the operation returns labels with a confidence value
    ///       greater than or equal to 50 percent.</p>
    public let confidence: Float?
    /// <p>The label name for the type of unsafe content detected in the image.</p>
    public let name: String?
    /// <p>The name for the parent label. Labels at the top level of the hierarchy have the parent
    ///       label <code>""</code>.</p>
    public let parentName: String?

    public init (
        confidence: Float? = nil,
        name: String? = nil,
        parentName: String? = nil
    )
    {
        self.confidence = confidence
        self.name = name
        self.parentName = parentName
    }
}

extension ModerationLabel: CustomDebugStringConvertible {
    public var debugDescription: String {
        "ModerationLabel(confidence: \(String(describing: confidence)), name: \(String(describing: name)), parentName: \(String(describing: parentName)))"}
}

// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct IndexFacesOutputResponse: Equatable {
    /// <p>The version number of the face detection model that's associated with the input
    ///       collection (<code>CollectionId</code>).</p>
    public let faceModelVersion: String?
    /// <p>An array of faces detected and added to the collection.
    ///       For more information, see Searching Faces in a Collection in the Amazon Rekognition Developer Guide.
    ///     </p>
    public let faceRecords: [FaceRecord]?
    /// <p>If your collection is associated with a face detection model that's later
    ///       than version 3.0, the value of <code>OrientationCorrection</code>
    ///       is always null and no orientation information is returned.</p>
    ///
    ///          <p>If your collection is associated with a face detection model that's
    ///       version 3.0 or earlier, the following applies:</p>
    ///          <ul>
    ///             <li>
    ///                <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata
    ///         that includes the image's orientation. Amazon Rekognition uses this orientation information to perform
    ///         image correction - the bounding box coordinates are translated to represent object locations
    ///         after the orientation information in the Exif metadata is used to correct the image orientation.
    ///         Images in .png format don't contain Exif metadata. The value of <code>OrientationCorrection</code>
    ///         is null.</p>
    ///             </li>
    ///             <li>
    ///                <p>If the image doesn't contain orientation information in its Exif metadata, Amazon Rekognition returns
    ///       an estimated orientation (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition doesnâ€™t perform
    ///       image correction for images. The bounding box coordinates aren't translated and represent the
    ///       object locations before the image is rotated.</p>
    ///             </li>
    ///          </ul>
    ///
    ///
    ///
    ///          <p>Bounding box information is returned in the <code>FaceRecords</code> array. You can get the
    ///     version of the face detection model by calling <a>DescribeCollection</a>. </p>
    public let orientationCorrection: OrientationCorrection?
    /// <p>An array of faces that were detected in the image but weren't indexed. They weren't
    ///       indexed because the quality filter identified them as low quality, or the
    ///         <code>MaxFaces</code> request parameter filtered them out. To use the quality filter, you
    ///       specify the <code>QualityFilter</code> request parameter.</p>
    public let unindexedFaces: [UnindexedFace]?

    public init (
        faceModelVersion: String? = nil,
        faceRecords: [FaceRecord]? = nil,
        orientationCorrection: OrientationCorrection? = nil,
        unindexedFaces: [UnindexedFace]? = nil
    )
    {
        self.faceModelVersion = faceModelVersion
        self.faceRecords = faceRecords
        self.orientationCorrection = orientationCorrection
        self.unindexedFaces = unindexedFaces
    }
}

extension IndexFacesOutputResponse: CustomDebugStringConvertible {
    public var debugDescription: String {
        "IndexFacesOutputResponse(faceModelVersion: \(String(describing: faceModelVersion)), faceRecords: \(String(describing: faceRecords)), orientationCorrection: \(String(describing: orientationCorrection)), unindexedFaces: \(String(describing: unindexedFaces)))"}
}

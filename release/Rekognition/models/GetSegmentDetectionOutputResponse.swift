// Code generated by smithy-swift-codegen. DO NOT EDIT!



public struct GetSegmentDetectionOutputResponse: Equatable {
    /// <p>An array of
    ///        objects. There can be multiple audio streams.
    ///       Each <code>AudioMetadata</code> object contains metadata for a single audio stream.
    ///       Audio information in an <code>AudioMetadata</code> objects includes
    ///       the audio codec, the number of audio channels, the duration of the audio stream,
    ///       and the sample rate. Audio metadata is returned in each page of information returned
    ///       by <code>GetSegmentDetection</code>.</p>
    public let audioMetadata: [AudioMetadata]?
    /// <p>Current status of the segment detection job.</p>
    public let jobStatus: VideoJobStatus?
    /// <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns
    ///       a pagination token in the response. You can use this pagination token to retrieve the next set of text.</p>
    public let nextToken: String?
    /// <p>An array of segments detected in a video.  The array is sorted by the segment types (TECHNICAL_CUE or SHOT)
    ///       specified in the <code>SegmentTypes</code> input parameter of <code>StartSegmentDetection</code>. Within
    ///       each segment type the array is sorted by timestamp values.</p>
    public let segments: [SegmentDetection]?
    /// <p>An array containing the segment types requested in the call to <code>StartSegmentDetection</code>.
    ///     </p>
    public let selectedSegmentTypes: [SegmentTypeInfo]?
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    public let statusMessage: String?
    /// <p>Currently, Amazon Rekognition Video returns a single   object in the
    ///       <code>VideoMetadata</code> array. The object
    ///       contains information about the video stream in the input file that Amazon Rekognition Video chose to analyze.
    ///       The <code>VideoMetadata</code> object includes the video codec, video format and other information.
    ///       Video metadata is returned in each page of information returned by <code>GetSegmentDetection</code>.</p>
    public let videoMetadata: [VideoMetadata]?

    public init (
        audioMetadata: [AudioMetadata]? = nil,
        jobStatus: VideoJobStatus? = nil,
        nextToken: String? = nil,
        segments: [SegmentDetection]? = nil,
        selectedSegmentTypes: [SegmentTypeInfo]? = nil,
        statusMessage: String? = nil,
        videoMetadata: [VideoMetadata]? = nil
    )
    {
        self.audioMetadata = audioMetadata
        self.jobStatus = jobStatus
        self.nextToken = nextToken
        self.segments = segments
        self.selectedSegmentTypes = selectedSegmentTypes
        self.statusMessage = statusMessage
        self.videoMetadata = videoMetadata
    }
}

extension GetSegmentDetectionOutputResponse: CustomDebugStringConvertible {
    public var debugDescription: String {
        "GetSegmentDetectionOutputResponse(audioMetadata: \(String(describing: audioMetadata)), jobStatus: \(String(describing: jobStatus)), nextToken: \(String(describing: nextToken)), segments: \(String(describing: segments)), selectedSegmentTypes: \(String(describing: selectedSegmentTypes)), statusMessage: \(String(describing: statusMessage)), videoMetadata: \(String(describing: videoMetadata)))"}
}

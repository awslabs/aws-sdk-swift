// Code generated by smithy-swift-codegen. DO NOT EDIT!



/// <p>Settings for exporting data to Amazon S3. </p>
public struct S3Settings: Equatable {
    /// <p> An optional parameter to set a folder name in the S3 bucket. If provided, tables are
    ///          created in the path
    ///                <code>
    ///                <i>bucketFolder</i>/<i>schema_name</i>/<i>table_name</i>/</code>.
    ///          If this parameter isn't specified, then the path used is
    ///                <code>
    ///                <i>schema_name</i>/<i>table_name</i>/</code>. </p>
    public let bucketFolder: String?
    /// <p> The name of the S3 bucket. </p>
    public let bucketName: String?
    /// <p>A value that enables a change data capture (CDC) load to write INSERT and UPDATE
    ///          operations to .csv or .parquet (columnar storage) output files. The default setting is
    ///             <code>false</code>, but when <code>CdcInsertsAndUpdates</code> is set to
    ///             <code>true</code> or <code>y</code>, only INSERTs and UPDATEs from the source database
    ///          are migrated to the .csv or .parquet file. </p>
    ///          <p>For .csv file format only, how these INSERTs and UPDATEs are recorded depends on the
    ///          value of the <code>IncludeOpForFullLoad</code> parameter. If
    ///             <code>IncludeOpForFullLoad</code> is set to <code>true</code>, the first field of every
    ///          CDC record is set to either <code>I</code> or <code>U</code> to indicate INSERT and UPDATE
    ///          operations at the source. But if <code>IncludeOpForFullLoad</code> is set to
    ///             <code>false</code>, CDC records are written without an indication of INSERT or UPDATE
    ///          operations at the source. For more information about how these settings work together, see
    ///             <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring.InsertOps">Indicating Source DB Operations in Migrated S3 Data</a> in the <i>AWS
    ///             Database Migration Service User Guide.</i>.</p>
    ///          <note>
    ///
    ///             <p>AWS DMS supports the use of the <code>CdcInsertsAndUpdates</code> parameter in
    ///             versions 3.3.1 and later.</p>
    ///
    ///             <p>
    ///                <code>CdcInsertsOnly</code> and <code>CdcInsertsAndUpdates</code> can't
    ///             both be set to <code>true</code> for the same endpoint. Set either
    ///             <code>CdcInsertsOnly</code> or <code>CdcInsertsAndUpdates</code> to <code>true</code>
    ///             for the same endpoint, but not both.</p>
    ///
    ///          </note>
    public let cdcInsertsAndUpdates: Bool?
    /// <p>A value that enables a change data capture (CDC) load to write only INSERT operations to
    ///          .csv or columnar storage (.parquet) output files. By default (the
    ///             <code>false</code> setting), the first field in a .csv or .parquet record contains the
    ///          letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was
    ///          inserted, updated, or deleted at the source database for a CDC load to the target.</p>
    ///          <p>If <code>CdcInsertsOnly</code> is set to <code>true</code> or <code>y</code>, only
    ///          INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format
    ///          only, how these INSERTs are recorded depends on the value of
    ///             <code>IncludeOpForFullLoad</code>. If <code>IncludeOpForFullLoad</code> is set to
    ///             <code>true</code>, the first field of every CDC record is set to I to indicate the
    ///          INSERT operation at the source. If <code>IncludeOpForFullLoad</code> is set to
    ///             <code>false</code>, every CDC record is written without a first field to indicate the
    ///          INSERT operation at the source. For more information about how these settings work
    ///          together, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring.InsertOps">Indicating Source DB Operations in Migrated S3 Data</a> in the <i>AWS
    ///             Database Migration Service User Guide.</i>.</p>
    ///
    ///          <note>
    ///
    ///                <p>AWS DMS supports the interaction described preceding between the
    ///                <code>CdcInsertsOnly</code> and <code>IncludeOpForFullLoad</code> parameters in
    ///             versions 3.1.4 and later. </p>
    ///
    ///                <p>
    ///                <code>CdcInsertsOnly</code> and <code>CdcInsertsAndUpdates</code> can't
    ///             both be set to <code>true</code> for the same endpoint. Set either
    ///                <code>CdcInsertsOnly</code> or <code>CdcInsertsAndUpdates</code> to <code>true</code>
    ///             for the same endpoint, but not both.</p>
    ///
    ///          </note>
    public let cdcInsertsOnly: Bool?
    /// <p>Specifies the folder path of CDC files. For an S3 source, this setting is required if a
    ///          task captures change data; otherwise, it's optional. If <code>CdcPath</code> is set, AWS
    ///          DMS reads CDC files from this path and replicates the data changes to the target endpoint.
    ///          For an S3 target if you set <a href="https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions">
    ///                <code>PreserveTransactions</code>
    ///             </a> to <code>true</code>, AWS
    ///          DMS verifies that you have set this parameter to a folder path on your S3 target where AWS
    ///          DMS can save the transaction order for the CDC load. AWS DMS creates this CDC folder path
    ///          in either your S3 target working directory or the S3 target location specified by <a href="https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder">
    ///                <code>BucketFolder</code>
    ///             </a> and <a href="https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName">
    ///                <code>BucketName</code>
    ///             </a>.</p>
    ///          <p>For example, if you specify <code>CdcPath</code> as <code>MyChangedData</code>, and you
    ///          specify <code>BucketName</code> as <code>MyTargetBucket</code> but do not specify
    ///             <code>BucketFolder</code>, AWS DMS creates the CDC folder path following:
    ///             <code>MyTargetBucket/MyChangedData</code>.</p>
    ///          <p>If you specify the same <code>CdcPath</code>, and you specify <code>BucketName</code> as
    ///             <code>MyTargetBucket</code> and <code>BucketFolder</code> as <code>MyTargetData</code>,
    ///          AWS DMS creates the CDC folder path following:
    ///             <code>MyTargetBucket/MyTargetData/MyChangedData</code>.</p>
    ///          <p>For more information on CDC including transaction order on an S3 target, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.EndpointSettings.CdcPath">Capturing data changes (CDC) including transaction order on the S3
    ///             target</a>.</p>
    ///          <note>
    ///             <p>This setting is supported in AWS DMS versions 3.4.2 and later.</p>
    ///          </note>
    public let cdcPath: String?
    /// <p>An optional parameter to use GZIP to compress the target files. Set to GZIP to compress
    ///          the target files. Either set this parameter to NONE (the default) or don't use it to leave the files uncompressed.
    ///          This parameter applies to both .csv and .parquet file formats. </p>
    public let compressionType: CompressionTypeValue?
    /// <p> The delimiter used to separate columns in the .csv file for both source and target. The default is a comma.
    ///       </p>
    public let csvDelimiter: String?
    /// <p>This setting only applies if your Amazon S3 output files during a change data capture
    ///          (CDC) load are written in .csv format. If <a href="https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue">
    ///                <code>UseCsvNoSupValue</code>
    ///             </a> is set to true, specify a
    ///          string value that you want AWS DMS to use for all columns not included in the supplemental
    ///          log. If you do not specify a string value, AWS DMS uses the null value for these columns
    ///          regardless of the <code>UseCsvNoSupValue</code> setting.</p>
    ///          <note>
    ///             <p>This setting is supported in AWS DMS versions 3.4.1 and later.</p>
    ///          </note>
    public let csvNoSupValue: String?
    /// <p> The delimiter used to separate rows in the .csv file for both source and target. The default is a carriage
    ///          return (<code>\n</code>). </p>
    public let csvRowDelimiter: String?
    /// <p>The format of the data that you want to use for output. You can choose one of the
    ///          following: </p>
    ///          <ul>
    ///             <li>
    ///                <p>
    ///                   <code>csv</code> : This is a row-based file format with comma-separated values
    ///                (.csv). </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>parquet</code> : Apache Parquet (.parquet) is a columnar storage file format
    ///                that features efficient compression and provides faster query response. </p>
    ///             </li>
    ///          </ul>
    public let dataFormat: DataFormatValue?
    /// <p>The size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB).
    ///          This number is used for .parquet file format only. </p>
    public let dataPageSize: Int?
    /// <p>Specifies a date separating delimiter to use during folder partitioning. The default value is
    ///          <code>SLASH</code>. Use this parameter when <code>DatePartitionedEnabled</code> is set to <code>true</code>.</p>
    public let datePartitionDelimiter: DatePartitionDelimiterValue?
    /// <p>When set to <code>true</code>, this parameter partitions S3 bucket folders based on transaction commit
    ///          dates. The default value is <code>false</code>. For more information about date-based folder partitoning,
    ///          see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.DatePartitioning">Using date-based folder partitioning</a>.</p>
    public let datePartitionEnabled: Bool?
    /// <p>Identifies the sequence of the date format to use during folder partitioning. The default value is
    ///          <code>YYYYMMDD</code>. Use this parameter when <code>DatePartitionedEnabled</code> is set to <code>true</code>.</p>
    public let datePartitionSequence: DatePartitionSequenceValue?
    /// <p>The maximum size of an encoded dictionary page of a column. If the dictionary page
    ///          exceeds this, this column is stored using an encoding type of <code>PLAIN</code>. This
    ///          parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page
    ///          before it reverts to <code>PLAIN</code> encoding. This size is used for
    ///            .parquet file format only. </p>
    public let dictPageSizeLimit: Int?
    /// <p>A value that enables statistics for Parquet pages and row groups. Choose
    ///             <code>true</code> to enable statistics, <code>false</code> to disable. Statistics
    ///          include <code>NULL</code>, <code>DISTINCT</code>, <code>MAX</code>, and <code>MIN</code>
    ///          values. This parameter defaults to <code>true</code>. This value is used for
    ///             .parquet file format only.</p>
    public let enableStatistics: Bool?
    /// <p>The type of encoding you are using: </p>
    ///          <ul>
    ///             <li>
    ///                <p>
    ///                   <code>RLE_DICTIONARY</code> uses a combination of bit-packing and run-length
    ///                encoding to store repeated values more efficiently. This is the default.</p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>PLAIN</code> doesn't use encoding at all. Values are stored as they
    ///                are.</p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>PLAIN_DICTIONARY</code> builds a dictionary of the values encountered in a
    ///                given column. The dictionary is stored in a dictionary page for each column
    ///                chunk.</p>
    ///             </li>
    ///          </ul>
    public let encodingType: EncodingTypeValue?
    /// <p>The type of server-side encryption that you want to use for your data. This encryption
    ///          type is part of the endpoint settings or the extra connections attributes for Amazon S3.
    ///          You can choose either <code>SSE_S3</code> (the default) or <code>SSE_KMS</code>. </p>
    ///          <note>
    ///             <p>For the <code>ModifyEndpoint</code> operation, you can change the existing value of
    ///             the <code>EncryptionMode</code> parameter from <code>SSE_KMS</code> to
    ///                <code>SSE_S3</code>. But you canâ€™t change the existing value from <code>SSE_S3</code>
    ///             to <code>SSE_KMS</code>.</p>
    ///          </note>
    ///          <p>To use <code>SSE_S3</code>, you need an AWS Identity and Access Management (IAM) role
    ///          with permission to allow <code>"arn:aws:s3:::dms-*"</code> to use the following
    ///          actions:</p>
    ///          <ul>
    ///             <li>
    ///                <p>
    ///                   <code>s3:CreateBucket</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:ListBucket</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:DeleteBucket</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:GetBucketLocation</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:GetObject</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:PutObject</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:DeleteObject</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:GetObjectVersion</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:GetBucketPolicy</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:PutBucketPolicy</code>
    ///                </p>
    ///             </li>
    ///             <li>
    ///                <p>
    ///                   <code>s3:DeleteBucketPolicy</code>
    ///                </p>
    ///             </li>
    ///          </ul>
    public let encryptionMode: EncryptionModeValue?
    /// <p> Specifies how tables are defined in the S3 source files only. </p>
    public let externalTableDefinition: String?
    /// <p>A value that enables a full load to write INSERT operations to the comma-separated value
    ///          (.csv) output files only to indicate how the rows were added to the source database.</p>
    ///          <note>
    ///             <p>AWS DMS supports the <code>IncludeOpForFullLoad</code> parameter in versions 3.1.4 and
    ///             later.</p>
    ///          </note>
    ///          <p>For full load, records can only be inserted. By default (the <code>false</code>
    ///          setting), no information is recorded in these output files for a full load to indicate that
    ///          the rows were inserted at the source database. If <code>IncludeOpForFullLoad</code> is set
    ///          to <code>true</code> or <code>y</code>, the INSERT is recorded as an I annotation in the
    ///          first field of the .csv file. This allows the format of your target records from a full
    ///          load to be consistent with the target records from a CDC load.</p>
    ///          <note>
    ///             <p>This setting works together with the <code>CdcInsertsOnly</code> and the
    ///                <code>CdcInsertsAndUpdates</code> parameters for output to .csv files only. For more
    ///             information about how these settings work together, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring.InsertOps">Indicating Source DB Operations in Migrated S3 Data</a> in the <i>AWS
    ///                Database Migration Service User Guide.</i>.</p>
    ///          </note>
    public let includeOpForFullLoad: Bool?
    /// <p>A value that specifies the precision of any <code>TIMESTAMP</code> column values that
    ///          are written to an Amazon S3 object file in .parquet format.</p>
    ///          <note>
    ///             <p>AWS DMS supports the <code>ParquetTimestampInMillisecond</code> parameter in versions
    ///             3.1.4 and later.</p>
    ///          </note>
    ///          <p>When <code>ParquetTimestampInMillisecond</code> is set to <code>true</code> or
    ///             <code>y</code>, AWS DMS writes all <code>TIMESTAMP</code> columns in a .parquet
    ///          formatted file with millisecond precision. Otherwise, DMS writes them with microsecond
    ///          precision.</p>
    ///          <p>Currently, Amazon Athena and AWS Glue can handle only
    ///          millisecond precision for <code>TIMESTAMP</code> values. Set
    ///          this parameter to <code>true</code> for S3 endpoint object
    ///          files that are .parquet formatted only if you plan to query or process the data with Athena or AWS Glue.</p>
    ///          <note>
    ///
    ///                <p>AWS DMS writes any <code>TIMESTAMP</code> column
    ///                   values written to an S3 file in .csv format with
    ///                   microsecond precision.</p>
    ///
    ///                <p>Setting <code>ParquetTimestampInMillisecond</code> has no effect on the string
    ///             format of the timestamp column value that is inserted by setting the
    ///                <code>TimestampColumnName</code> parameter.</p>
    ///
    ///          </note>
    public let parquetTimestampInMillisecond: Bool?
    /// <p>The version of the Apache Parquet format that you want to use: <code>parquet_1_0</code>
    ///          (the default) or <code>parquet_2_0</code>.</p>
    public let parquetVersion: ParquetVersionValue?
    /// <p>If set to <code>true</code>, AWS DMS saves the transaction order for a change data
    ///          capture (CDC) load on the Amazon S3 target specified by <a href="https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath">
    ///                <code>CdcPath</code>
    ///             </a>. For more information, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.EndpointSettings.CdcPath">Capturing data changes (CDC) including transaction order on the S3
    ///             target</a>.</p>
    ///          <note>
    ///             <p>This setting is supported in AWS DMS versions 3.4.2 and later.</p>
    ///          </note>
    public let preserveTransactions: Bool?
    /// <p>The number of rows in a row group. A smaller row group size provides faster reads. But
    ///          as the number of row groups grows, the slower writes become. This parameter defaults to
    ///          10,000 rows. This number is used for .parquet file format only. </p>
    ///          <p>If you choose a value larger than the maximum, <code>RowGroupLength</code> is set to the
    ///          max row group length in bytes (64 * 1024 * 1024). </p>
    public let rowGroupLength: Int?
    /// <p>If you are using <code>SSE_KMS</code> for the <code>EncryptionMode</code>, provide the
    ///          AWS KMS key ID. The key that you use needs an attached policy that enables AWS Identity and
    ///          Access Management (IAM) user permissions and allows use of the key.</p>
    ///          <p>Here is a CLI example: <code>aws dms create-endpoint --endpoint-identifier
    ///                <i>value</i> --endpoint-type target --engine-name s3 --s3-settings
    ///                ServiceAccessRoleArn=<i>value</i>,BucketFolder=<i>value</i>,BucketName=<i>value</i>,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=<i>value</i>
    ///             </code>
    ///          </p>
    public let serverSideEncryptionKmsKeyId: String?
    /// <p> The Amazon Resource Name (ARN) used by the service access IAM role. It is a required
    ///          parameter that enables DMS to write and read objects from an S3 bucket.</p>
    public let serviceAccessRoleArn: String?
    /// <p>A value that when nonblank causes AWS DMS to add a column with timestamp information to
    ///          the endpoint data for an Amazon S3 target.</p>
    ///          <note>
    ///             <p>AWS DMS supports the <code>TimestampColumnName</code> parameter in versions 3.1.4 and later.</p>
    ///          </note>
    ///          <p>DMS includes an additional <code>STRING</code> column in the
    ///          .csv or .parquet object files of your migrated data when you set
    ///          <code>TimestampColumnName</code> to a nonblank value.</p>
    ///          <p>For a full load, each row of this timestamp column contains a
    ///          timestamp for when the data was transferred from the source to
    ///          the target by DMS. </p>
    ///          <p>For a change data capture (CDC) load, each row of the timestamp column contains the
    ///          timestamp for the commit of that row in the source
    ///          database.</p>
    ///          <p>The string format for this timestamp column value is
    ///          <code>yyyy-MM-dd HH:mm:ss.SSSSSS</code>. By default, the
    ///          precision of this value is in microseconds. For a CDC load, the
    ///          rounding of the precision depends on the commit timestamp
    ///          supported by DMS for the source database.</p>
    ///          <p>When the <code>AddColumnName</code> parameter is set to <code>true</code>, DMS also
    ///          includes a name for the timestamp column that you set with
    ///          <code>TimestampColumnName</code>.</p>
    public let timestampColumnName: String?
    /// <p>This setting applies if the S3 output files during a change data capture (CDC) load are
    ///          written in .csv format. If set to <code>true</code> for columns not included in the
    ///          supplemental log, AWS DMS uses the value specified by <a href="https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue">
    ///                <code>CsvNoSupValue</code>
    ///             </a>. If not set or set to
    ///             <code>false</code>, AWS DMS uses the null value for these columns.</p>
    ///          <note>
    ///             <p>This setting is supported in AWS DMS versions 3.4.1 and later.</p>
    ///          </note>
    public let useCsvNoSupValue: Bool?

    public init (
        bucketFolder: String? = nil,
        bucketName: String? = nil,
        cdcInsertsAndUpdates: Bool? = nil,
        cdcInsertsOnly: Bool? = nil,
        cdcPath: String? = nil,
        compressionType: CompressionTypeValue? = nil,
        csvDelimiter: String? = nil,
        csvNoSupValue: String? = nil,
        csvRowDelimiter: String? = nil,
        dataFormat: DataFormatValue? = nil,
        dataPageSize: Int? = nil,
        datePartitionDelimiter: DatePartitionDelimiterValue? = nil,
        datePartitionEnabled: Bool? = nil,
        datePartitionSequence: DatePartitionSequenceValue? = nil,
        dictPageSizeLimit: Int? = nil,
        enableStatistics: Bool? = nil,
        encodingType: EncodingTypeValue? = nil,
        encryptionMode: EncryptionModeValue? = nil,
        externalTableDefinition: String? = nil,
        includeOpForFullLoad: Bool? = nil,
        parquetTimestampInMillisecond: Bool? = nil,
        parquetVersion: ParquetVersionValue? = nil,
        preserveTransactions: Bool? = nil,
        rowGroupLength: Int? = nil,
        serverSideEncryptionKmsKeyId: String? = nil,
        serviceAccessRoleArn: String? = nil,
        timestampColumnName: String? = nil,
        useCsvNoSupValue: Bool? = nil
    )
    {
        self.bucketFolder = bucketFolder
        self.bucketName = bucketName
        self.cdcInsertsAndUpdates = cdcInsertsAndUpdates
        self.cdcInsertsOnly = cdcInsertsOnly
        self.cdcPath = cdcPath
        self.compressionType = compressionType
        self.csvDelimiter = csvDelimiter
        self.csvNoSupValue = csvNoSupValue
        self.csvRowDelimiter = csvRowDelimiter
        self.dataFormat = dataFormat
        self.dataPageSize = dataPageSize
        self.datePartitionDelimiter = datePartitionDelimiter
        self.datePartitionEnabled = datePartitionEnabled
        self.datePartitionSequence = datePartitionSequence
        self.dictPageSizeLimit = dictPageSizeLimit
        self.enableStatistics = enableStatistics
        self.encodingType = encodingType
        self.encryptionMode = encryptionMode
        self.externalTableDefinition = externalTableDefinition
        self.includeOpForFullLoad = includeOpForFullLoad
        self.parquetTimestampInMillisecond = parquetTimestampInMillisecond
        self.parquetVersion = parquetVersion
        self.preserveTransactions = preserveTransactions
        self.rowGroupLength = rowGroupLength
        self.serverSideEncryptionKmsKeyId = serverSideEncryptionKmsKeyId
        self.serviceAccessRoleArn = serviceAccessRoleArn
        self.timestampColumnName = timestampColumnName
        self.useCsvNoSupValue = useCsvNoSupValue
    }
}

extension S3Settings: CustomDebugStringConvertible {
    public var debugDescription: String {
        "S3Settings(bucketFolder: \(String(describing: bucketFolder)), bucketName: \(String(describing: bucketName)), cdcInsertsAndUpdates: \(String(describing: cdcInsertsAndUpdates)), cdcInsertsOnly: \(String(describing: cdcInsertsOnly)), cdcPath: \(String(describing: cdcPath)), compressionType: \(String(describing: compressionType)), csvDelimiter: \(String(describing: csvDelimiter)), csvNoSupValue: \(String(describing: csvNoSupValue)), csvRowDelimiter: \(String(describing: csvRowDelimiter)), dataFormat: \(String(describing: dataFormat)), dataPageSize: \(String(describing: dataPageSize)), datePartitionDelimiter: \(String(describing: datePartitionDelimiter)), datePartitionEnabled: \(String(describing: datePartitionEnabled)), datePartitionSequence: \(String(describing: datePartitionSequence)), dictPageSizeLimit: \(String(describing: dictPageSizeLimit)), enableStatistics: \(String(describing: enableStatistics)), encodingType: \(String(describing: encodingType)), encryptionMode: \(String(describing: encryptionMode)), externalTableDefinition: \(String(describing: externalTableDefinition)), includeOpForFullLoad: \(String(describing: includeOpForFullLoad)), parquetTimestampInMillisecond: \(String(describing: parquetTimestampInMillisecond)), parquetVersion: \(String(describing: parquetVersion)), preserveTransactions: \(String(describing: preserveTransactions)), rowGroupLength: \(String(describing: rowGroupLength)), serverSideEncryptionKmsKeyId: \(String(describing: serverSideEncryptionKmsKeyId)), serviceAccessRoleArn: \(String(describing: serviceAccessRoleArn)), timestampColumnName: \(String(describing: timestampColumnName)), useCsvNoSupValue: \(String(describing: useCsvNoSupValue)))"}
}

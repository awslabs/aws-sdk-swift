// Code generated by smithy-swift-codegen. DO NOT EDIT!



/// <p>The evaluation metrics for the find matches algorithm. The quality of your machine
///       learning transform is measured by getting your transform to predict some matches and comparing
///       the results to known matches from the same dataset. The quality metrics are based on a subset
///       of your data, so they are not precise.</p>
public struct FindMatchesMetrics: Equatable {
    /// <p>The area under the precision/recall curve (AUPRC) is a single number measuring the overall
    ///       quality of the transform, that is independent of the choice made for precision vs. recall.
    ///       Higher values indicate that you have a more attractive precision vs. recall tradeoff.</p>
    /// 	        <p>For more information, see <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and recall</a> in Wikipedia.</p>
    public let areaUnderPRCurve: Double?
    /// <p>A list of <code>ColumnImportance</code> structures containing column importance metrics, sorted in order of descending importance.</p>
    public let columnImportances: [ColumnImportance]?
    /// <p>The confusion matrix shows you what your transform is predicting accurately and what types of errors it is making.</p>
    /// 	        <p>For more information, see <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion matrix</a> in Wikipedia.</p>
    public let confusionMatrix: ConfusionMatrix?
    /// <p>The maximum F1 metric indicates the transform's accuracy between 0 and 1, where 1 is the best accuracy.</p>
    ///          <p>For more information, see <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> in Wikipedia.</p>
    public let f1: Double?
    /// <p>The precision metric indicates when often your transform is correct when it predicts a match. Specifically, it measures how well the transform finds true positives from the total true positives possible.</p>
    ///          <p>For more information, see <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and recall</a> in Wikipedia.</p>
    public let precision: Double?
    /// <p>The recall metric indicates that for an actual match, how often your transform predicts
    ///       the match. Specifically, it measures how well the transform finds true positives from the
    ///       total records in the source data.</p>
    ///          <p>For more information, see <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and recall</a> in Wikipedia.</p>
    public let recall: Double?

    public init (
        areaUnderPRCurve: Double? = nil,
        columnImportances: [ColumnImportance]? = nil,
        confusionMatrix: ConfusionMatrix? = nil,
        f1: Double? = nil,
        precision: Double? = nil,
        recall: Double? = nil
    )
    {
        self.areaUnderPRCurve = areaUnderPRCurve
        self.columnImportances = columnImportances
        self.confusionMatrix = confusionMatrix
        self.f1 = f1
        self.precision = precision
        self.recall = recall
    }
}

//
// Copyright Amazon.com Inc. or its affiliates.
// All Rights Reserved.
//
// SPDX-License-Identifier: Apache-2.0
//

// Code generated by smithy-swift-codegen. DO NOT EDIT!

@_spi(SmithyReadWrite) import ClientRuntime
import Foundation
import class SmithyEventStreams.DefaultMessageDecoder
import class SmithyHTTPAPI.HTTPResponse
@_spi(SmithyReadWrite) import class SmithyJSON.Reader
@_spi(SmithyReadWrite) import class SmithyJSON.Writer
import enum ClientRuntime.ErrorFault
import enum Smithy.ClientError
import enum SmithyEventStreamsAPI.MessageType
import enum SmithyReadWrite.ReaderError
@_spi(SmithyReadWrite) import enum SmithyReadWrite.ReadingClosures
@_spi(SmithyReadWrite) import enum SmithyReadWrite.WritingClosures
@_spi(SmithyTimestamps) import enum SmithyTimestamps.TimestampFormat
@_spi(SmithyReadWrite) import func SmithyReadWrite.listWritingClosure
import protocol AWSClientRuntime.AWSServiceError
import protocol ClientRuntime.HTTPError
import protocol ClientRuntime.ModeledError
@_spi(SmithyReadWrite) import protocol SmithyReadWrite.SmithyReader
@_spi(SmithyReadWrite) import protocol SmithyReadWrite.SmithyWriter
@_spi(SmithyReadWrite) import struct AWSClientRuntime.RestJSONError
@_spi(UnknownAWSHTTPServiceError) import struct AWSClientRuntime.UnknownAWSHTTPServiceError
import struct SmithyEventStreams.DefaultMessageDecoderStream
import struct SmithyEventStreamsAPI.Header
import struct SmithyEventStreamsAPI.Message
import struct SmithyHTTPAPI.Header
import struct SmithyHTTPAPI.Headers
@_spi(SmithyReadWrite) import struct SmithyReadWrite.WritingClosureBox
import typealias SmithyEventStreamsAPI.MarshalClosure
import typealias SmithyEventStreamsAPI.UnmarshalClosure

extension TranscribeStreamingClientTypes {

    /// Contains entities identified as personally identifiable information (PII) in your transcription output, along with various associated attributes. Examples include category, confidence score, type, stability score, and start and end times.
    public struct Entity: Swift.Sendable {
        /// The category of information identified. The only category is PII.
        public var category: Swift.String?
        /// The confidence score associated with the identified PII entity in your audio. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words identified as PII.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the utterance that was identified as PII.
        public var endTime: Swift.Double
        /// The start time, in milliseconds, of the utterance that was identified as PII.
        public var startTime: Swift.Double
        /// The type of PII identified. For example, NAME or CREDIT_DEBIT_NUMBER.
        public var type: Swift.String?

        public init(
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            startTime: Swift.Double = 0.0,
            type: Swift.String? = nil
        ) {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
            self.type = type
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum ItemType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pronunciation
        case punctuation
        case sdkUnknown(Swift.String)

        public static var allCases: [ItemType] {
            return [
                .pronunciation,
                .punctuation
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pronunciation: return "pronunciation"
            case .punctuation: return "punctuation"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct Item: Swift.Sendable {
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endTime: Swift.Double
        /// If speaker partitioning is enabled, Speaker labels the speaker of the specified item.
        public var speaker: Swift.String?
        /// If partial result stabilization is enabled, Stable indicates whether the specified item is stable (true) or if it may change when the segment is complete (false).
        public var stable: Swift.Bool?
        /// The start time, in milliseconds, of the transcribed item.
        public var startTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your request. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool

        public init(
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            speaker: Swift.String? = nil,
            stable: Swift.Bool? = nil,
            startTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.ItemType? = nil,
            vocabularyFilterMatch: Swift.Bool = false
        ) {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.stable = stable
            self.startTime = startTime
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
    public struct Alternative: Swift.Sendable {
        /// Contains entities identified as personally identifiable information (PII) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.Entity]?
        /// Contains words, phrases, or punctuation marks in your transcription output.
        public var items: [TranscribeStreamingClientTypes.Item]?
        /// Contains transcribed text.
        public var transcript: Swift.String?

        public init(
            entities: [TranscribeStreamingClientTypes.Entity]? = nil,
            items: [TranscribeStreamingClientTypes.Item]? = nil,
            transcript: Swift.String? = nil
        ) {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A wrapper for your audio chunks. Your audio stream consists of one or more audio events, which consist of one or more audio chunks. For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
    public struct AudioEvent: Swift.Sendable {
        /// An audio blob containing the next segment of audio from your application, with a maximum duration of 1 second. The maximum size in bytes varies based on audio properties. Find recommended size in [Transcribing streaming best practices](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#best-practices). Size calculation: Duration (s) * Sample Rate (Hz) * Number of Channels * 2 (Bytes per Sample) For example, a 1-second chunk of 16 kHz, 2-channel, 16-bit audio would be 1 * 16000 * 2 * 2 = 64000 bytes. For 8 kHz, 1-channel, 16-bit audio, a 1-second chunk would be 1 * 8000 * 1 * 2 = 16000 bytes.
        public var audioChunk: Foundation.Data?

        public init(
            audioChunk: Foundation.Data? = nil
        ) {
            self.audioChunk = audioChunk
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum ParticipantRole: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case agent
        case customer
        case sdkUnknown(Swift.String)

        public static var allCases: [ParticipantRole] {
            return [
                .agent,
                .customer
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .agent: return "AGENT"
            case .customer: return "CUSTOMER"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Makes it possible to specify which speaker is on which audio channel. For example, if your agent is the first participant to speak, you would set ChannelId to 0 (to indicate the first channel) and ParticipantRole to AGENT (to indicate that it's the agent speaking).
    public struct ChannelDefinition: Swift.Sendable {
        /// Specify the audio channel you want to define.
        /// This member is required.
        public var channelId: Swift.Int
        /// Specify the speaker you want to define. Omitting this parameter is equivalent to specifying both participants.
        /// This member is required.
        public var participantRole: TranscribeStreamingClientTypes.ParticipantRole?

        public init(
            channelId: Swift.Int = 0,
            participantRole: TranscribeStreamingClientTypes.ParticipantRole? = nil
        ) {
            self.channelId = channelId
            self.participantRole = participantRole
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum ContentRedactionOutput: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case redacted
        case redactedAndUnredacted
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentRedactionOutput] {
            return [
                .redacted,
                .redactedAndUnredacted
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .redacted: return "redacted"
            case .redactedAndUnredacted: return "redacted_and_unredacted"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Allows you to specify additional settings for your Call Analytics post-call request, including output locations for your redacted transcript, which IAM role to use, and which encryption key to use. DataAccessRoleArn and OutputLocation are required fields. PostCallAnalyticsSettings provides you with the same insights as a Call Analytics post-call transcription. Refer to [Post-call analytics](https://docs.aws.amazon.com/transcribe/latest/dg/tca-post-call.html) for more information on this feature.
    public struct PostCallAnalyticsSettings: Swift.Sendable {
        /// Specify whether you want only a redacted transcript or both a redacted and an unredacted transcript. If you choose redacted and unredacted, two JSON files are generated and stored in the Amazon S3 output location you specify. Note that to include ContentRedactionOutput in your request, you must enable content redaction (ContentRedactionType).
        public var contentRedactionOutput: TranscribeStreamingClientTypes.ContentRedactionOutput?
        /// The Amazon Resource Name (ARN) of an IAM role that has permissions to access the Amazon S3 bucket that contains your input files. If the role that you specify doesn’t have the appropriate permissions to access the specified Amazon S3 location, your request fails. IAM role ARNs have the format arn:partition:iam::account:role/role-name-with-path. For example: arn:aws:iam::111122223333:role/Admin. For more information, see [IAM ARNs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns).
        /// This member is required.
        public var dataAccessRoleArn: Swift.String?
        /// The KMS key you want to use to encrypt your Call Analytics post-call output. If using a key located in the current Amazon Web Services account, you can specify your KMS key in one of four ways:
        ///
        /// * Use the KMS key ID itself. For example, 1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use an alias for the KMS key ID. For example, alias/ExampleAlias.
        ///
        /// * Use the Amazon Resource Name (ARN) for the KMS key ID. For example, arn:aws:kms:region:account-ID:key/1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use the ARN for the KMS key alias. For example, arn:aws:kms:region:account-ID:alias/ExampleAlias.
        ///
        ///
        /// If using a key located in a different Amazon Web Services account than the current Amazon Web Services account, you can specify your KMS key in one of two ways:
        ///
        /// * Use the ARN for the KMS key ID. For example, arn:aws:kms:region:account-ID:key/1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use the ARN for the KMS key alias. For example, arn:aws:kms:region:account-ID:alias/ExampleAlias.
        ///
        ///
        /// Note that the role making the request must have permission to use the specified KMS key.
        public var outputEncryptionKMSKeyId: Swift.String?
        /// The Amazon S3 location where you want your Call Analytics post-call transcription output stored. You can use any of the following formats to specify the output location:
        ///
        /// * s3://DOC-EXAMPLE-BUCKET
        ///
        /// * s3://DOC-EXAMPLE-BUCKET/my-output-folder/
        ///
        /// * s3://DOC-EXAMPLE-BUCKET/my-output-folder/my-call-analytics-job.json
        /// This member is required.
        public var outputLocation: Swift.String?

        public init(
            contentRedactionOutput: TranscribeStreamingClientTypes.ContentRedactionOutput? = nil,
            dataAccessRoleArn: Swift.String? = nil,
            outputEncryptionKMSKeyId: Swift.String? = nil,
            outputLocation: Swift.String? = nil
        ) {
            self.contentRedactionOutput = contentRedactionOutput
            self.dataAccessRoleArn = dataAccessRoleArn
            self.outputEncryptionKMSKeyId = outputEncryptionKMSKeyId
            self.outputLocation = outputLocation
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Allows you to set audio channel definitions and post-call analytics settings.
    public struct ConfigurationEvent: Swift.Sendable {
        /// Indicates which speaker is on which audio channel.
        public var channelDefinitions: [TranscribeStreamingClientTypes.ChannelDefinition]?
        /// Provides additional optional settings for your Call Analytics post-call request, including encryption and output locations for your redacted transcript. PostCallAnalyticsSettings provides you with the same insights as a Call Analytics post-call transcription. Refer to [Post-call analytics](https://docs.aws.amazon.com/transcribe/latest/dg/tca-post-call.html) for more information on this feature.
        public var postCallAnalyticsSettings: TranscribeStreamingClientTypes.PostCallAnalyticsSettings?

        public init(
            channelDefinitions: [TranscribeStreamingClientTypes.ChannelDefinition]? = nil,
            postCallAnalyticsSettings: TranscribeStreamingClientTypes.PostCallAnalyticsSettings? = nil
        ) {
            self.channelDefinitions = channelDefinitions
            self.postCallAnalyticsSettings = postCallAnalyticsSettings
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    public enum AudioStream: Swift.Sendable {
        /// A blob of audio from your application. Your audio stream consists of one or more audio events. For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
        case audioevent(TranscribeStreamingClientTypes.AudioEvent)
        /// Contains audio channel definitions and post-call analytics settings.
        case configurationevent(TranscribeStreamingClientTypes.ConfigurationEvent)
        case sdkUnknown(Swift.String)
    }
}

/// One or more arguments to the StartStreamTranscription, StartMedicalStreamTranscription, or StartCallAnalyticsStreamTranscription operation was not valid. For example, MediaEncoding or LanguageCode used unsupported values. Check the specified parameters and try your request again.
public struct BadRequestException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error, Swift.Sendable {

    public struct Properties: Swift.Sendable {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "BadRequestException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    ) {
        self.properties.message = message
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains entities identified as personally identifiable information (PII) in your transcription output, along with various associated attributes. Examples include category, confidence score, content, type, and start and end times.
    public struct CallAnalyticsEntity: Swift.Sendable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the identified entity.
        public var beginOffsetMillis: Swift.Int?
        /// The category of information identified. For example, PII.
        public var category: Swift.String?
        /// The confidence score associated with the identification of an entity in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words that represent the identified entity.
        public var content: Swift.String?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the identified entity.
        public var endOffsetMillis: Swift.Int?
        /// The type of PII identified. For example, NAME or CREDIT_DEBIT_NUMBER.
        public var type: Swift.String?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endOffsetMillis: Swift.Int? = nil,
            type: Swift.String? = nil
        ) {
            self.beginOffsetMillis = beginOffsetMillis
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endOffsetMillis = endOffsetMillis
            self.type = type
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A word, phrase, or punctuation mark in your Call Analytics transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct CallAnalyticsItem: Swift.Sendable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the identified item.
        public var beginOffsetMillis: Swift.Int?
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the identified item.
        public var endOffsetMillis: Swift.Int?
        /// If partial result stabilization is enabled, Stable indicates whether the specified item is stable (true) or if it may change when the segment is complete (false).
        public var stable: Swift.Bool?
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your Call Analytics request. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endOffsetMillis: Swift.Int? = nil,
            stable: Swift.Bool? = nil,
            type: TranscribeStreamingClientTypes.ItemType? = nil,
            vocabularyFilterMatch: Swift.Bool = false
        ) {
            self.beginOffsetMillis = beginOffsetMillis
            self.confidence = confidence
            self.content = content
            self.endOffsetMillis = endOffsetMillis
            self.stable = stable
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum CallAnalyticsLanguageCode: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case deDe
        case enAu
        case enGb
        case enUs
        case esUs
        case frCa
        case frFr
        case itIt
        case ptBr
        case sdkUnknown(Swift.String)

        public static var allCases: [CallAnalyticsLanguageCode] {
            return [
                .deDe,
                .enAu,
                .enGb,
                .enUs,
                .esUs,
                .frCa,
                .frFr,
                .itIt,
                .ptBr
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .deDe: return "de-DE"
            case .enAu: return "en-AU"
            case .enGb: return "en-GB"
            case .enUs: return "en-US"
            case .esUs: return "es-US"
            case .frCa: return "fr-CA"
            case .frFr: return "fr-FR"
            case .itIt: return "it-IT"
            case .ptBr: return "pt-BR"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains the timestamp range (start time through end time) of a matched category.
    public struct TimestampRange: Swift.Sendable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the category match.
        public var beginOffsetMillis: Swift.Int?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the category match.
        public var endOffsetMillis: Swift.Int?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            endOffsetMillis: Swift.Int? = nil
        ) {
            self.beginOffsetMillis = beginOffsetMillis
            self.endOffsetMillis = endOffsetMillis
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains the timestamps of matched categories.
    public struct PointsOfInterest: Swift.Sendable {
        /// Contains the timestamp ranges (start time through end time) of matched categories and rules.
        public var timestampRanges: [TranscribeStreamingClientTypes.TimestampRange]?

        public init(
            timestampRanges: [TranscribeStreamingClientTypes.TimestampRange]? = nil
        ) {
            self.timestampRanges = timestampRanges
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Provides information on any TranscriptFilterType categories that matched your transcription output. Matches are identified for each segment upon completion of that segment.
    public struct CategoryEvent: Swift.Sendable {
        /// Lists the categories that were matched in your audio segment.
        public var matchedCategories: [Swift.String]?
        /// Contains information about the matched categories, including category names and timestamps.
        public var matchedDetails: [Swift.String: TranscribeStreamingClientTypes.PointsOfInterest]?

        public init(
            matchedCategories: [Swift.String]? = nil,
            matchedDetails: [Swift.String: TranscribeStreamingClientTypes.PointsOfInterest]? = nil
        ) {
            self.matchedCategories = matchedCategories
            self.matchedDetails = matchedDetails
        }
    }
}

/// A new stream started with the same session ID. The current stream has been terminated.
public struct ConflictException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error, Swift.Sendable {

    public struct Properties: Swift.Sendable {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ConflictException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    ) {
        self.properties.message = message
    }
}

/// A problem occurred while processing the audio. Amazon Transcribe terminated processing.
public struct InternalFailureException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error, Swift.Sendable {

    public struct Properties: Swift.Sendable {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "InternalFailureException" }
    public static var fault: ClientRuntime.ErrorFault { .server }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    ) {
        self.properties.message = message
    }
}

/// Your client has exceeded one of the Amazon Transcribe limits. This is typically the audio length limit. Break your audio stream into smaller chunks and try your request again.
public struct LimitExceededException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error, Swift.Sendable {

    public struct Properties: Swift.Sendable {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "LimitExceededException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    ) {
        self.properties.message = message
    }
}

/// The service is currently unavailable. Try your request later.
public struct ServiceUnavailableException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error, Swift.Sendable {

    public struct Properties: Swift.Sendable {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ServiceUnavailableException" }
    public static var fault: ClientRuntime.ErrorFault { .server }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    ) {
        self.properties.message = message
    }
}

extension TranscribeStreamingClientTypes {

    /// Provides the location, using character count, in your transcript where a match is identified. For example, the location of an issue or a category match within a segment.
    public struct CharacterOffsets: Swift.Sendable {
        /// Provides the character count of the first character where a match is identified. For example, the first character associated with an issue or a category match in a segment transcript.
        public var begin: Swift.Int?
        /// Provides the character count of the last character where a match is identified. For example, the last character associated with an issue or a category match in a segment transcript.
        public var end: Swift.Int?

        public init(
            begin: Swift.Int? = nil,
            end: Swift.Int? = nil
        ) {
            self.begin = begin
            self.end = end
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Lists the issues that were identified in your audio segment.
    public struct IssueDetected: Swift.Sendable {
        /// Provides the timestamps that identify when in an audio segment the specified issue occurs.
        public var characterOffsets: TranscribeStreamingClientTypes.CharacterOffsets?

        public init(
            characterOffsets: TranscribeStreamingClientTypes.CharacterOffsets? = nil
        ) {
            self.characterOffsets = characterOffsets
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum Sentiment: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case mixed
        case negative
        case neutral
        case positive
        case sdkUnknown(Swift.String)

        public static var allCases: [Sentiment] {
            return [
                .mixed,
                .negative,
                .neutral,
                .positive
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .mixed: return "MIXED"
            case .negative: return "NEGATIVE"
            case .neutral: return "NEUTRAL"
            case .positive: return "POSITIVE"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains set of transcription results from one or more audio segments, along with additional information about the parameters included in your request. For example, channel definitions, partial result stabilization, sentiment, and issue detection.
    public struct UtteranceEvent: Swift.Sendable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the UtteranceEvent.
        public var beginOffsetMillis: Swift.Int?
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the UtteranceEvent.
        public var endOffsetMillis: Swift.Int?
        /// Contains entities identified as personally identifiable information (PII) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.CallAnalyticsEntity]?
        /// Indicates whether the segment in the UtteranceEvent is complete (FALSE) or partial (TRUE).
        public var isPartial: Swift.Bool
        /// Provides the issue that was detected in the specified segment.
        public var issuesDetected: [TranscribeStreamingClientTypes.IssueDetected]?
        /// Contains words, phrases, or punctuation marks that are associated with the specified UtteranceEvent.
        public var items: [TranscribeStreamingClientTypes.CallAnalyticsItem]?
        /// Provides the role of the speaker for each audio channel, either CUSTOMER or AGENT.
        public var participantRole: TranscribeStreamingClientTypes.ParticipantRole?
        /// Provides the sentiment that was detected in the specified segment.
        public var sentiment: TranscribeStreamingClientTypes.Sentiment?
        /// Contains transcribed text.
        public var transcript: Swift.String?
        /// The unique identifier that is associated with the specified UtteranceEvent.
        public var utteranceId: Swift.String?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            endOffsetMillis: Swift.Int? = nil,
            entities: [TranscribeStreamingClientTypes.CallAnalyticsEntity]? = nil,
            isPartial: Swift.Bool = false,
            issuesDetected: [TranscribeStreamingClientTypes.IssueDetected]? = nil,
            items: [TranscribeStreamingClientTypes.CallAnalyticsItem]? = nil,
            participantRole: TranscribeStreamingClientTypes.ParticipantRole? = nil,
            sentiment: TranscribeStreamingClientTypes.Sentiment? = nil,
            transcript: Swift.String? = nil,
            utteranceId: Swift.String? = nil
        ) {
            self.beginOffsetMillis = beginOffsetMillis
            self.endOffsetMillis = endOffsetMillis
            self.entities = entities
            self.isPartial = isPartial
            self.issuesDetected = issuesDetected
            self.items = items
            self.participantRole = participantRole
            self.sentiment = sentiment
            self.transcript = transcript
            self.utteranceId = utteranceId
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains detailed information about your real-time Call Analytics session. These details are provided in the UtteranceEvent and CategoryEvent objects.
    public enum CallAnalyticsTranscriptResultStream: Swift.Sendable {
        /// Contains set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to channel definitions, partial result stabilization, sentiment, issue detection, and other transcription-related data.
        case utteranceevent(TranscribeStreamingClientTypes.UtteranceEvent)
        /// Provides information on matched categories that were used to generate real-time supervisor alerts.
        case categoryevent(TranscribeStreamingClientTypes.CategoryEvent)
        case sdkUnknown(Swift.String)
    }
}

extension TranscribeStreamingClientTypes {

    public enum ClinicalNoteGenerationStatus: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case completed
        case failed
        case inProgress
        case sdkUnknown(Swift.String)

        public static var allCases: [ClinicalNoteGenerationStatus] {
            return [
                .completed,
                .failed,
                .inProgress
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .completed: return "COMPLETED"
            case .failed: return "FAILED"
            case .inProgress: return "IN_PROGRESS"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The details for clinical note generation, including status, and output locations for clinical note and aggregated transcript if the analytics completed, or failure reason if the analytics failed.
    public struct ClinicalNoteGenerationResult: Swift.Sendable {
        /// Holds the Amazon S3 URI for the output Clinical Note.
        public var clinicalNoteOutputLocation: Swift.String?
        /// If ClinicalNoteGenerationResult is FAILED, information about why it failed.
        public var failureReason: Swift.String?
        /// The status of the clinical note generation. Possible Values:
        ///
        /// * IN_PROGRESS
        ///
        /// * FAILED
        ///
        /// * COMPLETED
        ///
        ///
        /// After audio streaming finishes, and you send a MedicalScribeSessionControlEvent event (with END_OF_SESSION as the Type), the status is set to IN_PROGRESS. If the status is COMPLETED, the analytics completed successfully, and you can find the results at the locations specified in ClinicalNoteOutputLocation and TranscriptOutputLocation. If the status is FAILED, FailureReason provides details about the failure.
        public var status: TranscribeStreamingClientTypes.ClinicalNoteGenerationStatus?
        /// Holds the Amazon S3 URI for the output Transcript.
        public var transcriptOutputLocation: Swift.String?

        public init(
            clinicalNoteOutputLocation: Swift.String? = nil,
            failureReason: Swift.String? = nil,
            status: TranscribeStreamingClientTypes.ClinicalNoteGenerationStatus? = nil,
            transcriptOutputLocation: Swift.String? = nil
        ) {
            self.clinicalNoteOutputLocation = clinicalNoteOutputLocation
            self.failureReason = failureReason
            self.status = status
            self.transcriptOutputLocation = transcriptOutputLocation
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The output configuration for aggregated transcript and clinical note generation.
    public struct ClinicalNoteGenerationSettings: Swift.Sendable {
        /// The name of the Amazon S3 bucket where you want the output of Amazon Web Services HealthScribe post-stream analytics stored. Don't include the S3:// prefix of the specified bucket. HealthScribe outputs transcript and clinical note files under the prefix: S3://$output-bucket-name/healthscribe-streaming/session-id/post-stream-analytics/clinical-notes The role ResourceAccessRoleArn specified in the MedicalScribeConfigurationEvent must have permission to use the specified location. You can change Amazon S3 permissions using the [ Amazon Web Services Management Console ](https://console.aws.amazon.com/s3). See also [Permissions Required for IAM User Roles ](https://docs.aws.amazon.com/transcribe/latest/dg/security_iam_id-based-policy-examples.html#auth-role-iam-user) .
        /// This member is required.
        public var outputBucketName: Swift.String?

        public init(
            outputBucketName: Swift.String? = nil
        ) {
            self.outputBucketName = outputBucketName
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum ContentIdentificationType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pii
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentIdentificationType] {
            return [
                .pii
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pii: return "PII"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum ContentRedactionType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pii
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentRedactionType] {
            return [
                .pii
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pii: return "PII"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

/// The request references a resource which doesn't exist.
public struct ResourceNotFoundException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error, Swift.Sendable {

    public struct Properties: Swift.Sendable {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ResourceNotFoundException" }
    public static var fault: ClientRuntime.ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = SmithyHTTPAPI.HTTPResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    ) {
        self.properties.message = message
    }
}

public struct GetMedicalScribeStreamInput: Swift.Sendable {
    /// The identifier of the HealthScribe streaming session you want information about.
    /// This member is required.
    public var sessionId: Swift.String?

    public init(
        sessionId: Swift.String? = nil
    ) {
        self.sessionId = sessionId
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeParticipantRole: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case clinician
        case patient
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeParticipantRole] {
            return [
                .clinician,
                .patient
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .clinician: return "CLINICIAN"
            case .patient: return "PATIENT"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Makes it possible to specify which speaker is on which channel. For example, if the clinician is the first participant to speak, you would set the ChannelId of the first ChannelDefinition in the list to 0 (to indicate the first channel) and ParticipantRole to CLINICIAN (to indicate that it's the clinician speaking). Then you would set the ChannelId of the second ChannelDefinition in the list to 1 (to indicate the second channel) and ParticipantRole to PATIENT (to indicate that it's the patient speaking). If you don't specify a channel definition, HealthScribe will diarize the transcription and identify speaker roles for each speaker.
    public struct MedicalScribeChannelDefinition: Swift.Sendable {
        /// Specify the audio channel you want to define.
        /// This member is required.
        public var channelId: Swift.Int
        /// Specify the participant that you want to flag. The allowed options are CLINICIAN and PATIENT.
        /// This member is required.
        public var participantRole: TranscribeStreamingClientTypes.MedicalScribeParticipantRole?

        public init(
            channelId: Swift.Int = 0,
            participantRole: TranscribeStreamingClientTypes.MedicalScribeParticipantRole? = nil
        ) {
            self.channelId = channelId
            self.participantRole = participantRole
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains encryption related settings to be used for data encryption with Key Management Service, including KmsEncryptionContext and KmsKeyId. The KmsKeyId is required, while KmsEncryptionContext is optional for additional layer of security. By default, Amazon Web Services HealthScribe provides encryption at rest to protect sensitive customer data using Amazon S3-managed keys. HealthScribe uses the KMS key you specify as a second layer of encryption. Your ResourceAccessRoleArn must permission to use your KMS key. For more information, see [Data Encryption at rest for Amazon Web Services HealthScribe](https://docs.aws.amazon.com/transcribe/latest/dg/health-scribe-encryption.html).
    public struct MedicalScribeEncryptionSettings: Swift.Sendable {
        /// A map of plain text, non-secret key:value pairs, known as encryption context pairs, that provide an added layer of security for your data. For more information, see [KMSencryption context ](https://docs.aws.amazon.com/transcribe/latest/dg/key-management.html#kms-context) and [Asymmetric keys in KMS ](https://docs.aws.amazon.com/transcribe/latest/dg/symmetric-asymmetric.html).
        public var kmsEncryptionContext: [Swift.String: Swift.String]?
        /// The ID of the KMS key you want to use for your streaming session. You can specify its KMS key ID, key Amazon Resource Name (ARN), alias name, or alias ARN. When using an alias name, prefix it with "alias/". To specify a KMS key in a different Amazon Web Services account, you must use the key ARN or alias ARN. For example:
        ///
        /// * Key ID: 1234abcd-12ab-34cd-56ef-1234567890ab
        ///
        /// * Key ARN: arn:aws:kms:us-east-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab
        ///
        /// * Alias name: alias/ExampleAlias
        ///
        /// * Alias ARN: arn:aws:kms:us-east-2:111122223333:alias/ExampleAlias
        ///
        ///
        /// To get the key ID and key ARN for a KMS key, use the [ListKeys](https://docs.aws.amazon.com/kms/latest/APIReference/API_ListKeys.html) or [DescribeKey](https://docs.aws.amazon.com/kms/latest/APIReference/API_DescribeKey.html) KMS API operations. To get the alias name and alias ARN, use [ListKeys](https://docs.aws.amazon.com/kms/latest/APIReference/API_ListAliases.html) API operation.
        /// This member is required.
        public var kmsKeyId: Swift.String?

        public init(
            kmsEncryptionContext: [Swift.String: Swift.String]? = nil,
            kmsKeyId: Swift.String? = nil
        ) {
            self.kmsEncryptionContext = kmsEncryptionContext
            self.kmsKeyId = kmsKeyId
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeLanguageCode: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case enUs
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeLanguageCode] {
            return [
                .enUs
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .enUs: return "en-US"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeMediaEncoding: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case flac
        case oggOpus
        case pcm
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeMediaEncoding] {
            return [
                .flac,
                .oggOpus,
                .pcm
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .flac: return "flac"
            case .oggOpus: return "ogg-opus"
            case .pcm: return "pcm"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains details for the result of post-stream analytics.
    public struct MedicalScribePostStreamAnalyticsResult: Swift.Sendable {
        /// Provides the Clinical Note Generation result for post-stream analytics.
        public var clinicalNoteGenerationResult: TranscribeStreamingClientTypes.ClinicalNoteGenerationResult?

        public init(
            clinicalNoteGenerationResult: TranscribeStreamingClientTypes.ClinicalNoteGenerationResult? = nil
        ) {
            self.clinicalNoteGenerationResult = clinicalNoteGenerationResult
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The settings for post-stream analytics.
    public struct MedicalScribePostStreamAnalyticsSettings: Swift.Sendable {
        /// Specify settings for the post-stream clinical note generation.
        /// This member is required.
        public var clinicalNoteGenerationSettings: TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings?

        public init(
            clinicalNoteGenerationSettings: TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings? = nil
        ) {
            self.clinicalNoteGenerationSettings = clinicalNoteGenerationSettings
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeStreamStatus: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case completed
        case failed
        case inProgress
        case paused
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeStreamStatus] {
            return [
                .completed,
                .failed,
                .inProgress,
                .paused
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .completed: return "COMPLETED"
            case .failed: return "FAILED"
            case .inProgress: return "IN_PROGRESS"
            case .paused: return "PAUSED"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeVocabularyFilterMethod: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case mask
        case remove
        case tag
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeVocabularyFilterMethod] {
            return [
                .mask,
                .remove,
                .tag
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .mask: return "mask"
            case .remove: return "remove"
            case .tag: return "tag"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains details about a Amazon Web Services HealthScribe streaming session.
    public struct MedicalScribeStreamDetails: Swift.Sendable {
        /// The Channel Definitions of the HealthScribe streaming session.
        public var channelDefinitions: [TranscribeStreamingClientTypes.MedicalScribeChannelDefinition]?
        /// The Encryption Settings of the HealthScribe streaming session.
        public var encryptionSettings: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings?
        /// The Language Code of the HealthScribe streaming session.
        public var languageCode: TranscribeStreamingClientTypes.MedicalScribeLanguageCode?
        /// The Media Encoding of the HealthScribe streaming session.
        public var mediaEncoding: TranscribeStreamingClientTypes.MedicalScribeMediaEncoding?
        /// The sample rate (in hertz) of the HealthScribe streaming session.
        public var mediaSampleRateHertz: Swift.Int?
        /// The result of post-stream analytics for the HealthScribe streaming session.
        public var postStreamAnalyticsResult: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsResult?
        /// The post-stream analytics settings of the HealthScribe streaming session.
        public var postStreamAnalyticsSettings: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings?
        /// The Amazon Resource Name (ARN) of the role used in the HealthScribe streaming session.
        public var resourceAccessRoleArn: Swift.String?
        /// The identifier of the HealthScribe streaming session.
        public var sessionId: Swift.String?
        /// The date and time when the HealthScribe streaming session was created.
        public var streamCreatedAt: Foundation.Date?
        /// The date and time when the HealthScribe streaming session was ended.
        public var streamEndedAt: Foundation.Date?
        /// The streaming status of the HealthScribe streaming session. Possible Values:
        ///
        /// * IN_PROGRESS
        ///
        /// * PAUSED
        ///
        /// * FAILED
        ///
        /// * COMPLETED
        ///
        ///
        /// This status is specific to real-time streaming. A COMPLETED status doesn't mean that the post-stream analytics is complete. To get status of an analytics result, check the Status field for the analytics result within the MedicalScribePostStreamAnalyticsResult. For example, you can view the status of the ClinicalNoteGenerationResult.
        public var streamStatus: TranscribeStreamingClientTypes.MedicalScribeStreamStatus?
        /// The method of the vocabulary filter for the HealthScribe streaming session.
        public var vocabularyFilterMethod: TranscribeStreamingClientTypes.MedicalScribeVocabularyFilterMethod?
        /// The name of the vocabulary filter used for the HealthScribe streaming session .
        public var vocabularyFilterName: Swift.String?
        /// The vocabulary name of the HealthScribe streaming session.
        public var vocabularyName: Swift.String?

        public init(
            channelDefinitions: [TranscribeStreamingClientTypes.MedicalScribeChannelDefinition]? = nil,
            encryptionSettings: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings? = nil,
            languageCode: TranscribeStreamingClientTypes.MedicalScribeLanguageCode? = nil,
            mediaEncoding: TranscribeStreamingClientTypes.MedicalScribeMediaEncoding? = nil,
            mediaSampleRateHertz: Swift.Int? = nil,
            postStreamAnalyticsResult: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsResult? = nil,
            postStreamAnalyticsSettings: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings? = nil,
            resourceAccessRoleArn: Swift.String? = nil,
            sessionId: Swift.String? = nil,
            streamCreatedAt: Foundation.Date? = nil,
            streamEndedAt: Foundation.Date? = nil,
            streamStatus: TranscribeStreamingClientTypes.MedicalScribeStreamStatus? = nil,
            vocabularyFilterMethod: TranscribeStreamingClientTypes.MedicalScribeVocabularyFilterMethod? = nil,
            vocabularyFilterName: Swift.String? = nil,
            vocabularyName: Swift.String? = nil
        ) {
            self.channelDefinitions = channelDefinitions
            self.encryptionSettings = encryptionSettings
            self.languageCode = languageCode
            self.mediaEncoding = mediaEncoding
            self.mediaSampleRateHertz = mediaSampleRateHertz
            self.postStreamAnalyticsResult = postStreamAnalyticsResult
            self.postStreamAnalyticsSettings = postStreamAnalyticsSettings
            self.resourceAccessRoleArn = resourceAccessRoleArn
            self.sessionId = sessionId
            self.streamCreatedAt = streamCreatedAt
            self.streamEndedAt = streamEndedAt
            self.streamStatus = streamStatus
            self.vocabularyFilterMethod = vocabularyFilterMethod
            self.vocabularyFilterName = vocabularyFilterName
            self.vocabularyName = vocabularyName
        }
    }
}

public struct GetMedicalScribeStreamOutput: Swift.Sendable {
    /// Provides details about a HealthScribe streaming session.
    public var medicalScribeStreamDetails: TranscribeStreamingClientTypes.MedicalScribeStreamDetails?

    public init(
        medicalScribeStreamDetails: TranscribeStreamingClientTypes.MedicalScribeStreamDetails? = nil
    ) {
        self.medicalScribeStreamDetails = medicalScribeStreamDetails
    }
}

extension TranscribeStreamingClientTypes {

    public enum LanguageCode: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case afZa
        case arAe
        case arSa
        case caEs
        case csCz
        case daDk
        case deCh
        case deDe
        case elGr
        case enAb
        case enAu
        case enGb
        case enIe
        case enIn
        case enNz
        case enUs
        case enWl
        case enZa
        case esEs
        case esUs
        case euEs
        case faIr
        case fiFi
        case frCa
        case frFr
        case glEs
        case heIl
        case hiIn
        case hrHr
        case idId
        case itIt
        case jaJp
        case koKr
        case lvLv
        case msMy
        case nlNl
        case noNo
        case plPl
        case ptBr
        case ptPt
        case roRo
        case ruRu
        case skSk
        case soSo
        case srRs
        case svSe
        case thTh
        case tlPh
        case ukUa
        case viVn
        case zhCn
        case zhHk
        case zhTw
        case zuZa
        case sdkUnknown(Swift.String)

        public static var allCases: [LanguageCode] {
            return [
                .afZa,
                .arAe,
                .arSa,
                .caEs,
                .csCz,
                .daDk,
                .deCh,
                .deDe,
                .elGr,
                .enAb,
                .enAu,
                .enGb,
                .enIe,
                .enIn,
                .enNz,
                .enUs,
                .enWl,
                .enZa,
                .esEs,
                .esUs,
                .euEs,
                .faIr,
                .fiFi,
                .frCa,
                .frFr,
                .glEs,
                .heIl,
                .hiIn,
                .hrHr,
                .idId,
                .itIt,
                .jaJp,
                .koKr,
                .lvLv,
                .msMy,
                .nlNl,
                .noNo,
                .plPl,
                .ptBr,
                .ptPt,
                .roRo,
                .ruRu,
                .skSk,
                .soSo,
                .srRs,
                .svSe,
                .thTh,
                .tlPh,
                .ukUa,
                .viVn,
                .zhCn,
                .zhHk,
                .zhTw,
                .zuZa
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .afZa: return "af-ZA"
            case .arAe: return "ar-AE"
            case .arSa: return "ar-SA"
            case .caEs: return "ca-ES"
            case .csCz: return "cs-CZ"
            case .daDk: return "da-DK"
            case .deCh: return "de-CH"
            case .deDe: return "de-DE"
            case .elGr: return "el-GR"
            case .enAb: return "en-AB"
            case .enAu: return "en-AU"
            case .enGb: return "en-GB"
            case .enIe: return "en-IE"
            case .enIn: return "en-IN"
            case .enNz: return "en-NZ"
            case .enUs: return "en-US"
            case .enWl: return "en-WL"
            case .enZa: return "en-ZA"
            case .esEs: return "es-ES"
            case .esUs: return "es-US"
            case .euEs: return "eu-ES"
            case .faIr: return "fa-IR"
            case .fiFi: return "fi-FI"
            case .frCa: return "fr-CA"
            case .frFr: return "fr-FR"
            case .glEs: return "gl-ES"
            case .heIl: return "he-IL"
            case .hiIn: return "hi-IN"
            case .hrHr: return "hr-HR"
            case .idId: return "id-ID"
            case .itIt: return "it-IT"
            case .jaJp: return "ja-JP"
            case .koKr: return "ko-KR"
            case .lvLv: return "lv-LV"
            case .msMy: return "ms-MY"
            case .nlNl: return "nl-NL"
            case .noNo: return "no-NO"
            case .plPl: return "pl-PL"
            case .ptBr: return "pt-BR"
            case .ptPt: return "pt-PT"
            case .roRo: return "ro-RO"
            case .ruRu: return "ru-RU"
            case .skSk: return "sk-SK"
            case .soSo: return "so-SO"
            case .srRs: return "sr-RS"
            case .svSe: return "sv-SE"
            case .thTh: return "th-TH"
            case .tlPh: return "tl-PH"
            case .ukUa: return "uk-UA"
            case .viVn: return "vi-VN"
            case .zhCn: return "zh-CN"
            case .zhHk: return "zh-HK"
            case .zhTw: return "zh-TW"
            case .zuZa: return "zu-ZA"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The language code that represents the language identified in your audio, including the associated confidence score. If you enabled channel identification in your request and each channel contained a different language, you will have more than one LanguageWithScore result.
    public struct LanguageWithScore: Swift.Sendable {
        /// The language code of the identified language.
        public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
        /// The confidence score associated with the identified language code. Confidence scores are values between zero and one; larger values indicate a higher confidence in the identified language.
        public var score: Swift.Double

        public init(
            languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
            score: Swift.Double = 0.0
        ) {
            self.languageCode = languageCode
            self.score = score
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MediaEncoding: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case flac
        case oggOpus
        case pcm
        case sdkUnknown(Swift.String)

        public static var allCases: [MediaEncoding] {
            return [
                .flac,
                .oggOpus,
                .pcm
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .flac: return "flac"
            case .oggOpus: return "ogg-opus"
            case .pcm: return "pcm"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains entities identified as personal health information (PHI) in your transcription output, along with various associated attributes. Examples include category, confidence score, type, stability score, and start and end times.
    public struct MedicalEntity: Swift.Sendable {
        /// The category of information identified. The only category is PHI.
        public var category: Swift.String?
        /// The confidence score associated with the identified PHI entity in your audio. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words identified as PHI.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the utterance that was identified as PHI.
        public var endTime: Swift.Double
        /// The start time, in milliseconds, of the utterance that was identified as PHI.
        public var startTime: Swift.Double

        public init(
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            startTime: Swift.Double = 0.0
        ) {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct MedicalItem: Swift.Sendable {
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endTime: Swift.Double
        /// If speaker partitioning is enabled, Speaker labels the speaker of the specified item.
        public var speaker: Swift.String?
        /// The start time, in milliseconds, of the transcribed item.
        public var startTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?

        public init(
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            speaker: Swift.String? = nil,
            startTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.ItemType? = nil
        ) {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.startTime = startTime
            self.type = type
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
    public struct MedicalAlternative: Swift.Sendable {
        /// Contains entities identified as personal health information (PHI) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.MedicalEntity]?
        /// Contains words, phrases, or punctuation marks in your transcription output.
        public var items: [TranscribeStreamingClientTypes.MedicalItem]?
        /// Contains transcribed text.
        public var transcript: Swift.String?

        public init(
            entities: [TranscribeStreamingClientTypes.MedicalEntity]? = nil,
            items: [TranscribeStreamingClientTypes.MedicalItem]? = nil,
            transcript: Swift.String? = nil
        ) {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalContentIdentificationType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case phi
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalContentIdentificationType] {
            return [
                .phi
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .phi: return "PHI"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The Result associated with a . Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
    public struct MedicalResult: Swift.Sendable {
        /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
        public var alternatives: [TranscribeStreamingClientTypes.MedicalAlternative]?
        /// Indicates the channel identified for the Result.
        public var channelId: Swift.String?
        /// The end time, in milliseconds, of the Result.
        public var endTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// Provides a unique identifier for the Result.
        public var resultId: Swift.String?
        /// The start time, in milliseconds, of the Result.
        public var startTime: Swift.Double

        public init(
            alternatives: [TranscribeStreamingClientTypes.MedicalAlternative]? = nil,
            channelId: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            resultId: Swift.String? = nil,
            startTime: Swift.Double = 0.0
        ) {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.resultId = resultId
            self.startTime = startTime
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A wrapper for your audio chunks For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
    public struct MedicalScribeAudioEvent: Swift.Sendable {
        /// An audio blob containing the next segment of audio from your application, with a maximum duration of 1 second. The maximum size in bytes varies based on audio properties. Find recommended size in [Transcribing streaming best practices](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#best-practices). Size calculation: Duration (s) * Sample Rate (Hz) * Number of Channels * 2 (Bytes per Sample) For example, a 1-second chunk of 16 kHz, 2-channel, 16-bit audio would be 1 * 16000 * 2 * 2 = 64000 bytes. For 8 kHz, 1-channel, 16-bit audio, a 1-second chunk would be 1 * 8000 * 1 * 2 = 16000 bytes.
        /// This member is required.
        public var audioChunk: Foundation.Data?

        public init(
            audioChunk: Foundation.Data? = nil
        ) {
            self.audioChunk = audioChunk
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Specify details to configure the streaming session, including channel definitions, encryption settings, post-stream analytics settings, resource access role ARN and vocabulary settings. Whether you are starting a new session or resuming an existing session, your first event must be a MedicalScribeConfigurationEvent. If you are resuming a session, then this event must have the same configurations that you provided to start the session.
    public struct MedicalScribeConfigurationEvent: Swift.Sendable {
        /// Specify which speaker is on which audio channel.
        public var channelDefinitions: [TranscribeStreamingClientTypes.MedicalScribeChannelDefinition]?
        /// Specify the encryption settings for your streaming session.
        public var encryptionSettings: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings?
        /// Specify settings for post-stream analytics.
        /// This member is required.
        public var postStreamAnalyticsSettings: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings?
        /// The Amazon Resource Name (ARN) of an IAM role that has permissions to access the Amazon S3 output bucket you specified, and use your KMS key if supplied. If the role that you specify doesn’t have the appropriate permissions, your request fails. IAM role ARNs have the format arn:partition:iam::account:role/role-name-with-path. For example: arn:aws:iam::111122223333:role/Admin. For more information, see [Amazon Web Services HealthScribe](https://docs.aws.amazon.com/transcribe/latest/dg/health-scribe-streaming.html).
        /// This member is required.
        public var resourceAccessRoleArn: Swift.String?
        /// Specify how you want your custom vocabulary filter applied to the streaming session. To replace words with ***, specify mask. To delete words, specify remove. To flag words without changing them, specify tag.
        public var vocabularyFilterMethod: TranscribeStreamingClientTypes.MedicalScribeVocabularyFilterMethod?
        /// Specify the name of the custom vocabulary filter you want to include in your streaming session. Custom vocabulary filter names are case-sensitive. If you include VocabularyFilterName in the MedicalScribeConfigurationEvent, you must also include VocabularyFilterMethod.
        public var vocabularyFilterName: Swift.String?
        /// Specify the name of the custom vocabulary you want to use for your streaming session. Custom vocabulary names are case-sensitive.
        public var vocabularyName: Swift.String?

        public init(
            channelDefinitions: [TranscribeStreamingClientTypes.MedicalScribeChannelDefinition]? = nil,
            encryptionSettings: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings? = nil,
            postStreamAnalyticsSettings: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings? = nil,
            resourceAccessRoleArn: Swift.String? = nil,
            vocabularyFilterMethod: TranscribeStreamingClientTypes.MedicalScribeVocabularyFilterMethod? = nil,
            vocabularyFilterName: Swift.String? = nil,
            vocabularyName: Swift.String? = nil
        ) {
            self.channelDefinitions = channelDefinitions
            self.encryptionSettings = encryptionSettings
            self.postStreamAnalyticsSettings = postStreamAnalyticsSettings
            self.resourceAccessRoleArn = resourceAccessRoleArn
            self.vocabularyFilterMethod = vocabularyFilterMethod
            self.vocabularyFilterName = vocabularyFilterName
            self.vocabularyName = vocabularyName
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeSessionControlEventType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case endOfSession
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeSessionControlEventType] {
            return [
                .endOfSession
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .endOfSession: return "END_OF_SESSION"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Specify the lifecycle of your streaming session.
    public struct MedicalScribeSessionControlEvent: Swift.Sendable {
        /// The type of MedicalScribeSessionControlEvent. Possible Values:
        ///
        /// * END_OF_SESSION - Indicates the audio streaming is complete. After you send an END_OF_SESSION event, Amazon Web Services HealthScribe starts the post-stream analytics. The session can't be resumed after this event is sent. After Amazon Web Services HealthScribe processes the event, the real-time StreamStatus is COMPLETED. You get the StreamStatus and other stream details with the [GetMedicalScribeStream](https://docs.aws.amazon.com/transcribe/latest/APIReference/API_streaming_GetMedicalScribeStream.html) API operation. For more information about different streaming statuses, see the StreamStatus description in the [MedicalScribeStreamDetails](https://docs.aws.amazon.com/transcribe/latest/APIReference/API_streaming_MedicalScribeStreamDetails.html).
        /// This member is required.
        public var type: TranscribeStreamingClientTypes.MedicalScribeSessionControlEventType?

        public init(
            type: TranscribeStreamingClientTypes.MedicalScribeSessionControlEventType? = nil
        ) {
            self.type = type
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// An encoded stream of events. The stream is encoded as HTTP/2 data frames. An input stream consists of the following types of events. The first element of the input stream must be the MedicalScribeConfigurationEvent event type.
    ///
    /// * MedicalScribeConfigurationEvent
    ///
    /// * MedicalScribeAudioEvent
    ///
    /// * MedicalScribeSessionControlEvent
    public enum MedicalScribeInputStream: Swift.Sendable {
        /// A wrapper for your audio chunks For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
        case audioevent(TranscribeStreamingClientTypes.MedicalScribeAudioEvent)
        /// Specify the lifecycle of your streaming session, such as ending the session.
        case sessioncontrolevent(TranscribeStreamingClientTypes.MedicalScribeSessionControlEvent)
        /// Specify additional streaming session configurations beyond those provided in your initial start request headers. For example, specify channel definitions, encryption settings, and post-stream analytics settings. Whether you are starting a new session or resuming an existing session, your first event must be a MedicalScribeConfigurationEvent.
        case configurationevent(TranscribeStreamingClientTypes.MedicalScribeConfigurationEvent)
        case sdkUnknown(Swift.String)
    }
}

extension TranscribeStreamingClientTypes {

    public enum MedicalScribeTranscriptItemType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case pronunciation
        case punctuation
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalScribeTranscriptItemType] {
            return [
                .pronunciation,
                .punctuation
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .pronunciation: return "pronunciation"
            case .punctuation: return "punctuation"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct MedicalScribeTranscriptItem: Swift.Sendable {
        /// The start time, in milliseconds, of the transcribed item.
        public var beginAudioTime: Swift.Double
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word, phrase or punctuation mark that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endAudioTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.MedicalScribeTranscriptItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your configuration event. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool?

        public init(
            beginAudioTime: Swift.Double = 0.0,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endAudioTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.MedicalScribeTranscriptItemType? = nil,
            vocabularyFilterMatch: Swift.Bool? = nil
        ) {
            self.beginAudioTime = beginAudioTime
            self.confidence = confidence
            self.content = content
            self.endAudioTime = endAudioTime
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains a set of transcription results, along with additional information of the segment.
    public struct MedicalScribeTranscriptSegment: Swift.Sendable {
        /// The start time, in milliseconds, of the segment.
        public var beginAudioTime: Swift.Double
        /// Indicates which audio channel is associated with the MedicalScribeTranscriptSegment. If MedicalScribeChannelDefinition is not provided in the MedicalScribeConfigurationEvent, then this field will not be included.
        public var channelId: Swift.String?
        /// Contains transcribed text of the segment.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the segment.
        public var endAudioTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// Contains words, phrases, or punctuation marks in your segment.
        public var items: [TranscribeStreamingClientTypes.MedicalScribeTranscriptItem]?
        /// The identifier of the segment.
        public var segmentId: Swift.String?

        public init(
            beginAudioTime: Swift.Double = 0.0,
            channelId: Swift.String? = nil,
            content: Swift.String? = nil,
            endAudioTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            items: [TranscribeStreamingClientTypes.MedicalScribeTranscriptItem]? = nil,
            segmentId: Swift.String? = nil
        ) {
            self.beginAudioTime = beginAudioTime
            self.channelId = channelId
            self.content = content
            self.endAudioTime = endAudioTime
            self.isPartial = isPartial
            self.items = items
            self.segmentId = segmentId
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The event associated with MedicalScribeResultStream. Contains MedicalScribeTranscriptSegment, which contains segment related information.
    public struct MedicalScribeTranscriptEvent: Swift.Sendable {
        /// The TranscriptSegment associated with a MedicalScribeTranscriptEvent.
        public var transcriptSegment: TranscribeStreamingClientTypes.MedicalScribeTranscriptSegment?

        public init(
            transcriptSegment: TranscribeStreamingClientTypes.MedicalScribeTranscriptSegment? = nil
        ) {
            self.transcriptSegment = transcriptSegment
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Result stream where you will receive the output events. The details are provided in the MedicalScribeTranscriptEvent object.
    public enum MedicalScribeResultStream: Swift.Sendable {
        /// The transcript event that contains real-time transcription results.
        case transcriptevent(TranscribeStreamingClientTypes.MedicalScribeTranscriptEvent)
        case sdkUnknown(Swift.String)
    }
}

extension TranscribeStreamingClientTypes {

    /// The MedicalTranscript associated with a . MedicalTranscript contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct MedicalTranscript: Swift.Sendable {
        /// Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var results: [TranscribeStreamingClientTypes.MedicalResult]?

        public init(
            results: [TranscribeStreamingClientTypes.MedicalResult]? = nil
        ) {
            self.results = results
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The MedicalTranscriptEvent associated with a MedicalTranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct MedicalTranscriptEvent: Swift.Sendable {
        /// Contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var transcript: TranscribeStreamingClientTypes.MedicalTranscript?

        public init(
            transcript: TranscribeStreamingClientTypes.MedicalTranscript? = nil
        ) {
            self.transcript = transcript
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains detailed information about your streaming session.
    public enum MedicalTranscriptResultStream: Swift.Sendable {
        /// The MedicalTranscriptEvent associated with a MedicalTranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        case transcriptevent(TranscribeStreamingClientTypes.MedicalTranscriptEvent)
        case sdkUnknown(Swift.String)
    }
}

extension TranscribeStreamingClientTypes {

    public enum PartialResultsStability: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case high
        case low
        case medium
        case sdkUnknown(Swift.String)

        public static var allCases: [PartialResultsStability] {
            return [
                .high,
                .low,
                .medium
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .high: return "high"
            case .low: return "low"
            case .medium: return "medium"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The Result associated with a . Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
    public struct Result: Swift.Sendable {
        /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
        public var alternatives: [TranscribeStreamingClientTypes.Alternative]?
        /// Indicates which audio channel is associated with the Result.
        public var channelId: Swift.String?
        /// The end time, in milliseconds, of the Result.
        public var endTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// The language code that represents the language spoken in your audio stream.
        public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
        /// The language code of the dominant language identified in your stream. If you enabled channel identification and each channel of your audio contains a different language, you may have more than one result.
        public var languageIdentification: [TranscribeStreamingClientTypes.LanguageWithScore]?
        /// Provides a unique identifier for the Result.
        public var resultId: Swift.String?
        /// The start time, in milliseconds, of the Result.
        public var startTime: Swift.Double

        public init(
            alternatives: [TranscribeStreamingClientTypes.Alternative]? = nil,
            channelId: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
            languageIdentification: [TranscribeStreamingClientTypes.LanguageWithScore]? = nil,
            resultId: Swift.String? = nil,
            startTime: Swift.Double = 0.0
        ) {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.languageCode = languageCode
            self.languageIdentification = languageIdentification
            self.resultId = resultId
            self.startTime = startTime
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum Specialty: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case cardiology
        case neurology
        case oncology
        case primarycare
        case radiology
        case urology
        case sdkUnknown(Swift.String)

        public static var allCases: [Specialty] {
            return [
                .cardiology,
                .neurology,
                .oncology,
                .primarycare,
                .radiology,
                .urology
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .cardiology: return "CARDIOLOGY"
            case .neurology: return "NEUROLOGY"
            case .oncology: return "ONCOLOGY"
            case .primarycare: return "PRIMARYCARE"
            case .radiology: return "RADIOLOGY"
            case .urology: return "UROLOGY"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

extension TranscribeStreamingClientTypes {

    public enum VocabularyFilterMethod: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case mask
        case remove
        case tag
        case sdkUnknown(Swift.String)

        public static var allCases: [VocabularyFilterMethod] {
            return [
                .mask,
                .remove,
                .tag
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .mask: return "mask"
            case .remove: return "remove"
            case .tag: return "tag"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

public struct StartCallAnalyticsStreamTranscriptionInput: Swift.Sendable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in PiiEntityTypes is flagged upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is identified. You can’t set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in PiiEntityTypes is redacted upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is redacted. You can’t set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var enablePartialResultsStabilization: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. For a list of languages supported with real-time Call Analytics, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table.
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode?
    /// Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. For more information, see [Custom language models](https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html).
    public var languageModelName: Swift.String?
    /// Specify the encoding of your input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. Values must be comma-separated and can include: ADDRESS, BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, CREDIT_DEBIT_NUMBER, EMAIL, NAME, PHONE, PIN, SSN, or ALL. Note that if you include PiiEntityTypes in your request, you must also include ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or ContentIdentificationType in your request, but do not include PiiEntityTypes, all PII is redacted or identified.
    public var piiEntityTypes: Swift.String?
    /// Specify a name for your Call Analytics transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response.
    public var sessionId: Swift.String?
    /// Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterName: Swift.String?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyName: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enablePartialResultsStabilization: Swift.Bool? = false,
        languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyName: Swift.String? = nil
    ) {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.sessionId = sessionId
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyName = vocabularyName
    }
}

public struct StartCallAnalyticsStreamTranscriptionOutput: Swift.Sendable {
    /// Provides detailed information about your real-time Call Analytics session.
    public var callAnalyticsTranscriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream, Swift.Error>?
    /// Shows whether content identification was enabled for your Call Analytics transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Shows whether content redaction was enabled for your Call Analytics transcription.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Shows whether partial results stabilization was enabled for your Call Analytics transcription.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Provides the language code that you specified in your Call Analytics request.
    public var languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode?
    /// Provides the name of the custom language model that you specified in your Call Analytics request.
    public var languageModelName: Swift.String?
    /// Provides the media encoding you specified in your Call Analytics request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your Call Analytics request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the stabilization level used for your transcription.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Lists the PII entity types you specified in your Call Analytics request.
    public var piiEntityTypes: Swift.String?
    /// Provides the identifier for your real-time Call Analytics request.
    public var requestId: Swift.String?
    /// Provides the identifier for your Call Analytics transcription session.
    public var sessionId: Swift.String?
    /// Provides the vocabulary filtering method used in your Call Analytics transcription.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Provides the name of the custom vocabulary filter that you specified in your Call Analytics request.
    public var vocabularyFilterName: Swift.String?
    /// Provides the name of the custom vocabulary that you specified in your Call Analytics request.
    public var vocabularyName: Swift.String?

    public init(
        callAnalyticsTranscriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enablePartialResultsStabilization: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyName: Swift.String? = nil
    ) {
        self.callAnalyticsTranscriptResultStream = callAnalyticsTranscriptResultStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.requestId = requestId
        self.sessionId = sessionId
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyName = vocabularyName
    }
}

public struct StartMedicalScribeStreamInput: Swift.Sendable {
    /// Specify the input stream where you will send events in real time. The first element of the input stream must be a MedicalScribeConfigurationEvent.
    /// This member is required.
    public var inputStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalScribeInputStream, Swift.Error>?
    /// Specify the language code for your HealthScribe streaming session.
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.MedicalScribeLanguageCode?
    /// Specify the encoding used for the input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MedicalScribeMediaEncoding?
    /// Specify the sample rate of the input audio (in hertz). Amazon Web Services HealthScribe supports a range from 16,000 Hz to 48,000 Hz. The sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify an identifier for your streaming session (in UUID format). If you don't include a SessionId in your request, Amazon Web Services HealthScribe generates an ID and returns it in the response.
    public var sessionId: Swift.String?

    public init(
        inputStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalScribeInputStream, Swift.Error>? = nil,
        languageCode: TranscribeStreamingClientTypes.MedicalScribeLanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MedicalScribeMediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        sessionId: Swift.String? = nil
    ) {
        self.inputStream = inputStream
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.sessionId = sessionId
    }
}

public struct StartMedicalScribeStreamOutput: Swift.Sendable {
    /// The Language Code that you specified in your request. Same as provided in the StartMedicalScribeStreamRequest.
    public var languageCode: TranscribeStreamingClientTypes.MedicalScribeLanguageCode?
    /// The Media Encoding you specified in your request. Same as provided in the StartMedicalScribeStreamRequest
    public var mediaEncoding: TranscribeStreamingClientTypes.MedicalScribeMediaEncoding?
    /// The sample rate (in hertz) that you specified in your request. Same as provided in the StartMedicalScribeStreamRequest
    public var mediaSampleRateHertz: Swift.Int?
    /// The unique identifier for your streaming request.
    public var requestId: Swift.String?
    /// The result stream where you will receive the output events.
    public var resultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalScribeResultStream, Swift.Error>?
    /// The identifier (in UUID format) for your streaming session. If you already started streaming, this is same ID as the one you specified in your initial StartMedicalScribeStreamRequest.
    public var sessionId: Swift.String?

    public init(
        languageCode: TranscribeStreamingClientTypes.MedicalScribeLanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MedicalScribeMediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        requestId: Swift.String? = nil,
        resultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalScribeResultStream, Swift.Error>? = nil,
        sessionId: Swift.String? = nil
    ) {
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.requestId = requestId
        self.resultStream = resultStream
        self.sessionId = sessionId
    }
}

extension TranscribeStreamingClientTypes {

    public enum ModelType: Swift.Sendable, Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Hashable {
        case conversation
        case dictation
        case sdkUnknown(Swift.String)

        public static var allCases: [ModelType] {
            return [
                .conversation,
                .dictation
            ]
        }

        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }

        public var rawValue: Swift.String {
            switch self {
            case .conversation: return "CONVERSATION"
            case .dictation: return "DICTATION"
            case let .sdkUnknown(s): return s
            }
        }
    }
}

public struct StartMedicalStreamTranscriptionInput: Swift.Sendable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personal health information (PHI) identified in your transcript. Content identification is performed at the segment level; PHI is flagged upon complete transcription of an audio segment. For more information, see [Identifying personal health information (PHI) in a transcription](https://docs.aws.amazon.com/transcribe/latest/dg/phi-id.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. If you include EnableChannelIdentification in your request, you must also include NumberOfChannels. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. Amazon Transcribe Medical only supports US English (en-US).
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the encoding used for the input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Amazon Transcribe Medical supports a range from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. This value must be 2, as only two channels are supported. If your audio doesn't contain multiple channels, do not include this parameter in your request. If you include NumberOfChannels in your request, you must also include EnableChannelIdentification.
    public var numberOfChannels: Swift.Int?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe Medical generates an ID and returns it in the response.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool?
    /// Specify the medical specialty contained in your audio.
    /// This member is required.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Specify the type of input audio. For example, choose DICTATION for a provider dictating patient notes and CONVERSATION for a dialogue between a patient and a medical professional.
    /// This member is required.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive.
    public var vocabularyName: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool? = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool? = false,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    ) {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.type = type
        self.vocabularyName = vocabularyName
    }
}

public struct StartMedicalStreamTranscriptionOutput: Swift.Sendable {
    /// Shows whether content identification was enabled for your transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Shows whether channel identification was enabled for your transcription.
    public var enableChannelIdentification: Swift.Bool
    /// Provides the language code that you specified in your request. This must be en-US.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the media encoding you specified in your request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the number of channels that you specified in your request.
    public var numberOfChannels: Swift.Int?
    /// Provides the identifier for your streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your transcription session.
    public var sessionId: Swift.String?
    /// Shows whether speaker partitioning was enabled for your transcription.
    public var showSpeakerLabel: Swift.Bool
    /// Provides the medical specialty that you specified in your request.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Provides detailed information about your streaming session.
    public var transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream, Swift.Error>?
    /// Provides the type of audio you specified in your request.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Provides the name of the custom vocabulary that you specified in your request.
    public var vocabularyName: Swift.String?

    public init(
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream, Swift.Error>? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    ) {
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.requestId = requestId
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.transcriptResultStream = transcriptResultStream
        self.type = type
        self.vocabularyName = vocabularyName
    }
}

public struct StartStreamTranscriptionInput: Swift.Sendable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in PiiEntityTypes is flagged upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is identified. You can’t set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in PiiEntityTypes is redacted upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is redacted. You can’t set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. If you include EnableChannelIdentification in your request, you must also include NumberOfChannels. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool?
    /// Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var enablePartialResultsStabilization: Swift.Bool?
    /// Enables automatic language identification for your transcription. If you include IdentifyLanguage, you must include a list of language codes, using LanguageOptions, that you think may be present in your audio stream. You can also include a preferred language using PreferredLanguage. Adding a preferred language can help Amazon Transcribe identify the language faster than if you omit this parameter. If you have multi-channel audio that contains different languages on each channel, and you've enabled channel identification, automatic language identification identifies the dominant language on each audio channel. Note that you must include either LanguageCode or IdentifyLanguage or IdentifyMultipleLanguages in your request. If you include more than one of these parameters, your transcription job fails. Streaming language identification can't be combined with custom language models or redaction.
    public var identifyLanguage: Swift.Bool?
    /// Enables automatic multi-language identification in your transcription job request. Use this parameter if your stream contains more than one language. If your stream contains only one language, use IdentifyLanguage instead. If you include IdentifyMultipleLanguages, you must include a list of language codes, using LanguageOptions, that you think may be present in your stream. If you want to apply a custom vocabulary or a custom vocabulary filter to your automatic multiple language identification request, include VocabularyNames or VocabularyFilterNames. Note that you must include one of LanguageCode, IdentifyLanguage, or IdentifyMultipleLanguages in your request. If you include more than one of these parameters, your transcription job fails.
    public var identifyMultipleLanguages: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification. For a list of languages supported with Amazon Transcribe streaming, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. For more information, see [Custom language models](https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html).
    public var languageModelName: Swift.String?
    /// Specify two or more language codes that represent the languages you think may be present in your media; including more than five is not recommended. Including language options can improve the accuracy of language identification. If you include LanguageOptions in your request, you must also include IdentifyLanguage or IdentifyMultipleLanguages. For a list of languages supported with Amazon Transcribe streaming, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table. You can only include one language dialect per language per stream. For example, you cannot include en-US and en-AU in the same request.
    public var languageOptions: Swift.String?
    /// Specify the encoding of your input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. This value must be 2, as only two channels are supported. If your audio doesn't contain multiple channels, do not include this parameter in your request. If you include NumberOfChannels in your request, you must also include EnableChannelIdentification.
    public var numberOfChannels: Swift.Int?
    /// Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. Values must be comma-separated and can include: ADDRESS, BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, CREDIT_DEBIT_NUMBER, EMAIL, NAME, PHONE, PIN, SSN, or ALL. Note that if you include PiiEntityTypes in your request, you must also include ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or ContentIdentificationType in your request, but do not include PiiEntityTypes, all PII is redacted or identified.
    public var piiEntityTypes: Swift.String?
    /// Specify a preferred language from the subset of languages codes you specified in LanguageOptions. You can only use this parameter if you've included IdentifyLanguage and LanguageOptions in your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool?
    /// Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterName: Swift.String?
    /// Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If none of the languages of the specified custom vocabulary filters match the language identified in your media, your job fails. This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary filter with your transcription, use the VocabularyFilterName parameter instead. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterNames: Swift.String?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyName: Swift.String?
    /// Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If none of the languages of the specified custom vocabularies match the language identified in your media, your job fails. This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyNames: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool? = false,
        enablePartialResultsStabilization: Swift.Bool? = false,
        identifyLanguage: Swift.Bool? = false,
        identifyMultipleLanguages: Swift.Bool? = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool? = false,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    ) {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.identifyMultipleLanguages = identifyMultipleLanguages
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}

extension TranscribeStreamingClientTypes {

    /// The Transcript associated with a . Transcript contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct Transcript: Swift.Sendable {
        /// Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var results: [TranscribeStreamingClientTypes.Result]?

        public init(
            results: [TranscribeStreamingClientTypes.Result]? = nil
        ) {
            self.results = results
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// The TranscriptEvent associated with a TranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct TranscriptEvent: Swift.Sendable {
        /// Contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var transcript: TranscribeStreamingClientTypes.Transcript?

        public init(
            transcript: TranscribeStreamingClientTypes.Transcript? = nil
        ) {
            self.transcript = transcript
        }
    }
}

extension TranscribeStreamingClientTypes {

    /// Contains detailed information about your streaming session.
    public enum TranscriptResultStream: Swift.Sendable {
        /// Contains Transcript, which contains Results. The  object contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
        case transcriptevent(TranscribeStreamingClientTypes.TranscriptEvent)
        case sdkUnknown(Swift.String)
    }
}

public struct StartStreamTranscriptionOutput: Swift.Sendable {
    /// Shows whether content identification was enabled for your transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Shows whether content redaction was enabled for your transcription.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Shows whether channel identification was enabled for your transcription.
    public var enableChannelIdentification: Swift.Bool
    /// Shows whether partial results stabilization was enabled for your transcription.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Shows whether automatic language identification was enabled for your transcription.
    public var identifyLanguage: Swift.Bool
    /// Shows whether automatic multi-language identification was enabled for your transcription.
    public var identifyMultipleLanguages: Swift.Bool
    /// Provides the language code that you specified in your request.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the name of the custom language model that you specified in your request.
    public var languageModelName: Swift.String?
    /// Provides the language codes that you specified in your request.
    public var languageOptions: Swift.String?
    /// Provides the media encoding you specified in your request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the number of channels that you specified in your request.
    public var numberOfChannels: Swift.Int?
    /// Provides the stabilization level used for your transcription.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Lists the PII entity types you specified in your request.
    public var piiEntityTypes: Swift.String?
    /// Provides the preferred language that you specified in your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the identifier for your streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your transcription session.
    public var sessionId: Swift.String?
    /// Shows whether speaker partitioning was enabled for your transcription.
    public var showSpeakerLabel: Swift.Bool
    /// Provides detailed information about your streaming session.
    public var transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.TranscriptResultStream, Swift.Error>?
    /// Provides the vocabulary filtering method used in your transcription.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Provides the name of the custom vocabulary filter that you specified in your request.
    public var vocabularyFilterName: Swift.String?
    /// Provides the names of the custom vocabulary filters that you specified in your request.
    public var vocabularyFilterNames: Swift.String?
    /// Provides the name of the custom vocabulary that you specified in your request.
    public var vocabularyName: Swift.String?
    /// Provides the names of the custom vocabularies that you specified in your request.
    public var vocabularyNames: Swift.String?

    public init(
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        enablePartialResultsStabilization: Swift.Bool = false,
        identifyLanguage: Swift.Bool = false,
        identifyMultipleLanguages: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.TranscriptResultStream, Swift.Error>? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    ) {
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.identifyMultipleLanguages = identifyMultipleLanguages
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.requestId = requestId
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.transcriptResultStream = transcriptResultStream
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}

extension GetMedicalScribeStreamInput {

    static func urlPathProvider(_ value: GetMedicalScribeStreamInput) -> Swift.String? {
        guard let sessionId = value.sessionId else {
            return nil
        }
        return "/medical-scribe-stream/\(sessionId.urlPercentEncoding())"
    }
}

extension StartCallAnalyticsStreamTranscriptionInput {

    static func urlPathProvider(_ value: StartCallAnalyticsStreamTranscriptionInput) -> Swift.String? {
        return "/call-analytics-stream-transcription"
    }
}

extension StartCallAnalyticsStreamTranscriptionInput {

    static func headerProvider(_ value: StartCallAnalyticsStreamTranscriptionInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let contentIdentificationType = value.contentIdentificationType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let contentRedactionType = value.contentRedactionType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-redaction-type", value: Swift.String(contentRedactionType.rawValue)))
        }
        if let enablePartialResultsStabilization = value.enablePartialResultsStabilization {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-partial-results-stabilization", value: Swift.String(enablePartialResultsStabilization)))
        }
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let languageModelName = value.languageModelName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-model-name", value: Swift.String(languageModelName)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let partialResultsStability = value.partialResultsStability {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-partial-results-stability", value: Swift.String(partialResultsStability.rawValue)))
        }
        if let piiEntityTypes = value.piiEntityTypes {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-pii-entity-types", value: Swift.String(piiEntityTypes)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let vocabularyFilterMethod = value.vocabularyFilterMethod {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-method", value: Swift.String(vocabularyFilterMethod.rawValue)))
        }
        if let vocabularyFilterName = value.vocabularyFilterName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-name", value: Swift.String(vocabularyFilterName)))
        }
        if let vocabularyName = value.vocabularyName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        return items
    }
}

extension StartMedicalScribeStreamInput {

    static func urlPathProvider(_ value: StartMedicalScribeStreamInput) -> Swift.String? {
        return "/medical-scribe-stream"
    }
}

extension StartMedicalScribeStreamInput {

    static func headerProvider(_ value: StartMedicalScribeStreamInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        return items
    }
}

extension StartMedicalStreamTranscriptionInput {

    static func urlPathProvider(_ value: StartMedicalStreamTranscriptionInput) -> Swift.String? {
        return "/medical-stream-transcription"
    }
}

extension StartMedicalStreamTranscriptionInput {

    static func headerProvider(_ value: StartMedicalStreamTranscriptionInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let contentIdentificationType = value.contentIdentificationType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let enableChannelIdentification = value.enableChannelIdentification {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-channel-identification", value: Swift.String(enableChannelIdentification)))
        }
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let numberOfChannels = value.numberOfChannels {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-number-of-channels", value: Swift.String(numberOfChannels)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let showSpeakerLabel = value.showSpeakerLabel {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-show-speaker-label", value: Swift.String(showSpeakerLabel)))
        }
        if let specialty = value.specialty {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-specialty", value: Swift.String(specialty.rawValue)))
        }
        if let type = value.type {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-type", value: Swift.String(type.rawValue)))
        }
        if let vocabularyName = value.vocabularyName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        return items
    }
}

extension StartStreamTranscriptionInput {

    static func urlPathProvider(_ value: StartStreamTranscriptionInput) -> Swift.String? {
        return "/stream-transcription"
    }
}

extension StartStreamTranscriptionInput {

    static func headerProvider(_ value: StartStreamTranscriptionInput) -> SmithyHTTPAPI.Headers {
        var items = SmithyHTTPAPI.Headers()
        if let contentIdentificationType = value.contentIdentificationType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let contentRedactionType = value.contentRedactionType {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-content-redaction-type", value: Swift.String(contentRedactionType.rawValue)))
        }
        if let enableChannelIdentification = value.enableChannelIdentification {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-channel-identification", value: Swift.String(enableChannelIdentification)))
        }
        if let enablePartialResultsStabilization = value.enablePartialResultsStabilization {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-enable-partial-results-stabilization", value: Swift.String(enablePartialResultsStabilization)))
        }
        if let identifyLanguage = value.identifyLanguage {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-identify-language", value: Swift.String(identifyLanguage)))
        }
        if let identifyMultipleLanguages = value.identifyMultipleLanguages {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-identify-multiple-languages", value: Swift.String(identifyMultipleLanguages)))
        }
        if let languageCode = value.languageCode {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let languageModelName = value.languageModelName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-model-name", value: Swift.String(languageModelName)))
        }
        if let languageOptions = value.languageOptions {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-language-options", value: Swift.String(languageOptions)))
        }
        if let mediaEncoding = value.mediaEncoding {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = value.mediaSampleRateHertz {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let numberOfChannels = value.numberOfChannels {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-number-of-channels", value: Swift.String(numberOfChannels)))
        }
        if let partialResultsStability = value.partialResultsStability {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-partial-results-stability", value: Swift.String(partialResultsStability.rawValue)))
        }
        if let piiEntityTypes = value.piiEntityTypes {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-pii-entity-types", value: Swift.String(piiEntityTypes)))
        }
        if let preferredLanguage = value.preferredLanguage {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-preferred-language", value: Swift.String(preferredLanguage.rawValue)))
        }
        if let sessionId = value.sessionId {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let showSpeakerLabel = value.showSpeakerLabel {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-show-speaker-label", value: Swift.String(showSpeakerLabel)))
        }
        if let vocabularyFilterMethod = value.vocabularyFilterMethod {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-method", value: Swift.String(vocabularyFilterMethod.rawValue)))
        }
        if let vocabularyFilterName = value.vocabularyFilterName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-name", value: Swift.String(vocabularyFilterName)))
        }
        if let vocabularyFilterNames = value.vocabularyFilterNames {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-filter-names", value: Swift.String(vocabularyFilterNames)))
        }
        if let vocabularyName = value.vocabularyName {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        if let vocabularyNames = value.vocabularyNames {
            items.add(SmithyHTTPAPI.Header(name: "x-amzn-transcribe-vocabulary-names", value: Swift.String(vocabularyNames)))
        }
        return items
    }
}

extension GetMedicalScribeStreamOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> GetMedicalScribeStreamOutput {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let reader = responseReader
        var value = GetMedicalScribeStreamOutput()
        value.medicalScribeStreamDetails = try reader["MedicalScribeStreamDetails"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalScribeStreamDetails.read(from:))
        return value
    }
}

extension StartCallAnalyticsStreamTranscriptionOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartCallAnalyticsStreamTranscriptionOutput {
        var value = StartCallAnalyticsStreamTranscriptionOutput()
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            value.contentIdentificationType = TranscribeStreamingClientTypes.ContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        }
        if let contentRedactionTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-redaction-type") {
            value.contentRedactionType = TranscribeStreamingClientTypes.ContentRedactionType(rawValue: contentRedactionTypeHeaderValue)
        }
        if let enablePartialResultsStabilizationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-partial-results-stabilization") {
            value.enablePartialResultsStabilization = Swift.Bool(enablePartialResultsStabilizationHeaderValue) ?? false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.CallAnalyticsLanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let languageModelNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-model-name") {
            value.languageModelName = languageModelNameHeaderValue
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let partialResultsStabilityHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-partial-results-stability") {
            value.partialResultsStability = TranscribeStreamingClientTypes.PartialResultsStability(rawValue: partialResultsStabilityHeaderValue)
        }
        if let piiEntityTypesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-pii-entity-types") {
            value.piiEntityTypes = piiEntityTypesHeaderValue
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if let vocabularyFilterMethodHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-method") {
            value.vocabularyFilterMethod = TranscribeStreamingClientTypes.VocabularyFilterMethod(rawValue: vocabularyFilterMethodHeaderValue)
        }
        if let vocabularyFilterNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-name") {
            value.vocabularyFilterName = vocabularyFilterNameHeaderValue
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            value.vocabularyName = vocabularyNameHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream.unmarshal)
            value.callAnalyticsTranscriptResultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

extension StartMedicalScribeStreamOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartMedicalScribeStreamOutput {
        var value = StartMedicalScribeStreamOutput()
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.MedicalScribeLanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MedicalScribeMediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.MedicalScribeResultStream.unmarshal)
            value.resultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

extension StartMedicalStreamTranscriptionOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartMedicalStreamTranscriptionOutput {
        var value = StartMedicalStreamTranscriptionOutput()
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            value.contentIdentificationType = TranscribeStreamingClientTypes.MedicalContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        }
        if let enableChannelIdentificationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-channel-identification") {
            value.enableChannelIdentification = Swift.Bool(enableChannelIdentificationHeaderValue) ?? false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.LanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let numberOfChannelsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-number-of-channels") {
            value.numberOfChannels = Swift.Int(numberOfChannelsHeaderValue) ?? 0
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if let showSpeakerLabelHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-show-speaker-label") {
            value.showSpeakerLabel = Swift.Bool(showSpeakerLabelHeaderValue) ?? false
        }
        if let specialtyHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-specialty") {
            value.specialty = TranscribeStreamingClientTypes.Specialty(rawValue: specialtyHeaderValue)
        }
        if let typeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-type") {
            value.type = TranscribeStreamingClientTypes.ModelType(rawValue: typeHeaderValue)
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            value.vocabularyName = vocabularyNameHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.MedicalTranscriptResultStream.unmarshal)
            value.transcriptResultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

extension StartStreamTranscriptionOutput {

    static func httpOutput(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> StartStreamTranscriptionOutput {
        var value = StartStreamTranscriptionOutput()
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            value.contentIdentificationType = TranscribeStreamingClientTypes.ContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        }
        if let contentRedactionTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-redaction-type") {
            value.contentRedactionType = TranscribeStreamingClientTypes.ContentRedactionType(rawValue: contentRedactionTypeHeaderValue)
        }
        if let enableChannelIdentificationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-channel-identification") {
            value.enableChannelIdentification = Swift.Bool(enableChannelIdentificationHeaderValue) ?? false
        }
        if let enablePartialResultsStabilizationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-partial-results-stabilization") {
            value.enablePartialResultsStabilization = Swift.Bool(enablePartialResultsStabilizationHeaderValue) ?? false
        }
        if let identifyLanguageHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-identify-language") {
            value.identifyLanguage = Swift.Bool(identifyLanguageHeaderValue) ?? false
        }
        if let identifyMultipleLanguagesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-identify-multiple-languages") {
            value.identifyMultipleLanguages = Swift.Bool(identifyMultipleLanguagesHeaderValue) ?? false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            value.languageCode = TranscribeStreamingClientTypes.LanguageCode(rawValue: languageCodeHeaderValue)
        }
        if let languageModelNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-model-name") {
            value.languageModelName = languageModelNameHeaderValue
        }
        if let languageOptionsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-options") {
            value.languageOptions = languageOptionsHeaderValue
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            value.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            value.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        }
        if let numberOfChannelsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-number-of-channels") {
            value.numberOfChannels = Swift.Int(numberOfChannelsHeaderValue) ?? 0
        }
        if let partialResultsStabilityHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-partial-results-stability") {
            value.partialResultsStability = TranscribeStreamingClientTypes.PartialResultsStability(rawValue: partialResultsStabilityHeaderValue)
        }
        if let piiEntityTypesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-pii-entity-types") {
            value.piiEntityTypes = piiEntityTypesHeaderValue
        }
        if let preferredLanguageHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-preferred-language") {
            value.preferredLanguage = TranscribeStreamingClientTypes.LanguageCode(rawValue: preferredLanguageHeaderValue)
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            value.requestId = requestIdHeaderValue
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            value.sessionId = sessionIdHeaderValue
        }
        if let showSpeakerLabelHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-show-speaker-label") {
            value.showSpeakerLabel = Swift.Bool(showSpeakerLabelHeaderValue) ?? false
        }
        if let vocabularyFilterMethodHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-method") {
            value.vocabularyFilterMethod = TranscribeStreamingClientTypes.VocabularyFilterMethod(rawValue: vocabularyFilterMethodHeaderValue)
        }
        if let vocabularyFilterNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-name") {
            value.vocabularyFilterName = vocabularyFilterNameHeaderValue
        }
        if let vocabularyFilterNamesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-names") {
            value.vocabularyFilterNames = vocabularyFilterNamesHeaderValue
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            value.vocabularyName = vocabularyNameHeaderValue
        }
        if let vocabularyNamesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-names") {
            value.vocabularyNames = vocabularyNamesHeaderValue
        }
        if case .stream(let stream) = httpResponse.body {
            let messageDecoder = SmithyEventStreams.DefaultMessageDecoder()
            let decoderStream = SmithyEventStreams.DefaultMessageDecoderStream(stream: stream, messageDecoder: messageDecoder, unmarshalClosure: TranscribeStreamingClientTypes.TranscriptResultStream.unmarshal)
            value.transcriptResultStream = decoderStream.toAsyncStream()
        }
        return value
    }
}

enum GetMedicalScribeStreamOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ResourceNotFoundException": return try ResourceNotFoundException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

enum StartCallAnalyticsStreamTranscriptionOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

enum StartMedicalScribeStreamOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

enum StartMedicalStreamTranscriptionOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

enum StartStreamTranscriptionOutputError {

    static func httpError(from httpResponse: SmithyHTTPAPI.HTTPResponse) async throws -> Swift.Error {
        let data = try await httpResponse.data()
        let responseReader = try SmithyJSON.Reader.from(data: data)
        let baseError = try AWSClientRuntime.RestJSONError(httpResponse: httpResponse, responseReader: responseReader, noErrorWrapping: false)
        if let error = baseError.customError() { return error }
        switch baseError.code {
            case "BadRequestException": return try BadRequestException.makeError(baseError: baseError)
            case "ConflictException": return try ConflictException.makeError(baseError: baseError)
            case "InternalFailureException": return try InternalFailureException.makeError(baseError: baseError)
            case "LimitExceededException": return try LimitExceededException.makeError(baseError: baseError)
            case "ServiceUnavailableException": return try ServiceUnavailableException.makeError(baseError: baseError)
            default: return try AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(baseError: baseError)
        }
    }
}

extension BadRequestException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> BadRequestException {
        let reader = baseError.errorBodyReader
        var value = BadRequestException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension InternalFailureException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> InternalFailureException {
        let reader = baseError.errorBodyReader
        var value = InternalFailureException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension LimitExceededException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> LimitExceededException {
        let reader = baseError.errorBodyReader
        var value = LimitExceededException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension ResourceNotFoundException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> ResourceNotFoundException {
        let reader = baseError.errorBodyReader
        var value = ResourceNotFoundException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension ServiceUnavailableException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> ServiceUnavailableException {
        let reader = baseError.errorBodyReader
        var value = ServiceUnavailableException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension ConflictException {

    static func makeError(baseError: AWSClientRuntime.RestJSONError) throws -> ConflictException {
        let reader = baseError.errorBodyReader
        var value = ConflictException()
        value.properties.message = try reader["Message"].readIfPresent()
        value.httpResponse = baseError.httpResponse
        value.requestID = baseError.requestID
        value.message = baseError.message
        return value
    }
}

extension TranscribeStreamingClientTypes.AudioStream {
    static var marshal: SmithyEventStreamsAPI.MarshalClosure<TranscribeStreamingClientTypes.AudioStream> {
        { (self) in
            var headers: [SmithyEventStreamsAPI.Header] = [.init(name: ":message-type", value: .string("event"))]
            var payload: Foundation.Data? = nil
            switch self {
            case .audioevent(let value):
                headers.append(.init(name: ":event-type", value: .string("AudioEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/octet-stream")))
                payload = value.audioChunk
            case .configurationevent(let value):
                headers.append(.init(name: ":event-type", value: .string("ConfigurationEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/json")))
                let writer = SmithyJSON.Writer(nodeInfo: "")
                try writer["ChannelDefinitions"].write(value.channelDefinitions, with: SmithyReadWrite.listWritingClosure(memberWritingClosure: TranscribeStreamingClientTypes.ChannelDefinition.write(value:to:), memberNodeInfo: "member", isFlattened: false))
                try writer["PostCallAnalyticsSettings"].write(value.postCallAnalyticsSettings, with: TranscribeStreamingClientTypes.PostCallAnalyticsSettings.write(value:to:))
                payload = try writer.data()
            case .sdkUnknown(_):
                throw Smithy.ClientError.unknownError("cannot serialize the unknown event type!")
            }
            return SmithyEventStreamsAPI.Message(headers: headers, payload: payload ?? .init())
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeInputStream {
    static var marshal: SmithyEventStreamsAPI.MarshalClosure<TranscribeStreamingClientTypes.MedicalScribeInputStream> {
        { (self) in
            var headers: [SmithyEventStreamsAPI.Header] = [.init(name: ":message-type", value: .string("event"))]
            var payload: Foundation.Data? = nil
            switch self {
            case .audioevent(let value):
                headers.append(.init(name: ":event-type", value: .string("AudioEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/octet-stream")))
                payload = value.audioChunk
            case .sessioncontrolevent(let value):
                headers.append(.init(name: ":event-type", value: .string("SessionControlEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/json")))
                let writer = SmithyJSON.Writer(nodeInfo: "")
                try writer["Type"].write(value.type, with: SmithyReadWrite.WritingClosureBox<TranscribeStreamingClientTypes.MedicalScribeSessionControlEventType>().write(value:to:))
                payload = try writer.data()
            case .configurationevent(let value):
                headers.append(.init(name: ":event-type", value: .string("ConfigurationEvent")))
                headers.append(.init(name: ":content-type", value: .string("application/json")))
                let writer = SmithyJSON.Writer(nodeInfo: "")
                try writer["VocabularyName"].write(value.vocabularyName, with: SmithyReadWrite.WritingClosures.writeString(value:to:))
                try writer["VocabularyFilterName"].write(value.vocabularyFilterName, with: SmithyReadWrite.WritingClosures.writeString(value:to:))
                try writer["VocabularyFilterMethod"].write(value.vocabularyFilterMethod, with: SmithyReadWrite.WritingClosureBox<TranscribeStreamingClientTypes.MedicalScribeVocabularyFilterMethod>().write(value:to:))
                try writer["ResourceAccessRoleArn"].write(value.resourceAccessRoleArn, with: SmithyReadWrite.WritingClosures.writeString(value:to:))
                try writer["ChannelDefinitions"].write(value.channelDefinitions, with: SmithyReadWrite.listWritingClosure(memberWritingClosure: TranscribeStreamingClientTypes.MedicalScribeChannelDefinition.write(value:to:), memberNodeInfo: "member", isFlattened: false))
                try writer["EncryptionSettings"].write(value.encryptionSettings, with: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings.write(value:to:))
                try writer["PostStreamAnalyticsSettings"].write(value.postStreamAnalyticsSettings, with: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings.write(value:to:))
                payload = try writer.data()
            case .sdkUnknown(_):
                throw Smithy.ClientError.unknownError("cannot serialize the unknown event type!")
            }
            return SmithyEventStreamsAPI.Message(headers: headers, payload: payload ?? .init())
        }
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "UtteranceEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.UtteranceEvent.read(from:))
                    return .utteranceevent(value)
                case "CategoryEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.CategoryEvent.read(from:))
                    return .categoryevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.MedicalScribeResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "TranscriptEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.MedicalScribeTranscriptEvent.read(from:))
                    return .transcriptevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalTranscriptResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.MedicalTranscriptResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "TranscriptEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.MedicalTranscriptEvent.read(from:))
                    return .transcriptevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension TranscribeStreamingClientTypes.TranscriptResultStream {
    static var unmarshal: SmithyEventStreamsAPI.UnmarshalClosure<TranscribeStreamingClientTypes.TranscriptResultStream> {
        { message in
            switch try message.type() {
            case .event(let params):
                switch params.eventType {
                case "TranscriptEvent":
                    let value = try SmithyJSON.Reader.readFrom(message.payload, with: TranscribeStreamingClientTypes.TranscriptEvent.read(from:))
                    return .transcriptevent(value)
                default:
                    return .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
                }
            case .exception(let params):
                let makeError: (SmithyEventStreamsAPI.Message, SmithyEventStreamsAPI.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                    switch params.exceptionType {
                    case "BadRequestException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: BadRequestException.read(from:))
                        return value
                    case "LimitExceededException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: LimitExceededException.read(from:))
                        return value
                    case "InternalFailureException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: InternalFailureException.read(from:))
                        return value
                    case "ConflictException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ConflictException.read(from:))
                        return value
                    case "ServiceUnavailableException":
                        let value = try SmithyJSON.Reader.readFrom(message.payload, with: ServiceUnavailableException.read(from:))
                        return value
                    default:
                        let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                        return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                    }
                }
                let error = try makeError(message, params)
                throw error
            case .error(let params):
                let httpResponse = SmithyHTTPAPI.HTTPResponse(body: .data(message.payload), statusCode: .ok)
                throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
            case .unknown(messageType: let messageType):
                throw Smithy.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
            }
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeStreamDetails {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribeStreamDetails {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribeStreamDetails()
        value.sessionId = try reader["SessionId"].readIfPresent()
        value.streamCreatedAt = try reader["StreamCreatedAt"].readTimestampIfPresent(format: SmithyTimestamps.TimestampFormat.epochSeconds)
        value.streamEndedAt = try reader["StreamEndedAt"].readTimestampIfPresent(format: SmithyTimestamps.TimestampFormat.epochSeconds)
        value.languageCode = try reader["LanguageCode"].readIfPresent()
        value.mediaSampleRateHertz = try reader["MediaSampleRateHertz"].readIfPresent()
        value.mediaEncoding = try reader["MediaEncoding"].readIfPresent()
        value.vocabularyName = try reader["VocabularyName"].readIfPresent()
        value.vocabularyFilterName = try reader["VocabularyFilterName"].readIfPresent()
        value.vocabularyFilterMethod = try reader["VocabularyFilterMethod"].readIfPresent()
        value.resourceAccessRoleArn = try reader["ResourceAccessRoleArn"].readIfPresent()
        value.channelDefinitions = try reader["ChannelDefinitions"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalScribeChannelDefinition.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.encryptionSettings = try reader["EncryptionSettings"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings.read(from:))
        value.streamStatus = try reader["StreamStatus"].readIfPresent()
        value.postStreamAnalyticsSettings = try reader["PostStreamAnalyticsSettings"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings.read(from:))
        value.postStreamAnalyticsResult = try reader["PostStreamAnalyticsResult"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsResult.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsResult {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsResult {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsResult()
        value.clinicalNoteGenerationResult = try reader["ClinicalNoteGenerationResult"].readIfPresent(with: TranscribeStreamingClientTypes.ClinicalNoteGenerationResult.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.ClinicalNoteGenerationResult {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.ClinicalNoteGenerationResult {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.ClinicalNoteGenerationResult()
        value.clinicalNoteOutputLocation = try reader["ClinicalNoteOutputLocation"].readIfPresent()
        value.transcriptOutputLocation = try reader["TranscriptOutputLocation"].readIfPresent()
        value.status = try reader["Status"].readIfPresent()
        value.failureReason = try reader["FailureReason"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings {

    static func write(value: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ClinicalNoteGenerationSettings"].write(value.clinicalNoteGenerationSettings, with: TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings.write(value:to:))
    }

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings()
        value.clinicalNoteGenerationSettings = try reader["ClinicalNoteGenerationSettings"].readIfPresent(with: TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings {

    static func write(value: TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["OutputBucketName"].write(value.outputBucketName)
    }

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.ClinicalNoteGenerationSettings()
        value.outputBucketName = try reader["OutputBucketName"].readIfPresent() ?? ""
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings {

    static func write(value: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["KmsEncryptionContext"].writeMap(value.kmsEncryptionContext, valueWritingClosure: SmithyReadWrite.WritingClosures.writeString(value:to:), keyNodeInfo: "key", valueNodeInfo: "value", isFlattened: false)
        try writer["KmsKeyId"].write(value.kmsKeyId)
    }

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings()
        value.kmsEncryptionContext = try reader["KmsEncryptionContext"].readMapIfPresent(valueReadingClosure: SmithyReadWrite.ReadingClosures.readString(from:), keyNodeInfo: "key", valueNodeInfo: "value", isFlattened: false)
        value.kmsKeyId = try reader["KmsKeyId"].readIfPresent() ?? ""
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeChannelDefinition {

    static func write(value: TranscribeStreamingClientTypes.MedicalScribeChannelDefinition?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ChannelId"].write(value.channelId)
        try writer["ParticipantRole"].write(value.participantRole)
    }

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribeChannelDefinition {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribeChannelDefinition()
        value.channelId = try reader["ChannelId"].readIfPresent() ?? 0
        value.participantRole = try reader["ParticipantRole"].readIfPresent() ?? .sdkUnknown("")
        return value
    }
}

extension ServiceUnavailableException {

    static func read(from reader: SmithyJSON.Reader) throws -> ServiceUnavailableException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = ServiceUnavailableException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension ConflictException {

    static func read(from reader: SmithyJSON.Reader) throws -> ConflictException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = ConflictException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension InternalFailureException {

    static func read(from reader: SmithyJSON.Reader) throws -> InternalFailureException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = InternalFailureException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension LimitExceededException {

    static func read(from reader: SmithyJSON.Reader) throws -> LimitExceededException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = LimitExceededException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension BadRequestException {

    static func read(from reader: SmithyJSON.Reader) throws -> BadRequestException {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = BadRequestException()
        value.properties.message = try reader["Message"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.CategoryEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CategoryEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CategoryEvent()
        value.matchedCategories = try reader["MatchedCategories"].readListIfPresent(memberReadingClosure: SmithyReadWrite.ReadingClosures.readString(from:), memberNodeInfo: "member", isFlattened: false)
        value.matchedDetails = try reader["MatchedDetails"].readMapIfPresent(valueReadingClosure: TranscribeStreamingClientTypes.PointsOfInterest.read(from:), keyNodeInfo: "key", valueNodeInfo: "value", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.PointsOfInterest {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.PointsOfInterest {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.PointsOfInterest()
        value.timestampRanges = try reader["TimestampRanges"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.TimestampRange.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.TimestampRange {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.TimestampRange {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.TimestampRange()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.UtteranceEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.UtteranceEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.UtteranceEvent()
        value.utteranceId = try reader["UtteranceId"].readIfPresent()
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.participantRole = try reader["ParticipantRole"].readIfPresent()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        value.transcript = try reader["Transcript"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.CallAnalyticsItem.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.entities = try reader["Entities"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.CallAnalyticsEntity.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.sentiment = try reader["Sentiment"].readIfPresent()
        value.issuesDetected = try reader["IssuesDetected"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.IssueDetected.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.IssueDetected {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.IssueDetected {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.IssueDetected()
        value.characterOffsets = try reader["CharacterOffsets"].readIfPresent(with: TranscribeStreamingClientTypes.CharacterOffsets.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.CharacterOffsets {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CharacterOffsets {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CharacterOffsets()
        value.begin = try reader["Begin"].readIfPresent()
        value.end = try reader["End"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsEntity {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CallAnalyticsEntity {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CallAnalyticsEntity()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        value.category = try reader["Category"].readIfPresent()
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsItem {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.CallAnalyticsItem {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.CallAnalyticsItem()
        value.beginOffsetMillis = try reader["BeginOffsetMillis"].readIfPresent()
        value.endOffsetMillis = try reader["EndOffsetMillis"].readIfPresent()
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.vocabularyFilterMatch = try reader["VocabularyFilterMatch"].readIfPresent() ?? false
        value.stable = try reader["Stable"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeTranscriptEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribeTranscriptEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribeTranscriptEvent()
        value.transcriptSegment = try reader["TranscriptSegment"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalScribeTranscriptSegment.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeTranscriptSegment {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribeTranscriptSegment {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribeTranscriptSegment()
        value.segmentId = try reader["SegmentId"].readIfPresent()
        value.beginAudioTime = try reader["BeginAudioTime"].readIfPresent() ?? 0
        value.endAudioTime = try reader["EndAudioTime"].readIfPresent() ?? 0
        value.content = try reader["Content"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalScribeTranscriptItem.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.channelId = try reader["ChannelId"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeTranscriptItem {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalScribeTranscriptItem {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalScribeTranscriptItem()
        value.beginAudioTime = try reader["BeginAudioTime"].readIfPresent() ?? 0
        value.endAudioTime = try reader["EndAudioTime"].readIfPresent() ?? 0
        value.type = try reader["Type"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.vocabularyFilterMatch = try reader["VocabularyFilterMatch"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalTranscriptEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalTranscriptEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalTranscriptEvent()
        value.transcript = try reader["Transcript"].readIfPresent(with: TranscribeStreamingClientTypes.MedicalTranscript.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalTranscript {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalTranscript {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalTranscript()
        value.results = try reader["Results"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalResult.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalResult {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalResult {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalResult()
        value.resultId = try reader["ResultId"].readIfPresent()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.alternatives = try reader["Alternatives"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalAlternative.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.channelId = try reader["ChannelId"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalAlternative {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalAlternative {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalAlternative()
        value.transcript = try reader["Transcript"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalItem.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.entities = try reader["Entities"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.MedicalEntity.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalEntity {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalEntity {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalEntity()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.category = try reader["Category"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.MedicalItem {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.MedicalItem {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.MedicalItem()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.speaker = try reader["Speaker"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.TranscriptEvent {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.TranscriptEvent {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.TranscriptEvent()
        value.transcript = try reader["Transcript"].readIfPresent(with: TranscribeStreamingClientTypes.Transcript.read(from:))
        return value
    }
}

extension TranscribeStreamingClientTypes.Transcript {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Transcript {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Transcript()
        value.results = try reader["Results"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Result.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.Result {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Result {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Result()
        value.resultId = try reader["ResultId"].readIfPresent()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.isPartial = try reader["IsPartial"].readIfPresent() ?? false
        value.alternatives = try reader["Alternatives"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Alternative.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.channelId = try reader["ChannelId"].readIfPresent()
        value.languageCode = try reader["LanguageCode"].readIfPresent()
        value.languageIdentification = try reader["LanguageIdentification"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.LanguageWithScore.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.LanguageWithScore {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.LanguageWithScore {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.LanguageWithScore()
        value.languageCode = try reader["LanguageCode"].readIfPresent()
        value.score = try reader["Score"].readIfPresent() ?? 0
        return value
    }
}

extension TranscribeStreamingClientTypes.Alternative {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Alternative {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Alternative()
        value.transcript = try reader["Transcript"].readIfPresent()
        value.items = try reader["Items"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Item.read(from:), memberNodeInfo: "member", isFlattened: false)
        value.entities = try reader["Entities"].readListIfPresent(memberReadingClosure: TranscribeStreamingClientTypes.Entity.read(from:), memberNodeInfo: "member", isFlattened: false)
        return value
    }
}

extension TranscribeStreamingClientTypes.Entity {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Entity {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Entity()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.category = try reader["Category"].readIfPresent()
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.Item {

    static func read(from reader: SmithyJSON.Reader) throws -> TranscribeStreamingClientTypes.Item {
        guard reader.hasContent else { throw SmithyReadWrite.ReaderError.requiredValueNotPresent }
        var value = TranscribeStreamingClientTypes.Item()
        value.startTime = try reader["StartTime"].readIfPresent() ?? 0
        value.endTime = try reader["EndTime"].readIfPresent() ?? 0
        value.type = try reader["Type"].readIfPresent()
        value.content = try reader["Content"].readIfPresent()
        value.vocabularyFilterMatch = try reader["VocabularyFilterMatch"].readIfPresent() ?? false
        value.speaker = try reader["Speaker"].readIfPresent()
        value.confidence = try reader["Confidence"].readIfPresent()
        value.stable = try reader["Stable"].readIfPresent()
        return value
    }
}

extension TranscribeStreamingClientTypes.ConfigurationEvent {

    static func write(value: TranscribeStreamingClientTypes.ConfigurationEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ChannelDefinitions"].writeList(value.channelDefinitions, memberWritingClosure: TranscribeStreamingClientTypes.ChannelDefinition.write(value:to:), memberNodeInfo: "member", isFlattened: false)
        try writer["PostCallAnalyticsSettings"].write(value.postCallAnalyticsSettings, with: TranscribeStreamingClientTypes.PostCallAnalyticsSettings.write(value:to:))
    }
}

extension TranscribeStreamingClientTypes.PostCallAnalyticsSettings {

    static func write(value: TranscribeStreamingClientTypes.PostCallAnalyticsSettings?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ContentRedactionOutput"].write(value.contentRedactionOutput)
        try writer["DataAccessRoleArn"].write(value.dataAccessRoleArn)
        try writer["OutputEncryptionKMSKeyId"].write(value.outputEncryptionKMSKeyId)
        try writer["OutputLocation"].write(value.outputLocation)
    }
}

extension TranscribeStreamingClientTypes.ChannelDefinition {

    static func write(value: TranscribeStreamingClientTypes.ChannelDefinition?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ChannelId"].write(value.channelId)
        try writer["ParticipantRole"].write(value.participantRole)
    }
}

extension TranscribeStreamingClientTypes.AudioEvent {

    static func write(value: TranscribeStreamingClientTypes.AudioEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["AudioChunk"].write(value.audioChunk)
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeConfigurationEvent {

    static func write(value: TranscribeStreamingClientTypes.MedicalScribeConfigurationEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["ChannelDefinitions"].writeList(value.channelDefinitions, memberWritingClosure: TranscribeStreamingClientTypes.MedicalScribeChannelDefinition.write(value:to:), memberNodeInfo: "member", isFlattened: false)
        try writer["EncryptionSettings"].write(value.encryptionSettings, with: TranscribeStreamingClientTypes.MedicalScribeEncryptionSettings.write(value:to:))
        try writer["PostStreamAnalyticsSettings"].write(value.postStreamAnalyticsSettings, with: TranscribeStreamingClientTypes.MedicalScribePostStreamAnalyticsSettings.write(value:to:))
        try writer["ResourceAccessRoleArn"].write(value.resourceAccessRoleArn)
        try writer["VocabularyFilterMethod"].write(value.vocabularyFilterMethod)
        try writer["VocabularyFilterName"].write(value.vocabularyFilterName)
        try writer["VocabularyName"].write(value.vocabularyName)
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeSessionControlEvent {

    static func write(value: TranscribeStreamingClientTypes.MedicalScribeSessionControlEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["Type"].write(value.type)
    }
}

extension TranscribeStreamingClientTypes.MedicalScribeAudioEvent {

    static func write(value: TranscribeStreamingClientTypes.MedicalScribeAudioEvent?, to writer: SmithyJSON.Writer) throws {
        guard let value else { return }
        try writer["AudioChunk"].write(value.audioChunk)
    }
}

public enum TranscribeStreamingClientTypes {}

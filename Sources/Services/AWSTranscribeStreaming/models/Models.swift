// Code generated by smithy-swift-codegen. DO NOT EDIT!
import AWSClientRuntime
import ClientRuntime

extension TranscribeStreamingClientTypes.Alternative: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case entities = "Entities"
        case items = "Items"
        case transcript = "Transcript"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let entities = entities {
            var entitiesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .entities)
            for entity0 in entities {
                try entitiesContainer.encode(entity0)
            }
        }
        if let items = items {
            var itemsContainer = encodeContainer.nestedUnkeyedContainer(forKey: .items)
            for item0 in items {
                try itemsContainer.encode(item0)
            }
        }
        if let transcript = self.transcript {
            try encodeContainer.encode(transcript, forKey: .transcript)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let transcriptDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .transcript)
        transcript = transcriptDecoded
        let itemsContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.Item?].self, forKey: .items)
        var itemsDecoded0:[TranscribeStreamingClientTypes.Item]? = nil
        if let itemsContainer = itemsContainer {
            itemsDecoded0 = [TranscribeStreamingClientTypes.Item]()
            for structure0 in itemsContainer {
                if let structure0 = structure0 {
                    itemsDecoded0?.append(structure0)
                }
            }
        }
        items = itemsDecoded0
        let entitiesContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.Entity?].self, forKey: .entities)
        var entitiesDecoded0:[TranscribeStreamingClientTypes.Entity]? = nil
        if let entitiesContainer = entitiesContainer {
            entitiesDecoded0 = [TranscribeStreamingClientTypes.Entity]()
            for structure0 in entitiesContainer {
                if let structure0 = structure0 {
                    entitiesDecoded0?.append(structure0)
                }
            }
        }
        entities = entitiesDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
    public struct Alternative: Swift.Equatable {
        /// Contains entities identified as personally identifiable information (PII) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.Entity]?
        /// Contains words, phrases, or punctuation marks in your transcription output.
        public var items: [TranscribeStreamingClientTypes.Item]?
        /// Contains transcribed text.
        public var transcript: Swift.String?

        public init(
            entities: [TranscribeStreamingClientTypes.Entity]? = nil,
            items: [TranscribeStreamingClientTypes.Item]? = nil,
            transcript: Swift.String? = nil
        )
        {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes.AudioEvent: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case audioChunk = "AudioChunk"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let audioChunk = self.audioChunk {
            try encodeContainer.encode(audioChunk.base64EncodedString(), forKey: .audioChunk)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let audioChunkDecoded = try containerValues.decodeIfPresent(ClientRuntime.Data.self, forKey: .audioChunk)
        audioChunk = audioChunkDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// A wrapper for your audio chunks. Your audio stream consists of one or more audio events, which consist of one or more audio chunks. For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
    public struct AudioEvent: Swift.Equatable {
        /// An audio blob that contains the next part of the audio that you want to transcribe. The maximum audio chunk size is 32 KB.
        public var audioChunk: ClientRuntime.Data?

        public init(
            audioChunk: ClientRuntime.Data? = nil
        )
        {
            self.audioChunk = audioChunk
        }
    }

}

extension TranscribeStreamingClientTypes.AudioStream: ClientRuntime.MessageMarshallable {
    public func marshall(encoder: ClientRuntime.RequestEncoder) throws -> ClientRuntime.EventStream.Message {
        var headers: [ClientRuntime.EventStream.Header] = [.init(name: ":message-type", value: .string("event"))]
        var payload: ClientRuntime.Data? = nil
        switch self {
        case .audioevent(let value):
            headers.append(.init(name: ":event-type", value: .string("AudioEvent")))
            headers.append(.init(name: ":content-type", value: .string("application/octet-stream")))
            payload = value.audioChunk
        case .configurationevent(let value):
            headers.append(.init(name: ":event-type", value: .string("ConfigurationEvent")))
            headers.append(.init(name: ":content-type", value: .string("application/json")))
            payload = try encoder.encode(value)
        case .sdkUnknown(_):
            throw ClientRuntime.ClientError.unknownError("cannot serialize the unknown event type!")
        }
        return ClientRuntime.EventStream.Message(headers: headers, payload: payload ?? .init())
    }
}

extension TranscribeStreamingClientTypes {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    public enum AudioStream: Swift.Equatable {
        /// A blob of audio from your application. Your audio stream consists of one or more audio events. For more information, see [Event stream encoding](https://docs.aws.amazon.com/transcribe/latest/dg/event-stream.html).
        case audioevent(TranscribeStreamingClientTypes.AudioEvent)
        /// Contains audio channel definitions and post-call analytics settings.
        case configurationevent(TranscribeStreamingClientTypes.ConfigurationEvent)
        case sdkUnknown(Swift.String)
    }

}

extension BadRequestException: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let message = self.properties.message {
            try encodeContainer.encode(message, forKey: .message)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        properties.message = messageDecoded
    }
}

extension BadRequestException {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil, message: Swift.String? = nil, requestID: Swift.String? = nil) async throws {
        if let data = try await httpResponse.body.readData(),
            let responseDecoder = decoder {
            let output: BadRequestExceptionBody = try responseDecoder.decode(responseBody: data)
            self.properties.message = output.message
        } else {
            self.properties.message = nil
        }
        self.httpResponse = httpResponse
        self.requestID = requestID
        self.message = message
    }
}

/// One or more arguments to the StartStreamTranscription, StartMedicalStreamTranscription, or StartCallAnalyticsStreamTranscription operation was not valid. For example, MediaEncoding or LanguageCode used not valid values. Check the specified parameters and try your request again.
public struct BadRequestException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "BadRequestException" }
    public static var fault: ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = HttpResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

struct BadRequestExceptionBody: Swift.Equatable {
    let message: Swift.String?
}

extension BadRequestExceptionBody: Swift.Decodable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        message = messageDecoded
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsEntity: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case beginOffsetMillis = "BeginOffsetMillis"
        case category = "Category"
        case confidence = "Confidence"
        case content = "Content"
        case endOffsetMillis = "EndOffsetMillis"
        case type = "Type"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let beginOffsetMillis = self.beginOffsetMillis {
            try encodeContainer.encode(beginOffsetMillis, forKey: .beginOffsetMillis)
        }
        if let category = self.category {
            try encodeContainer.encode(category, forKey: .category)
        }
        if let confidence = self.confidence {
            try encodeContainer.encode(confidence, forKey: .confidence)
        }
        if let content = self.content {
            try encodeContainer.encode(content, forKey: .content)
        }
        if let endOffsetMillis = self.endOffsetMillis {
            try encodeContainer.encode(endOffsetMillis, forKey: .endOffsetMillis)
        }
        if let type = self.type {
            try encodeContainer.encode(type, forKey: .type)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let beginOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .beginOffsetMillis)
        beginOffsetMillis = beginOffsetMillisDecoded
        let endOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .endOffsetMillis)
        endOffsetMillis = endOffsetMillisDecoded
        let categoryDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .category)
        category = categoryDecoded
        let typeDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .type)
        type = typeDecoded
        let contentDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .content)
        content = contentDecoded
        let confidenceDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .confidence)
        confidence = confidenceDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains entities identified as personally identifiable information (PII) in your transcription output, along with various associated attributes. Examples include category, confidence score, content, type, and start and end times.
    public struct CallAnalyticsEntity: Swift.Equatable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the identified entity.
        public var beginOffsetMillis: Swift.Int?
        /// The category of information identified. For example, PII.
        public var category: Swift.String?
        /// The confidence score associated with the identification of an entity in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words that represent the identified entity.
        public var content: Swift.String?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the identified entity.
        public var endOffsetMillis: Swift.Int?
        /// The type of PII identified. For example, NAME or CREDIT_DEBIT_NUMBER.
        public var type: Swift.String?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endOffsetMillis: Swift.Int? = nil,
            type: Swift.String? = nil
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endOffsetMillis = endOffsetMillis
            self.type = type
        }
    }

}

extension TranscribeStreamingClientTypes.CallAnalyticsItem: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case beginOffsetMillis = "BeginOffsetMillis"
        case confidence = "Confidence"
        case content = "Content"
        case endOffsetMillis = "EndOffsetMillis"
        case stable = "Stable"
        case type = "Type"
        case vocabularyFilterMatch = "VocabularyFilterMatch"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let beginOffsetMillis = self.beginOffsetMillis {
            try encodeContainer.encode(beginOffsetMillis, forKey: .beginOffsetMillis)
        }
        if let confidence = self.confidence {
            try encodeContainer.encode(confidence, forKey: .confidence)
        }
        if let content = self.content {
            try encodeContainer.encode(content, forKey: .content)
        }
        if let endOffsetMillis = self.endOffsetMillis {
            try encodeContainer.encode(endOffsetMillis, forKey: .endOffsetMillis)
        }
        if let stable = self.stable {
            try encodeContainer.encode(stable, forKey: .stable)
        }
        if let type = self.type {
            try encodeContainer.encode(type.rawValue, forKey: .type)
        }
        if vocabularyFilterMatch != false {
            try encodeContainer.encode(vocabularyFilterMatch, forKey: .vocabularyFilterMatch)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let beginOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .beginOffsetMillis)
        beginOffsetMillis = beginOffsetMillisDecoded
        let endOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .endOffsetMillis)
        endOffsetMillis = endOffsetMillisDecoded
        let typeDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.ItemType.self, forKey: .type)
        type = typeDecoded
        let contentDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .content)
        content = contentDecoded
        let confidenceDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .confidence)
        confidence = confidenceDecoded
        let vocabularyFilterMatchDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .vocabularyFilterMatch) ?? false
        vocabularyFilterMatch = vocabularyFilterMatchDecoded
        let stableDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .stable)
        stable = stableDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// A word, phrase, or punctuation mark in your Call Analytics transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct CallAnalyticsItem: Swift.Equatable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the identified item.
        public var beginOffsetMillis: Swift.Int?
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the identified item.
        public var endOffsetMillis: Swift.Int?
        /// If partial result stabilization is enabled, Stable indicates whether the specified item is stable (true) or if it may change when the segment is complete (false).
        public var stable: Swift.Bool?
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your Call Analytics request. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endOffsetMillis: Swift.Int? = nil,
            stable: Swift.Bool? = nil,
            type: TranscribeStreamingClientTypes.ItemType? = nil,
            vocabularyFilterMatch: Swift.Bool = false
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.confidence = confidence
            self.content = content
            self.endOffsetMillis = endOffsetMillis
            self.stable = stable
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }

}

extension TranscribeStreamingClientTypes {
    public enum CallAnalyticsLanguageCode: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case deDe
        case enAu
        case enGb
        case enUs
        case esUs
        case frCa
        case frFr
        case itIt
        case ptBr
        case sdkUnknown(Swift.String)

        public static var allCases: [CallAnalyticsLanguageCode] {
            return [
                .deDe,
                .enAu,
                .enGb,
                .enUs,
                .esUs,
                .frCa,
                .frFr,
                .itIt,
                .ptBr,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .deDe: return "de-DE"
            case .enAu: return "en-AU"
            case .enGb: return "en-GB"
            case .enUs: return "en-US"
            case .esUs: return "es-US"
            case .frCa: return "fr-CA"
            case .frFr: return "fr-FR"
            case .itIt: return "it-IT"
            case .ptBr: return "pt-BR"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = CallAnalyticsLanguageCode(rawValue: rawValue) ?? CallAnalyticsLanguageCode.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream: ClientRuntime.MessageUnmarshallable {
    public init(message: ClientRuntime.EventStream.Message, decoder: ClientRuntime.ResponseDecoder) throws {
        switch try message.type() {
        case .event(let params):
            switch params.eventType {
            case "UtteranceEvent":
                self = .utteranceevent(try decoder.decode(responseBody: message.payload))
            case "CategoryEvent":
                self = .categoryevent(try decoder.decode(responseBody: message.payload))
            default:
                self = .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
            }
        case .exception(let params):
            let makeError: (ClientRuntime.EventStream.Message, ClientRuntime.EventStream.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                switch params.exceptionType {
                case "BadRequestException":
                    return try decoder.decode(responseBody: message.payload) as BadRequestException
                case "LimitExceededException":
                    return try decoder.decode(responseBody: message.payload) as LimitExceededException
                case "InternalFailureException":
                    return try decoder.decode(responseBody: message.payload) as InternalFailureException
                case "ConflictException":
                    return try decoder.decode(responseBody: message.payload) as ConflictException
                case "ServiceUnavailableException":
                    return try decoder.decode(responseBody: message.payload) as ServiceUnavailableException
                default:
                    let httpResponse = HttpResponse(body: .data(message.payload), statusCode: .ok)
                    return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                }
            }
            let error = try makeError(message, params)
            throw error
        case .error(let params):
            let httpResponse = HttpResponse(body: .data(message.payload), statusCode: .ok)
            throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
        case .unknown(messageType: let messageType):
            throw ClientRuntime.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains detailed information about your Call Analytics streaming session. These details are provided in the UtteranceEvent and CategoryEvent objects.
    public enum CallAnalyticsTranscriptResultStream: Swift.Equatable {
        /// Contains set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to channel definitions, partial result stabilization, sentiment, issue detection, and other transcription-related data.
        case utteranceevent(TranscribeStreamingClientTypes.UtteranceEvent)
        /// Provides information on matched categories that were used to generate real-time supervisor alerts.
        case categoryevent(TranscribeStreamingClientTypes.CategoryEvent)
        case sdkUnknown(Swift.String)
    }

}

extension TranscribeStreamingClientTypes.CategoryEvent: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case matchedCategories = "MatchedCategories"
        case matchedDetails = "MatchedDetails"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let matchedCategories = matchedCategories {
            var matchedCategoriesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .matchedCategories)
            for string0 in matchedCategories {
                try matchedCategoriesContainer.encode(string0)
            }
        }
        if let matchedDetails = matchedDetails {
            var matchedDetailsContainer = encodeContainer.nestedContainer(keyedBy: ClientRuntime.Key.self, forKey: .matchedDetails)
            for (dictKey0, matchedCategoryDetails0) in matchedDetails {
                try matchedDetailsContainer.encode(matchedCategoryDetails0, forKey: ClientRuntime.Key(stringValue: dictKey0))
            }
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let matchedCategoriesContainer = try containerValues.decodeIfPresent([Swift.String?].self, forKey: .matchedCategories)
        var matchedCategoriesDecoded0:[Swift.String]? = nil
        if let matchedCategoriesContainer = matchedCategoriesContainer {
            matchedCategoriesDecoded0 = [Swift.String]()
            for string0 in matchedCategoriesContainer {
                if let string0 = string0 {
                    matchedCategoriesDecoded0?.append(string0)
                }
            }
        }
        matchedCategories = matchedCategoriesDecoded0
        let matchedDetailsContainer = try containerValues.decodeIfPresent([Swift.String: TranscribeStreamingClientTypes.PointsOfInterest?].self, forKey: .matchedDetails)
        var matchedDetailsDecoded0: [Swift.String:TranscribeStreamingClientTypes.PointsOfInterest]? = nil
        if let matchedDetailsContainer = matchedDetailsContainer {
            matchedDetailsDecoded0 = [Swift.String:TranscribeStreamingClientTypes.PointsOfInterest]()
            for (key0, pointsofinterest0) in matchedDetailsContainer {
                if let pointsofinterest0 = pointsofinterest0 {
                    matchedDetailsDecoded0?[key0] = pointsofinterest0
                }
            }
        }
        matchedDetails = matchedDetailsDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// Provides information on any TranscriptFilterType categories that matched your transcription output. Matches are identified for each segment upon completion of that segment.
    public struct CategoryEvent: Swift.Equatable {
        /// Lists the categories that were matched in your audio segment.
        public var matchedCategories: [Swift.String]?
        /// Contains information about the matched categories, including category names and timestamps.
        public var matchedDetails: [Swift.String:TranscribeStreamingClientTypes.PointsOfInterest]?

        public init(
            matchedCategories: [Swift.String]? = nil,
            matchedDetails: [Swift.String:TranscribeStreamingClientTypes.PointsOfInterest]? = nil
        )
        {
            self.matchedCategories = matchedCategories
            self.matchedDetails = matchedDetails
        }
    }

}

extension TranscribeStreamingClientTypes.ChannelDefinition: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case channelId = "ChannelId"
        case participantRole = "ParticipantRole"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if channelId != 0 {
            try encodeContainer.encode(channelId, forKey: .channelId)
        }
        if let participantRole = self.participantRole {
            try encodeContainer.encode(participantRole.rawValue, forKey: .participantRole)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let channelIdDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .channelId) ?? 0
        channelId = channelIdDecoded
        let participantRoleDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.ParticipantRole.self, forKey: .participantRole)
        participantRole = participantRoleDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Makes it possible to specify which speaker is on which audio channel. For example, if your agent is the first participant to speak, you would set ChannelId to 0 (to indicate the first channel) and ParticipantRole to AGENT (to indicate that it's the agent speaking).
    public struct ChannelDefinition: Swift.Equatable {
        /// Specify the audio channel you want to define.
        /// This member is required.
        public var channelId: Swift.Int
        /// Specify the speaker you want to define. Omitting this parameter is equivalent to specifying both participants.
        /// This member is required.
        public var participantRole: TranscribeStreamingClientTypes.ParticipantRole?

        public init(
            channelId: Swift.Int = 0,
            participantRole: TranscribeStreamingClientTypes.ParticipantRole? = nil
        )
        {
            self.channelId = channelId
            self.participantRole = participantRole
        }
    }

}

extension TranscribeStreamingClientTypes.CharacterOffsets: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case begin = "Begin"
        case end = "End"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let begin = self.begin {
            try encodeContainer.encode(begin, forKey: .begin)
        }
        if let end = self.end {
            try encodeContainer.encode(end, forKey: .end)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let beginDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .begin)
        begin = beginDecoded
        let endDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .end)
        end = endDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Provides the location, using character count, in your transcript where a match is identified. For example, the location of an issue or a category match within a segment.
    public struct CharacterOffsets: Swift.Equatable {
        /// Provides the character count of the first character where a match is identified. For example, the first character associated with an issue or a category match in a segment transcript.
        public var begin: Swift.Int?
        /// Provides the character count of the last character where a match is identified. For example, the last character associated with an issue or a category match in a segment transcript.
        public var end: Swift.Int?

        public init(
            begin: Swift.Int? = nil,
            end: Swift.Int? = nil
        )
        {
            self.begin = begin
            self.end = end
        }
    }

}

extension TranscribeStreamingClientTypes.ConfigurationEvent: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case channelDefinitions = "ChannelDefinitions"
        case postCallAnalyticsSettings = "PostCallAnalyticsSettings"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let channelDefinitions = channelDefinitions {
            var channelDefinitionsContainer = encodeContainer.nestedUnkeyedContainer(forKey: .channelDefinitions)
            for channeldefinition0 in channelDefinitions {
                try channelDefinitionsContainer.encode(channeldefinition0)
            }
        }
        if let postCallAnalyticsSettings = self.postCallAnalyticsSettings {
            try encodeContainer.encode(postCallAnalyticsSettings, forKey: .postCallAnalyticsSettings)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let channelDefinitionsContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.ChannelDefinition?].self, forKey: .channelDefinitions)
        var channelDefinitionsDecoded0:[TranscribeStreamingClientTypes.ChannelDefinition]? = nil
        if let channelDefinitionsContainer = channelDefinitionsContainer {
            channelDefinitionsDecoded0 = [TranscribeStreamingClientTypes.ChannelDefinition]()
            for structure0 in channelDefinitionsContainer {
                if let structure0 = structure0 {
                    channelDefinitionsDecoded0?.append(structure0)
                }
            }
        }
        channelDefinitions = channelDefinitionsDecoded0
        let postCallAnalyticsSettingsDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.PostCallAnalyticsSettings.self, forKey: .postCallAnalyticsSettings)
        postCallAnalyticsSettings = postCallAnalyticsSettingsDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Allows you to set audio channel definitions and post-call analytics settings.
    public struct ConfigurationEvent: Swift.Equatable {
        /// Indicates which speaker is on which audio channel.
        public var channelDefinitions: [TranscribeStreamingClientTypes.ChannelDefinition]?
        /// Provides additional optional settings for your Call Analytics post-call request, including encryption and output locations for your redacted and unredacted transcript.
        public var postCallAnalyticsSettings: TranscribeStreamingClientTypes.PostCallAnalyticsSettings?

        public init(
            channelDefinitions: [TranscribeStreamingClientTypes.ChannelDefinition]? = nil,
            postCallAnalyticsSettings: TranscribeStreamingClientTypes.PostCallAnalyticsSettings? = nil
        )
        {
            self.channelDefinitions = channelDefinitions
            self.postCallAnalyticsSettings = postCallAnalyticsSettings
        }
    }

}

extension ConflictException: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let message = self.properties.message {
            try encodeContainer.encode(message, forKey: .message)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        properties.message = messageDecoded
    }
}

extension ConflictException {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil, message: Swift.String? = nil, requestID: Swift.String? = nil) async throws {
        if let data = try await httpResponse.body.readData(),
            let responseDecoder = decoder {
            let output: ConflictExceptionBody = try responseDecoder.decode(responseBody: data)
            self.properties.message = output.message
        } else {
            self.properties.message = nil
        }
        self.httpResponse = httpResponse
        self.requestID = requestID
        self.message = message
    }
}

/// A new stream started with the same session ID. The current stream has been terminated.
public struct ConflictException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ConflictException" }
    public static var fault: ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = HttpResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

struct ConflictExceptionBody: Swift.Equatable {
    let message: Swift.String?
}

extension ConflictExceptionBody: Swift.Decodable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        message = messageDecoded
    }
}

extension TranscribeStreamingClientTypes {
    public enum ContentIdentificationType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case pii
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentIdentificationType] {
            return [
                .pii,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .pii: return "PII"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = ContentIdentificationType(rawValue: rawValue) ?? ContentIdentificationType.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes {
    public enum ContentRedactionOutput: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case redacted
        case redactedAndUnredacted
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentRedactionOutput] {
            return [
                .redacted,
                .redactedAndUnredacted,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .redacted: return "redacted"
            case .redactedAndUnredacted: return "redacted_and_unredacted"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = ContentRedactionOutput(rawValue: rawValue) ?? ContentRedactionOutput.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes {
    public enum ContentRedactionType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case pii
        case sdkUnknown(Swift.String)

        public static var allCases: [ContentRedactionType] {
            return [
                .pii,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .pii: return "PII"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = ContentRedactionType(rawValue: rawValue) ?? ContentRedactionType.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.Entity: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case category = "Category"
        case confidence = "Confidence"
        case content = "Content"
        case endTime = "EndTime"
        case startTime = "StartTime"
        case type = "Type"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let category = self.category {
            try encodeContainer.encode(category, forKey: .category)
        }
        if let confidence = self.confidence {
            try encodeContainer.encode(confidence, forKey: .confidence)
        }
        if let content = self.content {
            try encodeContainer.encode(content, forKey: .content)
        }
        if endTime != 0.0 {
            try encodeContainer.encode(endTime, forKey: .endTime)
        }
        if startTime != 0.0 {
            try encodeContainer.encode(startTime, forKey: .startTime)
        }
        if let type = self.type {
            try encodeContainer.encode(type, forKey: .type)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let startTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .startTime) ?? 0.0
        startTime = startTimeDecoded
        let endTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .endTime) ?? 0.0
        endTime = endTimeDecoded
        let categoryDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .category)
        category = categoryDecoded
        let typeDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .type)
        type = typeDecoded
        let contentDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .content)
        content = contentDecoded
        let confidenceDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .confidence)
        confidence = confidenceDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains entities identified as personally identifiable information (PII) in your transcription output, along with various associated attributes. Examples include category, confidence score, type, stability score, and start and end times.
    public struct Entity: Swift.Equatable {
        /// The category of information identified. The only category is PII.
        public var category: Swift.String?
        /// The confidence score associated with the identified PII entity in your audio. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words identified as PII.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the utterance that was identified as PII.
        public var endTime: Swift.Double
        /// The start time, in milliseconds, of the utterance that was identified as PII.
        public var startTime: Swift.Double
        /// The type of PII identified. For example, NAME or CREDIT_DEBIT_NUMBER.
        public var type: Swift.String?

        public init(
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            startTime: Swift.Double = 0.0,
            type: Swift.String? = nil
        )
        {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
            self.type = type
        }
    }

}

extension InternalFailureException: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let message = self.properties.message {
            try encodeContainer.encode(message, forKey: .message)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        properties.message = messageDecoded
    }
}

extension InternalFailureException {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil, message: Swift.String? = nil, requestID: Swift.String? = nil) async throws {
        if let data = try await httpResponse.body.readData(),
            let responseDecoder = decoder {
            let output: InternalFailureExceptionBody = try responseDecoder.decode(responseBody: data)
            self.properties.message = output.message
        } else {
            self.properties.message = nil
        }
        self.httpResponse = httpResponse
        self.requestID = requestID
        self.message = message
    }
}

/// A problem occurred while processing the audio. Amazon Transcribe terminated processing.
public struct InternalFailureException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "InternalFailureException" }
    public static var fault: ErrorFault { .server }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = HttpResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

struct InternalFailureExceptionBody: Swift.Equatable {
    let message: Swift.String?
}

extension InternalFailureExceptionBody: Swift.Decodable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        message = messageDecoded
    }
}

extension TranscribeStreamingClientTypes.IssueDetected: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case characterOffsets = "CharacterOffsets"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let characterOffsets = self.characterOffsets {
            try encodeContainer.encode(characterOffsets, forKey: .characterOffsets)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let characterOffsetsDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.CharacterOffsets.self, forKey: .characterOffsets)
        characterOffsets = characterOffsetsDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Lists the issues that were identified in your audio segment.
    public struct IssueDetected: Swift.Equatable {
        /// Provides the timestamps that identify when in an audio segment the specified issue occurs.
        public var characterOffsets: TranscribeStreamingClientTypes.CharacterOffsets?

        public init(
            characterOffsets: TranscribeStreamingClientTypes.CharacterOffsets? = nil
        )
        {
            self.characterOffsets = characterOffsets
        }
    }

}

extension TranscribeStreamingClientTypes.Item: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case confidence = "Confidence"
        case content = "Content"
        case endTime = "EndTime"
        case speaker = "Speaker"
        case stable = "Stable"
        case startTime = "StartTime"
        case type = "Type"
        case vocabularyFilterMatch = "VocabularyFilterMatch"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let confidence = self.confidence {
            try encodeContainer.encode(confidence, forKey: .confidence)
        }
        if let content = self.content {
            try encodeContainer.encode(content, forKey: .content)
        }
        if endTime != 0.0 {
            try encodeContainer.encode(endTime, forKey: .endTime)
        }
        if let speaker = self.speaker {
            try encodeContainer.encode(speaker, forKey: .speaker)
        }
        if let stable = self.stable {
            try encodeContainer.encode(stable, forKey: .stable)
        }
        if startTime != 0.0 {
            try encodeContainer.encode(startTime, forKey: .startTime)
        }
        if let type = self.type {
            try encodeContainer.encode(type.rawValue, forKey: .type)
        }
        if vocabularyFilterMatch != false {
            try encodeContainer.encode(vocabularyFilterMatch, forKey: .vocabularyFilterMatch)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let startTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .startTime) ?? 0.0
        startTime = startTimeDecoded
        let endTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .endTime) ?? 0.0
        endTime = endTimeDecoded
        let typeDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.ItemType.self, forKey: .type)
        type = typeDecoded
        let contentDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .content)
        content = contentDecoded
        let vocabularyFilterMatchDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .vocabularyFilterMatch) ?? false
        vocabularyFilterMatch = vocabularyFilterMatchDecoded
        let speakerDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .speaker)
        speaker = speakerDecoded
        let confidenceDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .confidence)
        confidence = confidenceDecoded
        let stableDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .stable)
        stable = stableDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct Item: Swift.Equatable {
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endTime: Swift.Double
        /// If speaker partitioning is enabled, Speaker labels the speaker of the specified item.
        public var speaker: Swift.String?
        /// If partial result stabilization is enabled, Stable indicates whether the specified item is stable (true) or if it may change when the segment is complete (false).
        public var stable: Swift.Bool?
        /// The start time, in milliseconds, of the transcribed item.
        public var startTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?
        /// Indicates whether the specified item matches a word in the vocabulary filter included in your request. If true, there is a vocabulary filter match.
        public var vocabularyFilterMatch: Swift.Bool

        public init(
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            speaker: Swift.String? = nil,
            stable: Swift.Bool? = nil,
            startTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.ItemType? = nil,
            vocabularyFilterMatch: Swift.Bool = false
        )
        {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.stable = stable
            self.startTime = startTime
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }
    }

}

extension TranscribeStreamingClientTypes {
    public enum ItemType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case pronunciation
        case punctuation
        case sdkUnknown(Swift.String)

        public static var allCases: [ItemType] {
            return [
                .pronunciation,
                .punctuation,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .pronunciation: return "pronunciation"
            case .punctuation: return "punctuation"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = ItemType(rawValue: rawValue) ?? ItemType.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes {
    public enum LanguageCode: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case deDe
        case enAu
        case enGb
        case enUs
        case esUs
        case frCa
        case frFr
        case hiIn
        case itIt
        case jaJp
        case koKr
        case ptBr
        case thTh
        case zhCn
        case sdkUnknown(Swift.String)

        public static var allCases: [LanguageCode] {
            return [
                .deDe,
                .enAu,
                .enGb,
                .enUs,
                .esUs,
                .frCa,
                .frFr,
                .hiIn,
                .itIt,
                .jaJp,
                .koKr,
                .ptBr,
                .thTh,
                .zhCn,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .deDe: return "de-DE"
            case .enAu: return "en-AU"
            case .enGb: return "en-GB"
            case .enUs: return "en-US"
            case .esUs: return "es-US"
            case .frCa: return "fr-CA"
            case .frFr: return "fr-FR"
            case .hiIn: return "hi-IN"
            case .itIt: return "it-IT"
            case .jaJp: return "ja-JP"
            case .koKr: return "ko-KR"
            case .ptBr: return "pt-BR"
            case .thTh: return "th-TH"
            case .zhCn: return "zh-CN"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = LanguageCode(rawValue: rawValue) ?? LanguageCode.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.LanguageWithScore: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case languageCode = "LanguageCode"
        case score = "Score"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let languageCode = self.languageCode {
            try encodeContainer.encode(languageCode.rawValue, forKey: .languageCode)
        }
        if score != 0.0 {
            try encodeContainer.encode(score, forKey: .score)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let languageCodeDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.LanguageCode.self, forKey: .languageCode)
        languageCode = languageCodeDecoded
        let scoreDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .score) ?? 0.0
        score = scoreDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// The language code that represents the language identified in your audio, including the associated confidence score. If you enabled channel identification in your request and each channel contained a different language, you will have more than one LanguageWithScore result.
    public struct LanguageWithScore: Swift.Equatable {
        /// The language code of the identified language.
        public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
        /// The confidence score associated with the identified language code. Confidence scores are values between zero and one; larger values indicate a higher confidence in the identified language.
        public var score: Swift.Double

        public init(
            languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
            score: Swift.Double = 0.0
        )
        {
            self.languageCode = languageCode
            self.score = score
        }
    }

}

extension LimitExceededException: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let message = self.properties.message {
            try encodeContainer.encode(message, forKey: .message)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        properties.message = messageDecoded
    }
}

extension LimitExceededException {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil, message: Swift.String? = nil, requestID: Swift.String? = nil) async throws {
        if let data = try await httpResponse.body.readData(),
            let responseDecoder = decoder {
            let output: LimitExceededExceptionBody = try responseDecoder.decode(responseBody: data)
            self.properties.message = output.message
        } else {
            self.properties.message = nil
        }
        self.httpResponse = httpResponse
        self.requestID = requestID
        self.message = message
    }
}

/// Your client has exceeded one of the Amazon Transcribe limits. This is typically the audio length limit. Break your audio stream into smaller chunks and try your request again.
public struct LimitExceededException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "LimitExceededException" }
    public static var fault: ErrorFault { .client }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = HttpResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

struct LimitExceededExceptionBody: Swift.Equatable {
    let message: Swift.String?
}

extension LimitExceededExceptionBody: Swift.Decodable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        message = messageDecoded
    }
}

extension TranscribeStreamingClientTypes {
    public enum MediaEncoding: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case flac
        case oggOpus
        case pcm
        case sdkUnknown(Swift.String)

        public static var allCases: [MediaEncoding] {
            return [
                .flac,
                .oggOpus,
                .pcm,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .flac: return "flac"
            case .oggOpus: return "ogg-opus"
            case .pcm: return "pcm"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = MediaEncoding(rawValue: rawValue) ?? MediaEncoding.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalAlternative: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case entities = "Entities"
        case items = "Items"
        case transcript = "Transcript"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let entities = entities {
            var entitiesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .entities)
            for medicalentity0 in entities {
                try entitiesContainer.encode(medicalentity0)
            }
        }
        if let items = items {
            var itemsContainer = encodeContainer.nestedUnkeyedContainer(forKey: .items)
            for medicalitem0 in items {
                try itemsContainer.encode(medicalitem0)
            }
        }
        if let transcript = self.transcript {
            try encodeContainer.encode(transcript, forKey: .transcript)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let transcriptDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .transcript)
        transcript = transcriptDecoded
        let itemsContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.MedicalItem?].self, forKey: .items)
        var itemsDecoded0:[TranscribeStreamingClientTypes.MedicalItem]? = nil
        if let itemsContainer = itemsContainer {
            itemsDecoded0 = [TranscribeStreamingClientTypes.MedicalItem]()
            for structure0 in itemsContainer {
                if let structure0 = structure0 {
                    itemsDecoded0?.append(structure0)
                }
            }
        }
        items = itemsDecoded0
        let entitiesContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.MedicalEntity?].self, forKey: .entities)
        var entitiesDecoded0:[TranscribeStreamingClientTypes.MedicalEntity]? = nil
        if let entitiesContainer = entitiesContainer {
            entitiesDecoded0 = [TranscribeStreamingClientTypes.MedicalEntity]()
            for structure0 in entitiesContainer {
                if let structure0 = structure0 {
                    entitiesDecoded0?.append(structure0)
                }
            }
        }
        entities = entitiesDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
    public struct MedicalAlternative: Swift.Equatable {
        /// Contains entities identified as personal health information (PHI) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.MedicalEntity]?
        /// Contains words, phrases, or punctuation marks in your transcription output.
        public var items: [TranscribeStreamingClientTypes.MedicalItem]?
        /// Contains transcribed text.
        public var transcript: Swift.String?

        public init(
            entities: [TranscribeStreamingClientTypes.MedicalEntity]? = nil,
            items: [TranscribeStreamingClientTypes.MedicalItem]? = nil,
            transcript: Swift.String? = nil
        )
        {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes {
    public enum MedicalContentIdentificationType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case phi
        case sdkUnknown(Swift.String)

        public static var allCases: [MedicalContentIdentificationType] {
            return [
                .phi,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .phi: return "PHI"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = MedicalContentIdentificationType(rawValue: rawValue) ?? MedicalContentIdentificationType.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.MedicalEntity: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case category = "Category"
        case confidence = "Confidence"
        case content = "Content"
        case endTime = "EndTime"
        case startTime = "StartTime"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let category = self.category {
            try encodeContainer.encode(category, forKey: .category)
        }
        if let confidence = self.confidence {
            try encodeContainer.encode(confidence, forKey: .confidence)
        }
        if let content = self.content {
            try encodeContainer.encode(content, forKey: .content)
        }
        if endTime != 0.0 {
            try encodeContainer.encode(endTime, forKey: .endTime)
        }
        if startTime != 0.0 {
            try encodeContainer.encode(startTime, forKey: .startTime)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let startTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .startTime) ?? 0.0
        startTime = startTimeDecoded
        let endTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .endTime) ?? 0.0
        endTime = endTimeDecoded
        let categoryDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .category)
        category = categoryDecoded
        let contentDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .content)
        content = contentDecoded
        let confidenceDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .confidence)
        confidence = confidenceDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains entities identified as personal health information (PHI) in your transcription output, along with various associated attributes. Examples include category, confidence score, type, stability score, and start and end times.
    public struct MedicalEntity: Swift.Equatable {
        /// The category of information identified. The only category is PHI.
        public var category: Swift.String?
        /// The confidence score associated with the identified PHI entity in your audio. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified entity correctly matches the entity spoken in your media.
        public var confidence: Swift.Double?
        /// The word or words identified as PHI.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the utterance that was identified as PHI.
        public var endTime: Swift.Double
        /// The start time, in milliseconds, of the utterance that was identified as PHI.
        public var startTime: Swift.Double

        public init(
            category: Swift.String? = nil,
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            startTime: Swift.Double = 0.0
        )
        {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
        }
    }

}

extension TranscribeStreamingClientTypes.MedicalItem: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case confidence = "Confidence"
        case content = "Content"
        case endTime = "EndTime"
        case speaker = "Speaker"
        case startTime = "StartTime"
        case type = "Type"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let confidence = self.confidence {
            try encodeContainer.encode(confidence, forKey: .confidence)
        }
        if let content = self.content {
            try encodeContainer.encode(content, forKey: .content)
        }
        if endTime != 0.0 {
            try encodeContainer.encode(endTime, forKey: .endTime)
        }
        if let speaker = self.speaker {
            try encodeContainer.encode(speaker, forKey: .speaker)
        }
        if startTime != 0.0 {
            try encodeContainer.encode(startTime, forKey: .startTime)
        }
        if let type = self.type {
            try encodeContainer.encode(type.rawValue, forKey: .type)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let startTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .startTime) ?? 0.0
        startTime = startTimeDecoded
        let endTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .endTime) ?? 0.0
        endTime = endTimeDecoded
        let typeDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.ItemType.self, forKey: .type)
        type = typeDecoded
        let contentDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .content)
        content = contentDecoded
        let confidenceDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .confidence)
        confidence = confidenceDecoded
        let speakerDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .speaker)
        speaker = speakerDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// A word, phrase, or punctuation mark in your transcription output, along with various associated attributes, such as confidence score, type, and start and end times.
    public struct MedicalItem: Swift.Equatable {
        /// The confidence score associated with a word or phrase in your transcript. Confidence scores are values between 0 and 1. A larger value indicates a higher probability that the identified item correctly matches the item spoken in your media.
        public var confidence: Swift.Double?
        /// The word or punctuation that was transcribed.
        public var content: Swift.String?
        /// The end time, in milliseconds, of the transcribed item.
        public var endTime: Swift.Double
        /// If speaker partitioning is enabled, Speaker labels the speaker of the specified item.
        public var speaker: Swift.String?
        /// The start time, in milliseconds, of the transcribed item.
        public var startTime: Swift.Double
        /// The type of item identified. Options are: PRONUNCIATION (spoken words) and PUNCTUATION.
        public var type: TranscribeStreamingClientTypes.ItemType?

        public init(
            confidence: Swift.Double? = nil,
            content: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            speaker: Swift.String? = nil,
            startTime: Swift.Double = 0.0,
            type: TranscribeStreamingClientTypes.ItemType? = nil
        )
        {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.startTime = startTime
            self.type = type
        }
    }

}

extension TranscribeStreamingClientTypes.MedicalResult: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case alternatives = "Alternatives"
        case channelId = "ChannelId"
        case endTime = "EndTime"
        case isPartial = "IsPartial"
        case resultId = "ResultId"
        case startTime = "StartTime"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let alternatives = alternatives {
            var alternativesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .alternatives)
            for medicalalternative0 in alternatives {
                try alternativesContainer.encode(medicalalternative0)
            }
        }
        if let channelId = self.channelId {
            try encodeContainer.encode(channelId, forKey: .channelId)
        }
        if endTime != 0.0 {
            try encodeContainer.encode(endTime, forKey: .endTime)
        }
        if isPartial != false {
            try encodeContainer.encode(isPartial, forKey: .isPartial)
        }
        if let resultId = self.resultId {
            try encodeContainer.encode(resultId, forKey: .resultId)
        }
        if startTime != 0.0 {
            try encodeContainer.encode(startTime, forKey: .startTime)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let resultIdDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .resultId)
        resultId = resultIdDecoded
        let startTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .startTime) ?? 0.0
        startTime = startTimeDecoded
        let endTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .endTime) ?? 0.0
        endTime = endTimeDecoded
        let isPartialDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .isPartial) ?? false
        isPartial = isPartialDecoded
        let alternativesContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.MedicalAlternative?].self, forKey: .alternatives)
        var alternativesDecoded0:[TranscribeStreamingClientTypes.MedicalAlternative]? = nil
        if let alternativesContainer = alternativesContainer {
            alternativesDecoded0 = [TranscribeStreamingClientTypes.MedicalAlternative]()
            for structure0 in alternativesContainer {
                if let structure0 = structure0 {
                    alternativesDecoded0?.append(structure0)
                }
            }
        }
        alternatives = alternativesDecoded0
        let channelIdDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .channelId)
        channelId = channelIdDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// The Result associated with a . Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
    public struct MedicalResult: Swift.Equatable {
        /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
        public var alternatives: [TranscribeStreamingClientTypes.MedicalAlternative]?
        /// Indicates the channel identified for the Result.
        public var channelId: Swift.String?
        /// The end time, in milliseconds, of the Result.
        public var endTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// Provides a unique identifier for the Result.
        public var resultId: Swift.String?
        /// The start time, in milliseconds, of the Result.
        public var startTime: Swift.Double

        public init(
            alternatives: [TranscribeStreamingClientTypes.MedicalAlternative]? = nil,
            channelId: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            resultId: Swift.String? = nil,
            startTime: Swift.Double = 0.0
        )
        {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.resultId = resultId
            self.startTime = startTime
        }
    }

}

extension TranscribeStreamingClientTypes.MedicalTranscript: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case results = "Results"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let results = results {
            var resultsContainer = encodeContainer.nestedUnkeyedContainer(forKey: .results)
            for medicalresult0 in results {
                try resultsContainer.encode(medicalresult0)
            }
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let resultsContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.MedicalResult?].self, forKey: .results)
        var resultsDecoded0:[TranscribeStreamingClientTypes.MedicalResult]? = nil
        if let resultsContainer = resultsContainer {
            resultsDecoded0 = [TranscribeStreamingClientTypes.MedicalResult]()
            for structure0 in resultsContainer {
                if let structure0 = structure0 {
                    resultsDecoded0?.append(structure0)
                }
            }
        }
        results = resultsDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// The MedicalTranscript associated with a . MedicalTranscript contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct MedicalTranscript: Swift.Equatable {
        /// Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var results: [TranscribeStreamingClientTypes.MedicalResult]?

        public init(
            results: [TranscribeStreamingClientTypes.MedicalResult]? = nil
        )
        {
            self.results = results
        }
    }

}

extension TranscribeStreamingClientTypes.MedicalTranscriptEvent: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case transcript = "Transcript"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let transcript = self.transcript {
            try encodeContainer.encode(transcript, forKey: .transcript)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let transcriptDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.MedicalTranscript.self, forKey: .transcript)
        transcript = transcriptDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// The MedicalTranscriptEvent associated with a MedicalTranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct MedicalTranscriptEvent: Swift.Equatable {
        /// Contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var transcript: TranscribeStreamingClientTypes.MedicalTranscript?

        public init(
            transcript: TranscribeStreamingClientTypes.MedicalTranscript? = nil
        )
        {
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes.MedicalTranscriptResultStream: ClientRuntime.MessageUnmarshallable {
    public init(message: ClientRuntime.EventStream.Message, decoder: ClientRuntime.ResponseDecoder) throws {
        switch try message.type() {
        case .event(let params):
            switch params.eventType {
            case "TranscriptEvent":
                self = .transcriptevent(try decoder.decode(responseBody: message.payload))
            default:
                self = .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
            }
        case .exception(let params):
            let makeError: (ClientRuntime.EventStream.Message, ClientRuntime.EventStream.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                switch params.exceptionType {
                case "BadRequestException":
                    return try decoder.decode(responseBody: message.payload) as BadRequestException
                case "LimitExceededException":
                    return try decoder.decode(responseBody: message.payload) as LimitExceededException
                case "InternalFailureException":
                    return try decoder.decode(responseBody: message.payload) as InternalFailureException
                case "ConflictException":
                    return try decoder.decode(responseBody: message.payload) as ConflictException
                case "ServiceUnavailableException":
                    return try decoder.decode(responseBody: message.payload) as ServiceUnavailableException
                default:
                    let httpResponse = HttpResponse(body: .data(message.payload), statusCode: .ok)
                    return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                }
            }
            let error = try makeError(message, params)
            throw error
        case .error(let params):
            let httpResponse = HttpResponse(body: .data(message.payload), statusCode: .ok)
            throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
        case .unknown(messageType: let messageType):
            throw ClientRuntime.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains detailed information about your streaming session.
    public enum MedicalTranscriptResultStream: Swift.Equatable {
        /// The MedicalTranscriptEvent associated with a MedicalTranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        case transcriptevent(TranscribeStreamingClientTypes.MedicalTranscriptEvent)
        case sdkUnknown(Swift.String)
    }

}

extension TranscribeStreamingClientTypes {
    public enum PartialResultsStability: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case high
        case low
        case medium
        case sdkUnknown(Swift.String)

        public static var allCases: [PartialResultsStability] {
            return [
                .high,
                .low,
                .medium,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .high: return "high"
            case .low: return "low"
            case .medium: return "medium"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = PartialResultsStability(rawValue: rawValue) ?? PartialResultsStability.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes {
    public enum ParticipantRole: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case agent
        case customer
        case sdkUnknown(Swift.String)

        public static var allCases: [ParticipantRole] {
            return [
                .agent,
                .customer,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .agent: return "AGENT"
            case .customer: return "CUSTOMER"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = ParticipantRole(rawValue: rawValue) ?? ParticipantRole.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.PointsOfInterest: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case timestampRanges = "TimestampRanges"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let timestampRanges = timestampRanges {
            var timestampRangesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .timestampRanges)
            for timestamprange0 in timestampRanges {
                try timestampRangesContainer.encode(timestamprange0)
            }
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let timestampRangesContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.TimestampRange?].self, forKey: .timestampRanges)
        var timestampRangesDecoded0:[TranscribeStreamingClientTypes.TimestampRange]? = nil
        if let timestampRangesContainer = timestampRangesContainer {
            timestampRangesDecoded0 = [TranscribeStreamingClientTypes.TimestampRange]()
            for structure0 in timestampRangesContainer {
                if let structure0 = structure0 {
                    timestampRangesDecoded0?.append(structure0)
                }
            }
        }
        timestampRanges = timestampRangesDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains the timestamps of matched categories.
    public struct PointsOfInterest: Swift.Equatable {
        /// Contains the timestamp ranges (start time through end time) of matched categories and rules.
        public var timestampRanges: [TranscribeStreamingClientTypes.TimestampRange]?

        public init(
            timestampRanges: [TranscribeStreamingClientTypes.TimestampRange]? = nil
        )
        {
            self.timestampRanges = timestampRanges
        }
    }

}

extension TranscribeStreamingClientTypes.PostCallAnalyticsSettings: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case contentRedactionOutput = "ContentRedactionOutput"
        case dataAccessRoleArn = "DataAccessRoleArn"
        case outputEncryptionKMSKeyId = "OutputEncryptionKMSKeyId"
        case outputLocation = "OutputLocation"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let contentRedactionOutput = self.contentRedactionOutput {
            try encodeContainer.encode(contentRedactionOutput.rawValue, forKey: .contentRedactionOutput)
        }
        if let dataAccessRoleArn = self.dataAccessRoleArn {
            try encodeContainer.encode(dataAccessRoleArn, forKey: .dataAccessRoleArn)
        }
        if let outputEncryptionKMSKeyId = self.outputEncryptionKMSKeyId {
            try encodeContainer.encode(outputEncryptionKMSKeyId, forKey: .outputEncryptionKMSKeyId)
        }
        if let outputLocation = self.outputLocation {
            try encodeContainer.encode(outputLocation, forKey: .outputLocation)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let outputLocationDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .outputLocation)
        outputLocation = outputLocationDecoded
        let dataAccessRoleArnDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .dataAccessRoleArn)
        dataAccessRoleArn = dataAccessRoleArnDecoded
        let contentRedactionOutputDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.ContentRedactionOutput.self, forKey: .contentRedactionOutput)
        contentRedactionOutput = contentRedactionOutputDecoded
        let outputEncryptionKMSKeyIdDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .outputEncryptionKMSKeyId)
        outputEncryptionKMSKeyId = outputEncryptionKMSKeyIdDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Allows you to specify additional settings for your streaming Call Analytics post-call request, including output locations for your redacted and unredacted transcript, which IAM role to use, and, optionally, which encryption key to use. ContentRedactionOutput, DataAccessRoleArn, and OutputLocation are required fields.
    public struct PostCallAnalyticsSettings: Swift.Equatable {
        /// Specify whether you want only a redacted transcript or both a redacted and an unredacted transcript. If you choose redacted and unredacted, two JSON files are generated and stored in the Amazon S3 output location you specify. Note that to include ContentRedactionOutput in your request, you must enable content redaction (ContentRedactionType).
        public var contentRedactionOutput: TranscribeStreamingClientTypes.ContentRedactionOutput?
        /// The Amazon Resource Name (ARN) of an IAM role that has permissions to access the Amazon S3 bucket that contains your input files. If the role that you specify doesnt have the appropriate permissions to access the specified Amazon S3 location, your request fails. IAM role ARNs have the format arn:partition:iam::account:role/role-name-with-path. For example: arn:aws:iam::111122223333:role/Admin. For more information, see [IAM ARNs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns).
        /// This member is required.
        public var dataAccessRoleArn: Swift.String?
        /// The KMS key you want to use to encrypt your Call Analytics post-call output. If using a key located in the current Amazon Web Services account, you can specify your KMS key in one of four ways:
        ///
        /// * Use the KMS key ID itself. For example, 1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use an alias for the KMS key ID. For example, alias/ExampleAlias.
        ///
        /// * Use the Amazon Resource Name (ARN) for the KMS key ID. For example, arn:aws:kms:region:account-ID:key/1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use the ARN for the KMS key alias. For example, arn:aws:kms:region:account-ID:alias/ExampleAlias.
        ///
        ///
        /// If using a key located in a different Amazon Web Services account than the current Amazon Web Services account, you can specify your KMS key in one of two ways:
        ///
        /// * Use the ARN for the KMS key ID. For example, arn:aws:kms:region:account-ID:key/1234abcd-12ab-34cd-56ef-1234567890ab.
        ///
        /// * Use the ARN for the KMS key alias. For example, arn:aws:kms:region:account-ID:alias/ExampleAlias.
        ///
        ///
        /// Note that the user making the request must have permission to use the specified KMS key.
        public var outputEncryptionKMSKeyId: Swift.String?
        /// The Amazon S3 location where you want your Call Analytics post-call transcription output stored. You can use any of the following formats to specify the output location:
        ///
        /// * s3://DOC-EXAMPLE-BUCKET
        ///
        /// * s3://DOC-EXAMPLE-BUCKET/my-output-folder/
        ///
        /// * s3://DOC-EXAMPLE-BUCKET/my-output-folder/my-call-analytics-job.json
        /// This member is required.
        public var outputLocation: Swift.String?

        public init(
            contentRedactionOutput: TranscribeStreamingClientTypes.ContentRedactionOutput? = nil,
            dataAccessRoleArn: Swift.String? = nil,
            outputEncryptionKMSKeyId: Swift.String? = nil,
            outputLocation: Swift.String? = nil
        )
        {
            self.contentRedactionOutput = contentRedactionOutput
            self.dataAccessRoleArn = dataAccessRoleArn
            self.outputEncryptionKMSKeyId = outputEncryptionKMSKeyId
            self.outputLocation = outputLocation
        }
    }

}

extension TranscribeStreamingClientTypes.Result: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case alternatives = "Alternatives"
        case channelId = "ChannelId"
        case endTime = "EndTime"
        case isPartial = "IsPartial"
        case languageCode = "LanguageCode"
        case languageIdentification = "LanguageIdentification"
        case resultId = "ResultId"
        case startTime = "StartTime"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let alternatives = alternatives {
            var alternativesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .alternatives)
            for alternative0 in alternatives {
                try alternativesContainer.encode(alternative0)
            }
        }
        if let channelId = self.channelId {
            try encodeContainer.encode(channelId, forKey: .channelId)
        }
        if endTime != 0.0 {
            try encodeContainer.encode(endTime, forKey: .endTime)
        }
        if isPartial != false {
            try encodeContainer.encode(isPartial, forKey: .isPartial)
        }
        if let languageCode = self.languageCode {
            try encodeContainer.encode(languageCode.rawValue, forKey: .languageCode)
        }
        if let languageIdentification = languageIdentification {
            var languageIdentificationContainer = encodeContainer.nestedUnkeyedContainer(forKey: .languageIdentification)
            for languagewithscore0 in languageIdentification {
                try languageIdentificationContainer.encode(languagewithscore0)
            }
        }
        if let resultId = self.resultId {
            try encodeContainer.encode(resultId, forKey: .resultId)
        }
        if startTime != 0.0 {
            try encodeContainer.encode(startTime, forKey: .startTime)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let resultIdDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .resultId)
        resultId = resultIdDecoded
        let startTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .startTime) ?? 0.0
        startTime = startTimeDecoded
        let endTimeDecoded = try containerValues.decodeIfPresent(Swift.Double.self, forKey: .endTime) ?? 0.0
        endTime = endTimeDecoded
        let isPartialDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .isPartial) ?? false
        isPartial = isPartialDecoded
        let alternativesContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.Alternative?].self, forKey: .alternatives)
        var alternativesDecoded0:[TranscribeStreamingClientTypes.Alternative]? = nil
        if let alternativesContainer = alternativesContainer {
            alternativesDecoded0 = [TranscribeStreamingClientTypes.Alternative]()
            for structure0 in alternativesContainer {
                if let structure0 = structure0 {
                    alternativesDecoded0?.append(structure0)
                }
            }
        }
        alternatives = alternativesDecoded0
        let channelIdDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .channelId)
        channelId = channelIdDecoded
        let languageCodeDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.LanguageCode.self, forKey: .languageCode)
        languageCode = languageCodeDecoded
        let languageIdentificationContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.LanguageWithScore?].self, forKey: .languageIdentification)
        var languageIdentificationDecoded0:[TranscribeStreamingClientTypes.LanguageWithScore]? = nil
        if let languageIdentificationContainer = languageIdentificationContainer {
            languageIdentificationDecoded0 = [TranscribeStreamingClientTypes.LanguageWithScore]()
            for structure0 in languageIdentificationContainer {
                if let structure0 = structure0 {
                    languageIdentificationDecoded0?.append(structure0)
                }
            }
        }
        languageIdentification = languageIdentificationDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// The Result associated with a . Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
    public struct Result: Swift.Equatable {
        /// A list of possible alternative transcriptions for the input audio. Each alternative may contain one or more of Items, Entities, or Transcript.
        public var alternatives: [TranscribeStreamingClientTypes.Alternative]?
        /// Indicates which audio channel is associated with the Result.
        public var channelId: Swift.String?
        /// The end time, in milliseconds, of the Result.
        public var endTime: Swift.Double
        /// Indicates if the segment is complete. If IsPartial is true, the segment is not complete. If IsPartial is false, the segment is complete.
        public var isPartial: Swift.Bool
        /// The language code that represents the language spoken in your audio stream.
        public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
        /// The language code of the dominant language identified in your stream. If you enabled channel identification and each channel of your audio contains a different language, you may have more than one result.
        public var languageIdentification: [TranscribeStreamingClientTypes.LanguageWithScore]?
        /// Provides a unique identifier for the Result.
        public var resultId: Swift.String?
        /// The start time, in milliseconds, of the Result.
        public var startTime: Swift.Double

        public init(
            alternatives: [TranscribeStreamingClientTypes.Alternative]? = nil,
            channelId: Swift.String? = nil,
            endTime: Swift.Double = 0.0,
            isPartial: Swift.Bool = false,
            languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
            languageIdentification: [TranscribeStreamingClientTypes.LanguageWithScore]? = nil,
            resultId: Swift.String? = nil,
            startTime: Swift.Double = 0.0
        )
        {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.languageCode = languageCode
            self.languageIdentification = languageIdentification
            self.resultId = resultId
            self.startTime = startTime
        }
    }

}

extension TranscribeStreamingClientTypes {
    public enum Sentiment: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case mixed
        case negative
        case neutral
        case positive
        case sdkUnknown(Swift.String)

        public static var allCases: [Sentiment] {
            return [
                .mixed,
                .negative,
                .neutral,
                .positive,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .mixed: return "MIXED"
            case .negative: return "NEGATIVE"
            case .neutral: return "NEUTRAL"
            case .positive: return "POSITIVE"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = Sentiment(rawValue: rawValue) ?? Sentiment.sdkUnknown(rawValue)
        }
    }
}

extension ServiceUnavailableException: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let message = self.properties.message {
            try encodeContainer.encode(message, forKey: .message)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        properties.message = messageDecoded
    }
}

extension ServiceUnavailableException {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil, message: Swift.String? = nil, requestID: Swift.String? = nil) async throws {
        if let data = try await httpResponse.body.readData(),
            let responseDecoder = decoder {
            let output: ServiceUnavailableExceptionBody = try responseDecoder.decode(responseBody: data)
            self.properties.message = output.message
        } else {
            self.properties.message = nil
        }
        self.httpResponse = httpResponse
        self.requestID = requestID
        self.message = message
    }
}

/// The service is currently unavailable. Try your request later.
public struct ServiceUnavailableException: ClientRuntime.ModeledError, AWSClientRuntime.AWSServiceError, ClientRuntime.HTTPError, Swift.Error {

    public struct Properties {
        public internal(set) var message: Swift.String? = nil
    }

    public internal(set) var properties = Properties()
    public static var typeName: Swift.String { "ServiceUnavailableException" }
    public static var fault: ErrorFault { .server }
    public static var isRetryable: Swift.Bool { false }
    public static var isThrottling: Swift.Bool { false }
    public internal(set) var httpResponse = HttpResponse()
    public internal(set) var message: Swift.String?
    public internal(set) var requestID: Swift.String?

    public init(
        message: Swift.String? = nil
    )
    {
        self.properties.message = message
    }
}

struct ServiceUnavailableExceptionBody: Swift.Equatable {
    let message: Swift.String?
}

extension ServiceUnavailableExceptionBody: Swift.Decodable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case message = "Message"
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let messageDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .message)
        message = messageDecoded
    }
}

extension TranscribeStreamingClientTypes {
    public enum Specialty: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case cardiology
        case neurology
        case oncology
        case primarycare
        case radiology
        case urology
        case sdkUnknown(Swift.String)

        public static var allCases: [Specialty] {
            return [
                .cardiology,
                .neurology,
                .oncology,
                .primarycare,
                .radiology,
                .urology,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .cardiology: return "CARDIOLOGY"
            case .neurology: return "NEUROLOGY"
            case .oncology: return "ONCOLOGY"
            case .primarycare: return "PRIMARYCARE"
            case .radiology: return "RADIOLOGY"
            case .urology: return "UROLOGY"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = Specialty(rawValue: rawValue) ?? Specialty.sdkUnknown(rawValue)
        }
    }
}

public struct StartCallAnalyticsStreamTranscriptionInputBodyMiddleware: ClientRuntime.Middleware {
    public let id: Swift.String = "StartCallAnalyticsStreamTranscriptionInputBodyMiddleware"

    public init() {}

    public func handle<H>(context: Context,
                  input: ClientRuntime.SerializeStepInput<StartCallAnalyticsStreamTranscriptionInput>,
                  next: H) async throws -> ClientRuntime.OperationOutput<StartCallAnalyticsStreamTranscriptionOutput>
    where H: Handler,
    Self.MInput == H.Input,
    Self.MOutput == H.Output,
    Self.Context == H.Context
    {
        do {
            let encoder = context.getEncoder()
            if let audioStream = input.operationInput.audioStream {
                guard let messageEncoder = context.getMessageEncoder() else {
                    fatalError("Message encoder is required for streaming payload")
                }
                guard let messageSigner = context.getMessageSigner() else {
                    fatalError("Message signer is required for streaming payload")
                }
                let encoderStream = ClientRuntime.EventStream.DefaultMessageEncoderStream(stream: audioStream, messageEncoder: messageEncoder, requestEncoder: encoder, messageSinger: messageSigner)
                input.builder.withBody(.stream(encoderStream))
            } else {
                if encoder is JSONEncoder {
                    // Encode an empty body as an empty structure in JSON
                    let audioStreamData = "{}".data(using: .utf8)!
                    let audioStreamBody = ClientRuntime.HttpBody.data(audioStreamData)
                    input.builder.withBody(audioStreamBody)
                }
            }
        } catch let err {
            throw ClientRuntime.ClientError.unknownError(err.localizedDescription)
        }
        return try await next.handle(context: context, input: input)
    }

    public typealias MInput = ClientRuntime.SerializeStepInput<StartCallAnalyticsStreamTranscriptionInput>
    public typealias MOutput = ClientRuntime.OperationOutput<StartCallAnalyticsStreamTranscriptionOutput>
    public typealias Context = ClientRuntime.HttpContext
}

extension StartCallAnalyticsStreamTranscriptionInput: ClientRuntime.HeaderProvider {
    public var headers: ClientRuntime.Headers {
        var items = ClientRuntime.Headers()
        if let contentIdentificationType = contentIdentificationType {
            items.add(Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let contentRedactionType = contentRedactionType {
            items.add(Header(name: "x-amzn-transcribe-content-redaction-type", value: Swift.String(contentRedactionType.rawValue)))
        }
        if let enablePartialResultsStabilization = enablePartialResultsStabilization {
            items.add(Header(name: "x-amzn-transcribe-enable-partial-results-stabilization", value: Swift.String(enablePartialResultsStabilization)))
        }
        if let languageCode = languageCode {
            items.add(Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let languageModelName = languageModelName {
            items.add(Header(name: "x-amzn-transcribe-language-model-name", value: Swift.String(languageModelName)))
        }
        if let mediaEncoding = mediaEncoding {
            items.add(Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = mediaSampleRateHertz {
            items.add(Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let partialResultsStability = partialResultsStability {
            items.add(Header(name: "x-amzn-transcribe-partial-results-stability", value: Swift.String(partialResultsStability.rawValue)))
        }
        if let piiEntityTypes = piiEntityTypes {
            items.add(Header(name: "x-amzn-transcribe-pii-entity-types", value: Swift.String(piiEntityTypes)))
        }
        if let sessionId = sessionId {
            items.add(Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let vocabularyFilterMethod = vocabularyFilterMethod {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-filter-method", value: Swift.String(vocabularyFilterMethod.rawValue)))
        }
        if let vocabularyFilterName = vocabularyFilterName {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-filter-name", value: Swift.String(vocabularyFilterName)))
        }
        if let vocabularyName = vocabularyName {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        return items
    }
}

extension StartCallAnalyticsStreamTranscriptionInput: ClientRuntime.URLPathProvider {
    public var urlPath: Swift.String? {
        return "/call-analytics-stream-transcription"
    }
}

public struct StartCallAnalyticsStreamTranscriptionInput: Swift.Equatable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in PiiEntityTypes is flagged upon complete transcription of an audio segment. You cant set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in PiiEntityTypes is redacted upon complete transcription of an audio segment. You cant set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var enablePartialResultsStabilization: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification. For a list of languages supported with streaming Call Analytics, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table.
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode?
    /// Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. For more information, see [Custom language models](https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html).
    public var languageModelName: Swift.String?
    /// Specify the encoding of your input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. To include PiiEntityTypes in your Call Analytics request, you must also include either ContentIdentificationType or ContentRedactionType. Values must be comma-separated and can include: BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_NUMBER, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, PIN, EMAIL, ADDRESS, NAME, PHONE, SSN, or ALL.
    public var piiEntityTypes: Swift.String?
    /// Specify a name for your Call Analytics transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterName: Swift.String?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyName: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enablePartialResultsStabilization: Swift.Bool? = nil,
        languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.sessionId = sessionId
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyName = vocabularyName
    }
}

extension StartCallAnalyticsStreamTranscriptionOutput: ClientRuntime.HttpResponseBinding {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil) async throws {
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            self.contentIdentificationType = TranscribeStreamingClientTypes.ContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        } else {
            self.contentIdentificationType = nil
        }
        if let contentRedactionTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-redaction-type") {
            self.contentRedactionType = TranscribeStreamingClientTypes.ContentRedactionType(rawValue: contentRedactionTypeHeaderValue)
        } else {
            self.contentRedactionType = nil
        }
        if let enablePartialResultsStabilizationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-partial-results-stabilization") {
            self.enablePartialResultsStabilization = Swift.Bool(enablePartialResultsStabilizationHeaderValue) ?? false
        } else {
            self.enablePartialResultsStabilization = false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            self.languageCode = TranscribeStreamingClientTypes.CallAnalyticsLanguageCode(rawValue: languageCodeHeaderValue)
        } else {
            self.languageCode = nil
        }
        if let languageModelNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-model-name") {
            self.languageModelName = languageModelNameHeaderValue
        } else {
            self.languageModelName = nil
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            self.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        } else {
            self.mediaEncoding = nil
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            self.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        } else {
            self.mediaSampleRateHertz = nil
        }
        if let partialResultsStabilityHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-partial-results-stability") {
            self.partialResultsStability = TranscribeStreamingClientTypes.PartialResultsStability(rawValue: partialResultsStabilityHeaderValue)
        } else {
            self.partialResultsStability = nil
        }
        if let piiEntityTypesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-pii-entity-types") {
            self.piiEntityTypes = piiEntityTypesHeaderValue
        } else {
            self.piiEntityTypes = nil
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            self.requestId = requestIdHeaderValue
        } else {
            self.requestId = nil
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            self.sessionId = sessionIdHeaderValue
        } else {
            self.sessionId = nil
        }
        if let vocabularyFilterMethodHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-method") {
            self.vocabularyFilterMethod = TranscribeStreamingClientTypes.VocabularyFilterMethod(rawValue: vocabularyFilterMethodHeaderValue)
        } else {
            self.vocabularyFilterMethod = nil
        }
        if let vocabularyFilterNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-name") {
            self.vocabularyFilterName = vocabularyFilterNameHeaderValue
        } else {
            self.vocabularyFilterName = nil
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            self.vocabularyName = vocabularyNameHeaderValue
        } else {
            self.vocabularyName = nil
        }
        if case let .stream(stream) = httpResponse.body, let responseDecoder = decoder {
            let messageDecoder = AWSClientRuntime.AWSEventStream.AWSMessageDecoder()
            let decoderStream = ClientRuntime.EventStream.DefaultMessageDecoderStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream>(stream: stream, messageDecoder: messageDecoder, responseDecoder: responseDecoder)
            self.callAnalyticsTranscriptResultStream = decoderStream.toAsyncStream()
        } else {
            self.callAnalyticsTranscriptResultStream = nil
        }
    }
}

public struct StartCallAnalyticsStreamTranscriptionOutput: Swift.Equatable {
    /// Provides detailed information about your Call Analytics streaming session.
    public var callAnalyticsTranscriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream, Swift.Error>?
    /// Shows whether content identification was enabled for your Call Analytics transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Shows whether content redaction was enabled for your Call Analytics transcription.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Shows whether partial results stabilization was enabled for your Call Analytics transcription.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Provides the language code that you specified in your Call Analytics request.
    public var languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode?
    /// Provides the name of the custom language model that you specified in your Call Analytics request.
    public var languageModelName: Swift.String?
    /// Provides the media encoding you specified in your Call Analytics request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your Call Analytics request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the stabilization level used for your transcription.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Lists the PII entity types you specified in your Call Analytics request.
    public var piiEntityTypes: Swift.String?
    /// Provides the identifier for your Call Analytics streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your Call Analytics transcription session.
    public var sessionId: Swift.String?
    /// Provides the vocabulary filtering method used in your Call Analytics transcription.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Provides the name of the custom vocabulary filter that you specified in your Call Analytics request.
    public var vocabularyFilterName: Swift.String?
    /// Provides the name of the custom vocabulary that you specified in your Call Analytics request.
    public var vocabularyName: Swift.String?

    public init(
        callAnalyticsTranscriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.CallAnalyticsTranscriptResultStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enablePartialResultsStabilization: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.CallAnalyticsLanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.callAnalyticsTranscriptResultStream = callAnalyticsTranscriptResultStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.requestId = requestId
        self.sessionId = sessionId
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyName = vocabularyName
    }
}

enum StartCallAnalyticsStreamTranscriptionOutputError: ClientRuntime.HttpResponseErrorBinding {
    static func makeError(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil) async throws -> Swift.Error {
        let restJSONError = try await AWSClientRuntime.RestJSONError(httpResponse: httpResponse)
        let requestID = httpResponse.requestId
        switch restJSONError.errorType {
            case "BadRequestException": return try await BadRequestException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "ConflictException": return try await ConflictException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "InternalFailureException": return try await InternalFailureException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "LimitExceededException": return try await LimitExceededException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "ServiceUnavailableException": return try await ServiceUnavailableException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            default: return try await AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(httpResponse: httpResponse, message: restJSONError.errorMessage, requestID: requestID, typeName: restJSONError.errorType)
        }
    }
}

public struct StartMedicalStreamTranscriptionInputBodyMiddleware: ClientRuntime.Middleware {
    public let id: Swift.String = "StartMedicalStreamTranscriptionInputBodyMiddleware"

    public init() {}

    public func handle<H>(context: Context,
                  input: ClientRuntime.SerializeStepInput<StartMedicalStreamTranscriptionInput>,
                  next: H) async throws -> ClientRuntime.OperationOutput<StartMedicalStreamTranscriptionOutput>
    where H: Handler,
    Self.MInput == H.Input,
    Self.MOutput == H.Output,
    Self.Context == H.Context
    {
        do {
            let encoder = context.getEncoder()
            if let audioStream = input.operationInput.audioStream {
                guard let messageEncoder = context.getMessageEncoder() else {
                    fatalError("Message encoder is required for streaming payload")
                }
                guard let messageSigner = context.getMessageSigner() else {
                    fatalError("Message signer is required for streaming payload")
                }
                let encoderStream = ClientRuntime.EventStream.DefaultMessageEncoderStream(stream: audioStream, messageEncoder: messageEncoder, requestEncoder: encoder, messageSinger: messageSigner)
                input.builder.withBody(.stream(encoderStream))
            } else {
                if encoder is JSONEncoder {
                    // Encode an empty body as an empty structure in JSON
                    let audioStreamData = "{}".data(using: .utf8)!
                    let audioStreamBody = ClientRuntime.HttpBody.data(audioStreamData)
                    input.builder.withBody(audioStreamBody)
                }
            }
        } catch let err {
            throw ClientRuntime.ClientError.unknownError(err.localizedDescription)
        }
        return try await next.handle(context: context, input: input)
    }

    public typealias MInput = ClientRuntime.SerializeStepInput<StartMedicalStreamTranscriptionInput>
    public typealias MOutput = ClientRuntime.OperationOutput<StartMedicalStreamTranscriptionOutput>
    public typealias Context = ClientRuntime.HttpContext
}

extension StartMedicalStreamTranscriptionInput: ClientRuntime.HeaderProvider {
    public var headers: ClientRuntime.Headers {
        var items = ClientRuntime.Headers()
        if let contentIdentificationType = contentIdentificationType {
            items.add(Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let enableChannelIdentification = enableChannelIdentification {
            items.add(Header(name: "x-amzn-transcribe-enable-channel-identification", value: Swift.String(enableChannelIdentification)))
        }
        if let languageCode = languageCode {
            items.add(Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let mediaEncoding = mediaEncoding {
            items.add(Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = mediaSampleRateHertz {
            items.add(Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let numberOfChannels = numberOfChannels {
            items.add(Header(name: "x-amzn-transcribe-number-of-channels", value: Swift.String(numberOfChannels)))
        }
        if let sessionId = sessionId {
            items.add(Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let showSpeakerLabel = showSpeakerLabel {
            items.add(Header(name: "x-amzn-transcribe-show-speaker-label", value: Swift.String(showSpeakerLabel)))
        }
        if let specialty = specialty {
            items.add(Header(name: "x-amzn-transcribe-specialty", value: Swift.String(specialty.rawValue)))
        }
        if let type = type {
            items.add(Header(name: "x-amzn-transcribe-type", value: Swift.String(type.rawValue)))
        }
        if let vocabularyName = vocabularyName {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        return items
    }
}

extension StartMedicalStreamTranscriptionInput: ClientRuntime.URLPathProvider {
    public var urlPath: Swift.String? {
        return "/medical-stream-transcription"
    }
}

public struct StartMedicalStreamTranscriptionInput: Swift.Equatable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personal health information (PHI) identified in your transcript. Content identification is performed at the segment level; PHI is flagged upon complete transcription of an audio segment. For more information, see [Identifying personal health information (PHI) in a transcription](https://docs.aws.amazon.com/transcribe/latest/dg/phi-id.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. Amazon Transcribe Medical only supports US English (en-US).
    /// This member is required.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the encoding used for the input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Amazon Transcribe Medical supports a range from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. Up to two channels are supported.
    public var numberOfChannels: Swift.Int?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe Medical generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool?
    /// Specify the medical specialty contained in your audio.
    /// This member is required.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Specify the type of input audio. For example, choose DICTATION for a provider dictating patient notes and CONVERSATION for a dialogue between a patient and a medical professional.
    /// This member is required.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive.
    public var vocabularyName: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool? = nil,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool? = nil,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.type = type
        self.vocabularyName = vocabularyName
    }
}

extension StartMedicalStreamTranscriptionOutput: ClientRuntime.HttpResponseBinding {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil) async throws {
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            self.contentIdentificationType = TranscribeStreamingClientTypes.MedicalContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        } else {
            self.contentIdentificationType = nil
        }
        if let enableChannelIdentificationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-channel-identification") {
            self.enableChannelIdentification = Swift.Bool(enableChannelIdentificationHeaderValue) ?? false
        } else {
            self.enableChannelIdentification = false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            self.languageCode = TranscribeStreamingClientTypes.LanguageCode(rawValue: languageCodeHeaderValue)
        } else {
            self.languageCode = nil
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            self.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        } else {
            self.mediaEncoding = nil
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            self.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        } else {
            self.mediaSampleRateHertz = nil
        }
        if let numberOfChannelsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-number-of-channels") {
            self.numberOfChannels = Swift.Int(numberOfChannelsHeaderValue) ?? 0
        } else {
            self.numberOfChannels = nil
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            self.requestId = requestIdHeaderValue
        } else {
            self.requestId = nil
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            self.sessionId = sessionIdHeaderValue
        } else {
            self.sessionId = nil
        }
        if let showSpeakerLabelHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-show-speaker-label") {
            self.showSpeakerLabel = Swift.Bool(showSpeakerLabelHeaderValue) ?? false
        } else {
            self.showSpeakerLabel = false
        }
        if let specialtyHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-specialty") {
            self.specialty = TranscribeStreamingClientTypes.Specialty(rawValue: specialtyHeaderValue)
        } else {
            self.specialty = nil
        }
        if let typeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-type") {
            self.type = TranscribeStreamingClientTypes.ModelType(rawValue: typeHeaderValue)
        } else {
            self.type = nil
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            self.vocabularyName = vocabularyNameHeaderValue
        } else {
            self.vocabularyName = nil
        }
        if case let .stream(stream) = httpResponse.body, let responseDecoder = decoder {
            let messageDecoder = AWSClientRuntime.AWSEventStream.AWSMessageDecoder()
            let decoderStream = ClientRuntime.EventStream.DefaultMessageDecoderStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream>(stream: stream, messageDecoder: messageDecoder, responseDecoder: responseDecoder)
            self.transcriptResultStream = decoderStream.toAsyncStream()
        } else {
            self.transcriptResultStream = nil
        }
    }
}

public struct StartMedicalStreamTranscriptionOutput: Swift.Equatable {
    /// Shows whether content identification was enabled for your transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType?
    /// Shows whether channel identification was enabled for your transcription.
    public var enableChannelIdentification: Swift.Bool
    /// Provides the language code that you specified in your request. This must be en-US.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the media encoding you specified in your request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the number of channels that you specified in your request.
    public var numberOfChannels: Swift.Int?
    /// Provides the identifier for your streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your transcription session.
    public var sessionId: Swift.String?
    /// Shows whether speaker partitioning was enabled for your transcription.
    public var showSpeakerLabel: Swift.Bool
    /// Provides the medical specialty that you specified in your request.
    public var specialty: TranscribeStreamingClientTypes.Specialty?
    /// Provides detailed information about your streaming session.
    public var transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream, Swift.Error>?
    /// Provides the type of audio you specified in your request.
    public var type: TranscribeStreamingClientTypes.ModelType?
    /// Provides the name of the custom vocabulary that you specified in your request.
    public var vocabularyName: Swift.String?

    public init(
        contentIdentificationType: TranscribeStreamingClientTypes.MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        specialty: TranscribeStreamingClientTypes.Specialty? = nil,
        transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.MedicalTranscriptResultStream, Swift.Error>? = nil,
        type: TranscribeStreamingClientTypes.ModelType? = nil,
        vocabularyName: Swift.String? = nil
    )
    {
        self.contentIdentificationType = contentIdentificationType
        self.enableChannelIdentification = enableChannelIdentification
        self.languageCode = languageCode
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.requestId = requestId
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.specialty = specialty
        self.transcriptResultStream = transcriptResultStream
        self.type = type
        self.vocabularyName = vocabularyName
    }
}

enum StartMedicalStreamTranscriptionOutputError: ClientRuntime.HttpResponseErrorBinding {
    static func makeError(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil) async throws -> Swift.Error {
        let restJSONError = try await AWSClientRuntime.RestJSONError(httpResponse: httpResponse)
        let requestID = httpResponse.requestId
        switch restJSONError.errorType {
            case "BadRequestException": return try await BadRequestException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "ConflictException": return try await ConflictException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "InternalFailureException": return try await InternalFailureException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "LimitExceededException": return try await LimitExceededException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "ServiceUnavailableException": return try await ServiceUnavailableException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            default: return try await AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(httpResponse: httpResponse, message: restJSONError.errorMessage, requestID: requestID, typeName: restJSONError.errorType)
        }
    }
}

public struct StartStreamTranscriptionInputBodyMiddleware: ClientRuntime.Middleware {
    public let id: Swift.String = "StartStreamTranscriptionInputBodyMiddleware"

    public init() {}

    public func handle<H>(context: Context,
                  input: ClientRuntime.SerializeStepInput<StartStreamTranscriptionInput>,
                  next: H) async throws -> ClientRuntime.OperationOutput<StartStreamTranscriptionOutput>
    where H: Handler,
    Self.MInput == H.Input,
    Self.MOutput == H.Output,
    Self.Context == H.Context
    {
        do {
            let encoder = context.getEncoder()
            if let audioStream = input.operationInput.audioStream {
                guard let messageEncoder = context.getMessageEncoder() else {
                    fatalError("Message encoder is required for streaming payload")
                }
                guard let messageSigner = context.getMessageSigner() else {
                    fatalError("Message signer is required for streaming payload")
                }
                let encoderStream = ClientRuntime.EventStream.DefaultMessageEncoderStream(stream: audioStream, messageEncoder: messageEncoder, requestEncoder: encoder, messageSinger: messageSigner)
                input.builder.withBody(.stream(encoderStream))
            } else {
                if encoder is JSONEncoder {
                    // Encode an empty body as an empty structure in JSON
                    let audioStreamData = "{}".data(using: .utf8)!
                    let audioStreamBody = ClientRuntime.HttpBody.data(audioStreamData)
                    input.builder.withBody(audioStreamBody)
                }
            }
        } catch let err {
            throw ClientRuntime.ClientError.unknownError(err.localizedDescription)
        }
        return try await next.handle(context: context, input: input)
    }

    public typealias MInput = ClientRuntime.SerializeStepInput<StartStreamTranscriptionInput>
    public typealias MOutput = ClientRuntime.OperationOutput<StartStreamTranscriptionOutput>
    public typealias Context = ClientRuntime.HttpContext
}

extension StartStreamTranscriptionInput: ClientRuntime.HeaderProvider {
    public var headers: ClientRuntime.Headers {
        var items = ClientRuntime.Headers()
        if let contentIdentificationType = contentIdentificationType {
            items.add(Header(name: "x-amzn-transcribe-content-identification-type", value: Swift.String(contentIdentificationType.rawValue)))
        }
        if let contentRedactionType = contentRedactionType {
            items.add(Header(name: "x-amzn-transcribe-content-redaction-type", value: Swift.String(contentRedactionType.rawValue)))
        }
        if let enableChannelIdentification = enableChannelIdentification {
            items.add(Header(name: "x-amzn-transcribe-enable-channel-identification", value: Swift.String(enableChannelIdentification)))
        }
        if let enablePartialResultsStabilization = enablePartialResultsStabilization {
            items.add(Header(name: "x-amzn-transcribe-enable-partial-results-stabilization", value: Swift.String(enablePartialResultsStabilization)))
        }
        if let identifyLanguage = identifyLanguage {
            items.add(Header(name: "x-amzn-transcribe-identify-language", value: Swift.String(identifyLanguage)))
        }
        if let languageCode = languageCode {
            items.add(Header(name: "x-amzn-transcribe-language-code", value: Swift.String(languageCode.rawValue)))
        }
        if let languageModelName = languageModelName {
            items.add(Header(name: "x-amzn-transcribe-language-model-name", value: Swift.String(languageModelName)))
        }
        if let languageOptions = languageOptions {
            items.add(Header(name: "x-amzn-transcribe-language-options", value: Swift.String(languageOptions)))
        }
        if let mediaEncoding = mediaEncoding {
            items.add(Header(name: "x-amzn-transcribe-media-encoding", value: Swift.String(mediaEncoding.rawValue)))
        }
        if let mediaSampleRateHertz = mediaSampleRateHertz {
            items.add(Header(name: "x-amzn-transcribe-sample-rate", value: Swift.String(mediaSampleRateHertz)))
        }
        if let numberOfChannels = numberOfChannels {
            items.add(Header(name: "x-amzn-transcribe-number-of-channels", value: Swift.String(numberOfChannels)))
        }
        if let partialResultsStability = partialResultsStability {
            items.add(Header(name: "x-amzn-transcribe-partial-results-stability", value: Swift.String(partialResultsStability.rawValue)))
        }
        if let piiEntityTypes = piiEntityTypes {
            items.add(Header(name: "x-amzn-transcribe-pii-entity-types", value: Swift.String(piiEntityTypes)))
        }
        if let preferredLanguage = preferredLanguage {
            items.add(Header(name: "x-amzn-transcribe-preferred-language", value: Swift.String(preferredLanguage.rawValue)))
        }
        if let sessionId = sessionId {
            items.add(Header(name: "x-amzn-transcribe-session-id", value: Swift.String(sessionId)))
        }
        if let showSpeakerLabel = showSpeakerLabel {
            items.add(Header(name: "x-amzn-transcribe-show-speaker-label", value: Swift.String(showSpeakerLabel)))
        }
        if let vocabularyFilterMethod = vocabularyFilterMethod {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-filter-method", value: Swift.String(vocabularyFilterMethod.rawValue)))
        }
        if let vocabularyFilterName = vocabularyFilterName {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-filter-name", value: Swift.String(vocabularyFilterName)))
        }
        if let vocabularyFilterNames = vocabularyFilterNames {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-filter-names", value: Swift.String(vocabularyFilterNames)))
        }
        if let vocabularyName = vocabularyName {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-name", value: Swift.String(vocabularyName)))
        }
        if let vocabularyNames = vocabularyNames {
            items.add(Header(name: "x-amzn-transcribe-vocabulary-names", value: Swift.String(vocabularyNames)))
        }
        return items
    }
}

extension StartStreamTranscriptionInput: ClientRuntime.URLPathProvider {
    public var urlPath: Swift.String? {
        return "/stream-transcription"
    }
}

public struct StartStreamTranscriptionInput: Swift.Equatable {
    /// An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket data frames. For more information, see [Transcribing streaming audio](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html).
    /// This member is required.
    public var audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>?
    /// Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in PiiEntityTypes is flagged upon complete transcription of an audio segment. You cant set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in PiiEntityTypes is redacted upon complete transcription of an audio segment. You cant set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see [Redacting or identifying personally identifiable information](https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html).
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is transcribed in a continuous manner and your transcript is not separated by channel. For more information, see [Transcribing multi-channel audio](https://docs.aws.amazon.com/transcribe/latest/dg/channel-id.html).
    public var enableChannelIdentification: Swift.Bool?
    /// Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var enablePartialResultsStabilization: Swift.Bool?
    /// Enables automatic language identification for your transcription. If you include IdentifyLanguage, you can optionally include a list of language codes, using LanguageOptions, that you think may be present in your audio stream. Including language options can improve transcription accuracy. You can also include a preferred language using PreferredLanguage. Adding a preferred language can help Amazon Transcribe identify the language faster than if you omit this parameter. If you have multi-channel audio that contains different languages on each channel, and you've enabled channel identification, automatic language identification identifies the dominant language on each audio channel. Note that you must include either LanguageCode or IdentifyLanguage in your request. If you include both parameters, your request fails. Streaming language identification can't be combined with custom language models or redaction.
    public var identifyLanguage: Swift.Bool?
    /// Specify the language code that represents the language spoken in your audio. If you're unsure of the language spoken in your audio, consider using IdentifyLanguage to enable automatic language identification. For a list of languages supported with Amazon Transcribe streaming, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied. There are no errors or warnings associated with a language mismatch. For more information, see [Custom language models](https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html).
    public var languageModelName: Swift.String?
    /// Specify two or more language codes that represent the languages you think may be present in your media; including more than five is not recommended. If you're unsure what languages are present, do not include this parameter. Including language options can improve the accuracy of language identification. If you include LanguageOptions in your request, you must also include IdentifyLanguage. For a list of languages supported with Amazon Transcribe streaming, refer to the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) table. You can only include one language dialect per language per stream. For example, you cannot include en-US and en-AU in the same request.
    public var languageOptions: Swift.String?
    /// Specify the encoding of your input audio. Supported formats are:
    ///
    /// * FLAC
    ///
    /// * OPUS-encoded audio in an Ogg container
    ///
    /// * PCM (only signed 16-bit little-endian audio formats, which does not include WAV)
    ///
    ///
    /// For more information, see [Media formats](https://docs.aws.amazon.com/transcribe/latest/dg/how-input.html#how-input-audio).
    /// This member is required.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    /// This member is required.
    public var mediaSampleRateHertz: Swift.Int?
    /// Specify the number of channels in your audio stream. Up to two channels are supported.
    public var numberOfChannels: Swift.Int?
    /// Specify the level of stability to use when you enable partial results stabilization (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see [Partial-result stabilization](https://docs.aws.amazon.com/transcribe/latest/dg/streaming.html#streaming-partial-result-stabilization).
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Specify which types of personally identifiable information (PII) you want to redact in your transcript. You can include as many types as you'd like, or you can select ALL. To include PiiEntityTypes in your request, you must also include either ContentIdentificationType or ContentRedactionType. Values must be comma-separated and can include: BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_NUMBER, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, PIN, EMAIL, ADDRESS, NAME, PHONE, SSN, or ALL.
    public var piiEntityTypes: Swift.String?
    /// Specify a preferred language from the subset of languages codes you specified in LanguageOptions. You can only use this parameter if you've included IdentifyLanguage and LanguageOptions in your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// Specify a name for your transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response. You can use a session ID to retry a streaming session.
    public var sessionId: Swift.String?
    /// Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see [Partitioning speakers (diarization)](https://docs.aws.amazon.com/transcribe/latest/dg/diarization.html).
    public var showSpeakerLabel: Swift.Bool?
    /// Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterName: Swift.String?
    /// Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If none of the languages of the specified custom vocabulary filters match the language identified in your media, your job fails. This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary filter with your transcription, use the VocabularyFilterName parameter instead. For more information, see [Using vocabulary filtering with unwanted words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).
    public var vocabularyFilterNames: Swift.String?
    /// Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyName: Swift.String?
    /// Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If none of the languages of the specified custom vocabularies match the language identified in your media, your job fails. This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead. For more information, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).
    public var vocabularyNames: Swift.String?

    public init(
        audioStream: AsyncThrowingStream<TranscribeStreamingClientTypes.AudioStream, Swift.Error>? = nil,
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool? = nil,
        enablePartialResultsStabilization: Swift.Bool? = nil,
        identifyLanguage: Swift.Bool? = nil,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    )
    {
        self.audioStream = audioStream
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}

extension StartStreamTranscriptionOutput: ClientRuntime.HttpResponseBinding {
    public init(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil) async throws {
        if let contentIdentificationTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-identification-type") {
            self.contentIdentificationType = TranscribeStreamingClientTypes.ContentIdentificationType(rawValue: contentIdentificationTypeHeaderValue)
        } else {
            self.contentIdentificationType = nil
        }
        if let contentRedactionTypeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-content-redaction-type") {
            self.contentRedactionType = TranscribeStreamingClientTypes.ContentRedactionType(rawValue: contentRedactionTypeHeaderValue)
        } else {
            self.contentRedactionType = nil
        }
        if let enableChannelIdentificationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-channel-identification") {
            self.enableChannelIdentification = Swift.Bool(enableChannelIdentificationHeaderValue) ?? false
        } else {
            self.enableChannelIdentification = false
        }
        if let enablePartialResultsStabilizationHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-enable-partial-results-stabilization") {
            self.enablePartialResultsStabilization = Swift.Bool(enablePartialResultsStabilizationHeaderValue) ?? false
        } else {
            self.enablePartialResultsStabilization = false
        }
        if let identifyLanguageHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-identify-language") {
            self.identifyLanguage = Swift.Bool(identifyLanguageHeaderValue) ?? false
        } else {
            self.identifyLanguage = false
        }
        if let languageCodeHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-code") {
            self.languageCode = TranscribeStreamingClientTypes.LanguageCode(rawValue: languageCodeHeaderValue)
        } else {
            self.languageCode = nil
        }
        if let languageModelNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-model-name") {
            self.languageModelName = languageModelNameHeaderValue
        } else {
            self.languageModelName = nil
        }
        if let languageOptionsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-language-options") {
            self.languageOptions = languageOptionsHeaderValue
        } else {
            self.languageOptions = nil
        }
        if let mediaEncodingHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-media-encoding") {
            self.mediaEncoding = TranscribeStreamingClientTypes.MediaEncoding(rawValue: mediaEncodingHeaderValue)
        } else {
            self.mediaEncoding = nil
        }
        if let mediaSampleRateHertzHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-sample-rate") {
            self.mediaSampleRateHertz = Swift.Int(mediaSampleRateHertzHeaderValue) ?? 0
        } else {
            self.mediaSampleRateHertz = nil
        }
        if let numberOfChannelsHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-number-of-channels") {
            self.numberOfChannels = Swift.Int(numberOfChannelsHeaderValue) ?? 0
        } else {
            self.numberOfChannels = nil
        }
        if let partialResultsStabilityHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-partial-results-stability") {
            self.partialResultsStability = TranscribeStreamingClientTypes.PartialResultsStability(rawValue: partialResultsStabilityHeaderValue)
        } else {
            self.partialResultsStability = nil
        }
        if let piiEntityTypesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-pii-entity-types") {
            self.piiEntityTypes = piiEntityTypesHeaderValue
        } else {
            self.piiEntityTypes = nil
        }
        if let preferredLanguageHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-preferred-language") {
            self.preferredLanguage = TranscribeStreamingClientTypes.LanguageCode(rawValue: preferredLanguageHeaderValue)
        } else {
            self.preferredLanguage = nil
        }
        if let requestIdHeaderValue = httpResponse.headers.value(for: "x-amzn-request-id") {
            self.requestId = requestIdHeaderValue
        } else {
            self.requestId = nil
        }
        if let sessionIdHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-session-id") {
            self.sessionId = sessionIdHeaderValue
        } else {
            self.sessionId = nil
        }
        if let showSpeakerLabelHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-show-speaker-label") {
            self.showSpeakerLabel = Swift.Bool(showSpeakerLabelHeaderValue) ?? false
        } else {
            self.showSpeakerLabel = false
        }
        if let vocabularyFilterMethodHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-method") {
            self.vocabularyFilterMethod = TranscribeStreamingClientTypes.VocabularyFilterMethod(rawValue: vocabularyFilterMethodHeaderValue)
        } else {
            self.vocabularyFilterMethod = nil
        }
        if let vocabularyFilterNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-name") {
            self.vocabularyFilterName = vocabularyFilterNameHeaderValue
        } else {
            self.vocabularyFilterName = nil
        }
        if let vocabularyFilterNamesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-filter-names") {
            self.vocabularyFilterNames = vocabularyFilterNamesHeaderValue
        } else {
            self.vocabularyFilterNames = nil
        }
        if let vocabularyNameHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-name") {
            self.vocabularyName = vocabularyNameHeaderValue
        } else {
            self.vocabularyName = nil
        }
        if let vocabularyNamesHeaderValue = httpResponse.headers.value(for: "x-amzn-transcribe-vocabulary-names") {
            self.vocabularyNames = vocabularyNamesHeaderValue
        } else {
            self.vocabularyNames = nil
        }
        if case let .stream(stream) = httpResponse.body, let responseDecoder = decoder {
            let messageDecoder = AWSClientRuntime.AWSEventStream.AWSMessageDecoder()
            let decoderStream = ClientRuntime.EventStream.DefaultMessageDecoderStream<TranscribeStreamingClientTypes.TranscriptResultStream>(stream: stream, messageDecoder: messageDecoder, responseDecoder: responseDecoder)
            self.transcriptResultStream = decoderStream.toAsyncStream()
        } else {
            self.transcriptResultStream = nil
        }
    }
}

public struct StartStreamTranscriptionOutput: Swift.Equatable {
    /// Shows whether content identification was enabled for your transcription.
    public var contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType?
    /// Shows whether content redaction was enabled for your transcription.
    public var contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType?
    /// Shows whether channel identification was enabled for your transcription.
    public var enableChannelIdentification: Swift.Bool
    /// Shows whether partial results stabilization was enabled for your transcription.
    public var enablePartialResultsStabilization: Swift.Bool
    /// Shows whether automatic language identification was enabled for your transcription.
    public var identifyLanguage: Swift.Bool
    /// Provides the language code that you specified in your request.
    public var languageCode: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the name of the custom language model that you specified in your request.
    public var languageModelName: Swift.String?
    /// Provides the language codes that you specified in your request.
    public var languageOptions: Swift.String?
    /// Provides the media encoding you specified in your request.
    public var mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding?
    /// Provides the sample rate that you specified in your request.
    public var mediaSampleRateHertz: Swift.Int?
    /// Provides the number of channels that you specified in your request.
    public var numberOfChannels: Swift.Int?
    /// Provides the stabilization level used for your transcription.
    public var partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability?
    /// Lists the PII entity types you specified in your request.
    public var piiEntityTypes: Swift.String?
    /// Provides the preferred language that you specified in your request.
    public var preferredLanguage: TranscribeStreamingClientTypes.LanguageCode?
    /// Provides the identifier for your streaming request.
    public var requestId: Swift.String?
    /// Provides the identifier for your transcription session.
    public var sessionId: Swift.String?
    /// Shows whether speaker partitioning was enabled for your transcription.
    public var showSpeakerLabel: Swift.Bool
    /// Provides detailed information about your streaming session.
    public var transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.TranscriptResultStream, Swift.Error>?
    /// Provides the vocabulary filtering method used in your transcription.
    public var vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod?
    /// Provides the name of the custom vocabulary filter that you specified in your request.
    public var vocabularyFilterName: Swift.String?
    /// Provides the names of the custom vocabulary filters that you specified in your request.
    public var vocabularyFilterNames: Swift.String?
    /// Provides the name of the custom vocabulary that you specified in your request.
    public var vocabularyName: Swift.String?
    /// Provides the names of the custom vocabularies that you specified in your request.
    public var vocabularyNames: Swift.String?

    public init(
        contentIdentificationType: TranscribeStreamingClientTypes.ContentIdentificationType? = nil,
        contentRedactionType: TranscribeStreamingClientTypes.ContentRedactionType? = nil,
        enableChannelIdentification: Swift.Bool = false,
        enablePartialResultsStabilization: Swift.Bool = false,
        identifyLanguage: Swift.Bool = false,
        languageCode: TranscribeStreamingClientTypes.LanguageCode? = nil,
        languageModelName: Swift.String? = nil,
        languageOptions: Swift.String? = nil,
        mediaEncoding: TranscribeStreamingClientTypes.MediaEncoding? = nil,
        mediaSampleRateHertz: Swift.Int? = nil,
        numberOfChannels: Swift.Int? = nil,
        partialResultsStability: TranscribeStreamingClientTypes.PartialResultsStability? = nil,
        piiEntityTypes: Swift.String? = nil,
        preferredLanguage: TranscribeStreamingClientTypes.LanguageCode? = nil,
        requestId: Swift.String? = nil,
        sessionId: Swift.String? = nil,
        showSpeakerLabel: Swift.Bool = false,
        transcriptResultStream: AsyncThrowingStream<TranscribeStreamingClientTypes.TranscriptResultStream, Swift.Error>? = nil,
        vocabularyFilterMethod: TranscribeStreamingClientTypes.VocabularyFilterMethod? = nil,
        vocabularyFilterName: Swift.String? = nil,
        vocabularyFilterNames: Swift.String? = nil,
        vocabularyName: Swift.String? = nil,
        vocabularyNames: Swift.String? = nil
    )
    {
        self.contentIdentificationType = contentIdentificationType
        self.contentRedactionType = contentRedactionType
        self.enableChannelIdentification = enableChannelIdentification
        self.enablePartialResultsStabilization = enablePartialResultsStabilization
        self.identifyLanguage = identifyLanguage
        self.languageCode = languageCode
        self.languageModelName = languageModelName
        self.languageOptions = languageOptions
        self.mediaEncoding = mediaEncoding
        self.mediaSampleRateHertz = mediaSampleRateHertz
        self.numberOfChannels = numberOfChannels
        self.partialResultsStability = partialResultsStability
        self.piiEntityTypes = piiEntityTypes
        self.preferredLanguage = preferredLanguage
        self.requestId = requestId
        self.sessionId = sessionId
        self.showSpeakerLabel = showSpeakerLabel
        self.transcriptResultStream = transcriptResultStream
        self.vocabularyFilterMethod = vocabularyFilterMethod
        self.vocabularyFilterName = vocabularyFilterName
        self.vocabularyFilterNames = vocabularyFilterNames
        self.vocabularyName = vocabularyName
        self.vocabularyNames = vocabularyNames
    }
}

enum StartStreamTranscriptionOutputError: ClientRuntime.HttpResponseErrorBinding {
    static func makeError(httpResponse: ClientRuntime.HttpResponse, decoder: ClientRuntime.ResponseDecoder? = nil) async throws -> Swift.Error {
        let restJSONError = try await AWSClientRuntime.RestJSONError(httpResponse: httpResponse)
        let requestID = httpResponse.requestId
        switch restJSONError.errorType {
            case "BadRequestException": return try await BadRequestException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "ConflictException": return try await ConflictException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "InternalFailureException": return try await InternalFailureException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "LimitExceededException": return try await LimitExceededException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            case "ServiceUnavailableException": return try await ServiceUnavailableException(httpResponse: httpResponse, decoder: decoder, message: restJSONError.errorMessage, requestID: requestID)
            default: return try await AWSClientRuntime.UnknownAWSHTTPServiceError.makeError(httpResponse: httpResponse, message: restJSONError.errorMessage, requestID: requestID, typeName: restJSONError.errorType)
        }
    }
}

extension TranscribeStreamingClientTypes.TimestampRange: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case beginOffsetMillis = "BeginOffsetMillis"
        case endOffsetMillis = "EndOffsetMillis"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let beginOffsetMillis = self.beginOffsetMillis {
            try encodeContainer.encode(beginOffsetMillis, forKey: .beginOffsetMillis)
        }
        if let endOffsetMillis = self.endOffsetMillis {
            try encodeContainer.encode(endOffsetMillis, forKey: .endOffsetMillis)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let beginOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .beginOffsetMillis)
        beginOffsetMillis = beginOffsetMillisDecoded
        let endOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .endOffsetMillis)
        endOffsetMillis = endOffsetMillisDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains the timestamp range (start time through end time) of a matched category.
    public struct TimestampRange: Swift.Equatable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the category match.
        public var beginOffsetMillis: Swift.Int?
        /// The time, in milliseconds, from the beginning of the audio stream to the end of the category match.
        public var endOffsetMillis: Swift.Int?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            endOffsetMillis: Swift.Int? = nil
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.endOffsetMillis = endOffsetMillis
        }
    }

}

extension TranscribeStreamingClientTypes.Transcript: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case results = "Results"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let results = results {
            var resultsContainer = encodeContainer.nestedUnkeyedContainer(forKey: .results)
            for result0 in results {
                try resultsContainer.encode(result0)
            }
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let resultsContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.Result?].self, forKey: .results)
        var resultsDecoded0:[TranscribeStreamingClientTypes.Result]? = nil
        if let resultsContainer = resultsContainer {
            resultsDecoded0 = [TranscribeStreamingClientTypes.Result]()
            for structure0 in resultsContainer {
                if let structure0 = structure0 {
                    resultsDecoded0?.append(structure0)
                }
            }
        }
        results = resultsDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// The Transcript associated with a . Transcript contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct Transcript: Swift.Equatable {
        /// Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var results: [TranscribeStreamingClientTypes.Result]?

        public init(
            results: [TranscribeStreamingClientTypes.Result]? = nil
        )
        {
            self.results = results
        }
    }

}

extension TranscribeStreamingClientTypes.TranscriptEvent: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case transcript = "Transcript"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let transcript = self.transcript {
            try encodeContainer.encode(transcript, forKey: .transcript)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let transcriptDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.Transcript.self, forKey: .transcript)
        transcript = transcriptDecoded
    }
}

extension TranscribeStreamingClientTypes {
    /// The TranscriptEvent associated with a TranscriptResultStream. Contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
    public struct TranscriptEvent: Swift.Equatable {
        /// Contains Results, which contains a set of transcription results from one or more audio segments, along with additional information per your request parameters. This can include information relating to alternative transcriptions, channel identification, partial result stabilization, language identification, and other transcription-related data.
        public var transcript: TranscribeStreamingClientTypes.Transcript?

        public init(
            transcript: TranscribeStreamingClientTypes.Transcript? = nil
        )
        {
            self.transcript = transcript
        }
    }

}

extension TranscribeStreamingClientTypes.TranscriptResultStream: ClientRuntime.MessageUnmarshallable {
    public init(message: ClientRuntime.EventStream.Message, decoder: ClientRuntime.ResponseDecoder) throws {
        switch try message.type() {
        case .event(let params):
            switch params.eventType {
            case "TranscriptEvent":
                self = .transcriptevent(try decoder.decode(responseBody: message.payload))
            default:
                self = .sdkUnknown("error processing event stream, unrecognized event: \(params.eventType)")
            }
        case .exception(let params):
            let makeError: (ClientRuntime.EventStream.Message, ClientRuntime.EventStream.MessageType.ExceptionParams) throws -> Swift.Error = { message, params in
                switch params.exceptionType {
                case "BadRequestException":
                    return try decoder.decode(responseBody: message.payload) as BadRequestException
                case "LimitExceededException":
                    return try decoder.decode(responseBody: message.payload) as LimitExceededException
                case "InternalFailureException":
                    return try decoder.decode(responseBody: message.payload) as InternalFailureException
                case "ConflictException":
                    return try decoder.decode(responseBody: message.payload) as ConflictException
                case "ServiceUnavailableException":
                    return try decoder.decode(responseBody: message.payload) as ServiceUnavailableException
                default:
                    let httpResponse = HttpResponse(body: .data(message.payload), statusCode: .ok)
                    return AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':exceptionType': \(params.exceptionType); contentType: \(params.contentType ?? "nil")", requestID: nil, typeName: nil)
                }
            }
            let error = try makeError(message, params)
            throw error
        case .error(let params):
            let httpResponse = HttpResponse(body: .data(message.payload), statusCode: .ok)
            throw AWSClientRuntime.UnknownAWSHTTPServiceError(httpResponse: httpResponse, message: "error processing event stream, unrecognized ':errorType': \(params.errorCode); message: \(params.message ?? "nil")", requestID: nil, typeName: nil)
        case .unknown(messageType: let messageType):
            throw ClientRuntime.ClientError.unknownError("unrecognized event stream message ':message-type': \(messageType)")
        }
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains detailed information about your streaming session.
    public enum TranscriptResultStream: Swift.Equatable {
        /// Contains Transcript, which contains Results. The  object contains a set of transcription results from one or more audio segments, along with additional information per your request parameters.
        case transcriptevent(TranscribeStreamingClientTypes.TranscriptEvent)
        case sdkUnknown(Swift.String)
    }

}

extension TranscribeStreamingClientTypes {
    public enum ModelType: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case conversation
        case dictation
        case sdkUnknown(Swift.String)

        public static var allCases: [ModelType] {
            return [
                .conversation,
                .dictation,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .conversation: return "CONVERSATION"
            case .dictation: return "DICTATION"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = ModelType(rawValue: rawValue) ?? ModelType.sdkUnknown(rawValue)
        }
    }
}

extension TranscribeStreamingClientTypes.UtteranceEvent: Swift.Codable {
    enum CodingKeys: Swift.String, Swift.CodingKey {
        case beginOffsetMillis = "BeginOffsetMillis"
        case endOffsetMillis = "EndOffsetMillis"
        case entities = "Entities"
        case isPartial = "IsPartial"
        case issuesDetected = "IssuesDetected"
        case items = "Items"
        case participantRole = "ParticipantRole"
        case sentiment = "Sentiment"
        case transcript = "Transcript"
        case utteranceId = "UtteranceId"
    }

    public func encode(to encoder: Swift.Encoder) throws {
        var encodeContainer = encoder.container(keyedBy: CodingKeys.self)
        if let beginOffsetMillis = self.beginOffsetMillis {
            try encodeContainer.encode(beginOffsetMillis, forKey: .beginOffsetMillis)
        }
        if let endOffsetMillis = self.endOffsetMillis {
            try encodeContainer.encode(endOffsetMillis, forKey: .endOffsetMillis)
        }
        if let entities = entities {
            var entitiesContainer = encodeContainer.nestedUnkeyedContainer(forKey: .entities)
            for callanalyticsentity0 in entities {
                try entitiesContainer.encode(callanalyticsentity0)
            }
        }
        if isPartial != false {
            try encodeContainer.encode(isPartial, forKey: .isPartial)
        }
        if let issuesDetected = issuesDetected {
            var issuesDetectedContainer = encodeContainer.nestedUnkeyedContainer(forKey: .issuesDetected)
            for issuedetected0 in issuesDetected {
                try issuesDetectedContainer.encode(issuedetected0)
            }
        }
        if let items = items {
            var itemsContainer = encodeContainer.nestedUnkeyedContainer(forKey: .items)
            for callanalyticsitem0 in items {
                try itemsContainer.encode(callanalyticsitem0)
            }
        }
        if let participantRole = self.participantRole {
            try encodeContainer.encode(participantRole.rawValue, forKey: .participantRole)
        }
        if let sentiment = self.sentiment {
            try encodeContainer.encode(sentiment.rawValue, forKey: .sentiment)
        }
        if let transcript = self.transcript {
            try encodeContainer.encode(transcript, forKey: .transcript)
        }
        if let utteranceId = self.utteranceId {
            try encodeContainer.encode(utteranceId, forKey: .utteranceId)
        }
    }

    public init(from decoder: Swift.Decoder) throws {
        let containerValues = try decoder.container(keyedBy: CodingKeys.self)
        let utteranceIdDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .utteranceId)
        utteranceId = utteranceIdDecoded
        let isPartialDecoded = try containerValues.decodeIfPresent(Swift.Bool.self, forKey: .isPartial) ?? false
        isPartial = isPartialDecoded
        let participantRoleDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.ParticipantRole.self, forKey: .participantRole)
        participantRole = participantRoleDecoded
        let beginOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .beginOffsetMillis)
        beginOffsetMillis = beginOffsetMillisDecoded
        let endOffsetMillisDecoded = try containerValues.decodeIfPresent(Swift.Int.self, forKey: .endOffsetMillis)
        endOffsetMillis = endOffsetMillisDecoded
        let transcriptDecoded = try containerValues.decodeIfPresent(Swift.String.self, forKey: .transcript)
        transcript = transcriptDecoded
        let itemsContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.CallAnalyticsItem?].self, forKey: .items)
        var itemsDecoded0:[TranscribeStreamingClientTypes.CallAnalyticsItem]? = nil
        if let itemsContainer = itemsContainer {
            itemsDecoded0 = [TranscribeStreamingClientTypes.CallAnalyticsItem]()
            for structure0 in itemsContainer {
                if let structure0 = structure0 {
                    itemsDecoded0?.append(structure0)
                }
            }
        }
        items = itemsDecoded0
        let entitiesContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.CallAnalyticsEntity?].self, forKey: .entities)
        var entitiesDecoded0:[TranscribeStreamingClientTypes.CallAnalyticsEntity]? = nil
        if let entitiesContainer = entitiesContainer {
            entitiesDecoded0 = [TranscribeStreamingClientTypes.CallAnalyticsEntity]()
            for structure0 in entitiesContainer {
                if let structure0 = structure0 {
                    entitiesDecoded0?.append(structure0)
                }
            }
        }
        entities = entitiesDecoded0
        let sentimentDecoded = try containerValues.decodeIfPresent(TranscribeStreamingClientTypes.Sentiment.self, forKey: .sentiment)
        sentiment = sentimentDecoded
        let issuesDetectedContainer = try containerValues.decodeIfPresent([TranscribeStreamingClientTypes.IssueDetected?].self, forKey: .issuesDetected)
        var issuesDetectedDecoded0:[TranscribeStreamingClientTypes.IssueDetected]? = nil
        if let issuesDetectedContainer = issuesDetectedContainer {
            issuesDetectedDecoded0 = [TranscribeStreamingClientTypes.IssueDetected]()
            for structure0 in issuesDetectedContainer {
                if let structure0 = structure0 {
                    issuesDetectedDecoded0?.append(structure0)
                }
            }
        }
        issuesDetected = issuesDetectedDecoded0
    }
}

extension TranscribeStreamingClientTypes {
    /// Contains set of transcription results from one or more audio segments, along with additional information about the parameters included in your request. For example, channel definitions, partial result stabilization, sentiment, and issue detection.
    public struct UtteranceEvent: Swift.Equatable {
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the UtteranceEvent.
        public var beginOffsetMillis: Swift.Int?
        /// The time, in milliseconds, from the beginning of the audio stream to the start of the UtteranceEvent.
        public var endOffsetMillis: Swift.Int?
        /// Contains entities identified as personally identifiable information (PII) in your transcription output.
        public var entities: [TranscribeStreamingClientTypes.CallAnalyticsEntity]?
        /// Indicates whether the segment in the UtteranceEvent is complete (FALSE) or partial (TRUE).
        public var isPartial: Swift.Bool
        /// Provides the issue that was detected in the specified segment.
        public var issuesDetected: [TranscribeStreamingClientTypes.IssueDetected]?
        /// Contains words, phrases, or punctuation marks that are associated with the specified UtteranceEvent.
        public var items: [TranscribeStreamingClientTypes.CallAnalyticsItem]?
        /// Provides the role of the speaker for each audio channel, either CUSTOMER or AGENT.
        public var participantRole: TranscribeStreamingClientTypes.ParticipantRole?
        /// Provides the sentiment that was detected in the specified segment.
        public var sentiment: TranscribeStreamingClientTypes.Sentiment?
        /// Contains transcribed text.
        public var transcript: Swift.String?
        /// The unique identifier that is associated with the specified UtteranceEvent.
        public var utteranceId: Swift.String?

        public init(
            beginOffsetMillis: Swift.Int? = nil,
            endOffsetMillis: Swift.Int? = nil,
            entities: [TranscribeStreamingClientTypes.CallAnalyticsEntity]? = nil,
            isPartial: Swift.Bool = false,
            issuesDetected: [TranscribeStreamingClientTypes.IssueDetected]? = nil,
            items: [TranscribeStreamingClientTypes.CallAnalyticsItem]? = nil,
            participantRole: TranscribeStreamingClientTypes.ParticipantRole? = nil,
            sentiment: TranscribeStreamingClientTypes.Sentiment? = nil,
            transcript: Swift.String? = nil,
            utteranceId: Swift.String? = nil
        )
        {
            self.beginOffsetMillis = beginOffsetMillis
            self.endOffsetMillis = endOffsetMillis
            self.entities = entities
            self.isPartial = isPartial
            self.issuesDetected = issuesDetected
            self.items = items
            self.participantRole = participantRole
            self.sentiment = sentiment
            self.transcript = transcript
            self.utteranceId = utteranceId
        }
    }

}

extension TranscribeStreamingClientTypes {
    public enum VocabularyFilterMethod: Swift.Equatable, Swift.RawRepresentable, Swift.CaseIterable, Swift.Codable, Swift.Hashable {
        case mask
        case remove
        case tag
        case sdkUnknown(Swift.String)

        public static var allCases: [VocabularyFilterMethod] {
            return [
                .mask,
                .remove,
                .tag,
                .sdkUnknown("")
            ]
        }
        public init?(rawValue: Swift.String) {
            let value = Self.allCases.first(where: { $0.rawValue == rawValue })
            self = value ?? Self.sdkUnknown(rawValue)
        }
        public var rawValue: Swift.String {
            switch self {
            case .mask: return "mask"
            case .remove: return "remove"
            case .tag: return "tag"
            case let .sdkUnknown(s): return s
            }
        }
        public init(from decoder: Swift.Decoder) throws {
            let container = try decoder.singleValueContainer()
            let rawValue = try container.decode(RawValue.self)
            self = VocabularyFilterMethod(rawValue: rawValue) ?? VocabularyFilterMethod.sdkUnknown(rawValue)
        }
    }
}
